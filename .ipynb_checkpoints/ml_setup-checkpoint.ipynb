{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLPClassifier--multi-layer perceptron trains using backpropagation\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import scipy.io as spio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spio.loadmat(\"../../../features.mat\",squeeze_me=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'animal_id_features', 'animal_names', 'dataset_params', 'feat_names', 'features', 'i_period', 'label_names', 'labels_features'])\n"
     ]
    }
   ],
   "source": [
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "animal_names = data['animal_names'];\n",
    "# i = 0\n",
    "# for fn in animal_names:\n",
    "#     print(\"\\n \" + str(i) + \". \" + fn)\n",
    "#     i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_params = data['dataset_params'];\n",
    "# print(dataset_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_names = data['feat_names'];\n",
    "i = 0\n",
    "# for fn in feat_names:\n",
    "#     print(\"\\n \" + str(i) + \". \" + fn)\n",
    "#     i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "animal_id_features = data['animal_id_features'];\n",
    "# print(animal_id_features);\n",
    "# print(len(animal_id_features));\n",
    "features = data['features'];\n",
    "i_period = data['i_period'];\n",
    "label_names = data['label_names'];\n",
    "label_feats = data['labels_features'];\n",
    "# print(len(label_feats));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [69883 85492 39066 ..., 42613 43567 68268] TEST: [72121 78670 55899 ..., 49813 42561 23367]\n",
      "TRAIN: [76293 64771 21915 ..., 90052 23850 19022] TEST: [ 9730 64941 69255 ..., 41486 73341 75121]\n",
      "TRAIN: [64982  4463 95192 ..., 32502 85357 70554] TEST: [67977 29070  3087 ..., 31413 95793 12811]\n",
      "TRAIN: [83966 81661 84269 ..., 64266 26136 82805] TEST: [14788 50854 62294 ..., 89022 77294  7669]\n",
      "TRAIN: [70880  9947 19443 ..., 15180 56940 74562] TEST: [40031 94478 47987 ..., 24812 90619 42046]\n",
      "TRAIN: [80708 84309 48877 ..., 89406 88931 18552] TEST: [79726 48478 43140 ..., 45977 53467 70255]\n",
      "TRAIN: [24589 55470 67371 ...,  6161 36681 80399] TEST: [42946 64676 80975 ..., 81225 42310 47322]\n",
      "TRAIN: [51371 55308 70879 ..., 48878  4921 21449] TEST: [37952 33773   313 ...,  2677 76237 17035]\n",
      "TRAIN: [  870 94888 35945 ..., 34629 63673 20868] TEST: [81634 44010 77257 ..., 33213 84815 81576]\n",
      "TRAIN: [33727 26863 38434 ..., 12756 13457 74746] TEST: [10431 40507 29603 ..., 75833 36805 43568]\n"
     ]
    }
   ],
   "source": [
    "## Using this very hackery method I discovered 96,666 features for Rat 1.\n",
    "# i = 0\n",
    "# while (animal_id_features[i] == 1):\n",
    "#     i=i+1\n",
    "# print(\"There are \" + str(i) + \" recorded features for Rat 0 *fKH41*.\");\n",
    "\n",
    "# Now we split fKH41's features from the ndarray.\n",
    "fKH41_features = features[0:96666];\n",
    "fKH41_labels = label_feats[0:96666];\n",
    "\n",
    "# Separate data into 80% train and 20% test using xval technique.\n",
    "\n",
    "\n",
    "# rs = ShuffleSplit(n_splits=10,test_size=0.2,random_state=0)\n",
    "# # print(rs)\n",
    "# for train_index, test_index in rs.split(fKH41_features,fKH41_labels):\n",
    "#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "\n",
    "\n",
    "# train_features = fKH41_features[0:77333]\n",
    "# train_labels = label_feats[0:77333]\n",
    "\n",
    "# test_features = fKH41_features[77334:]\n",
    "# test_labels = label_feats[77334:96666]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data into two NumPy arrays\n",
    "features = train_features;\n",
    "labels = train_labels;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(5, 2), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=1, shuffle=True, solver='lbfgs', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a basic classifier from SKLearn\n",
    "clf = MLPClassifier(solver='lbfgs',alpha=1e-5,\n",
    "                   hidden_layer_sizes=(5,2),random_state=1)\n",
    "# Classifier trains on array (n_samples, n_features) and array (n_labels)\n",
    "clf.fit(train_features,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1], dtype=uint8)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After fitting, the model can predict labels for the test data.\n",
    "clf.predict(test_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset in terms of placeholder tensors, and feed arrays when initialize iterator.\n",
    "features_placeholder = tf.placeholder(features.dtype, features.shape)\n",
    "labels_placeholder = tf.placeholder(labels.dtype, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48849.75\n"
     ]
    }
   ],
   "source": [
    "# To begin, we will use the EEG recordings from a single rat.\n",
    "# We will train on 80% of the recordings, reserving 20% for testing.\n",
    "\n",
    "# Get all features of rat 1. \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: ((141,), ()), types: (tf.float64, tf.uint8)>\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "586197"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RMS ch1'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
