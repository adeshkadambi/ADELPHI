{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "from matplotlib.font_manager import _rebuild; _rebuild()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.io as spio\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras import optimizers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "tf.reset_default_graph()\n",
    "# Initialize random number generator for reproducibility.\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "# sys.stdout = open(\"keras_pooled_mlp_log.txt\", \"w\")\n",
    "\n",
    "# Set up logging for pipeline mechanics.\n",
    "# logging.basicConfig(filename='pooled_mlp_keras.log', \n",
    "#                     filemode='w', \n",
    "#                     format='%(name)s - %(levelname)s - %(message)s',\n",
    "#                     level=logging.INFO)\n",
    "# logging.info(\"Loading data...\")\n",
    "\n",
    "# Load in dataset.\n",
    "data = spio.loadmat(\"features_10s_2019-01-30.mat\");\n",
    "features = data['features'];\n",
    "labels = data['labels_features'];\n",
    "animal_id_features = data['animal_id_features'].transpose();\n",
    "animal_names = data['animal_names'].transpose();\n",
    "feat_names = data['feat_names'];\n",
    "col_names = pd.DataFrame(feat_names)\n",
    "logging.info(\"Data loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(141,)\n"
     ]
    }
   ],
   "source": [
    "# Label each feature column with its description.\n",
    "def find_between(s):\n",
    "    start = '\\'';\n",
    "    end = '\\'';\n",
    "    return((s.split(start))[1].split(end)[0])\n",
    "cols = [];\n",
    "c_names = col_names.values.ravel();\n",
    "\n",
    "for x in range(len(c_names)):\n",
    "    name = str (c_names[x]);\n",
    "    cols.append(find_between(name))\n",
    "\n",
    "# Create a dataframe of features with columns named & rows labeled.\n",
    "feat_data = pd.DataFrame(data=features,columns=cols)\n",
    "feat_data.insert(0,'AnimalId',animal_id_features)\n",
    "feat_data.insert(0,'Labels',labels.transpose())\n",
    "\n",
    "# Separate features from targets. (Drop AnimalID as confounding.)\n",
    "y = feat_data['Labels']\n",
    "X = feat_data.drop(columns={'Labels','AnimalId'})\n",
    "\n",
    "print(X.iloc[0].shape)\n",
    "\n",
    "# Encode class values as integers.\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "encoded_y = encoder.transform(y)\n",
    "\n",
    "\n",
    "# Split data into training (80%) and testing (20%).\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, encoded_y, test_size=0.2)\n",
    "# Encodes into 3 categorical features\n",
    "# y_train_cat = np_utils.to_categorical(y_train) \n",
    "# y_test_cat = np_utils.to_categorical(y_test)\n",
    "\n",
    "# Train the scaler, which standarizes features (mean=0 & unit variance)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Apply the transformations to the data.\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 375165 samples, validate on 93792 samples\n",
      "Epoch 1/150\n",
      "375165/375165 [==============================] - 16s 43us/step - loss: 0.6533 - acc: 0.6141 - val_loss: 0.6237 - val_acc: 0.6411\n",
      "Epoch 2/150\n",
      "375165/375165 [==============================] - 16s 43us/step - loss: 0.6239 - acc: 0.6385 - val_loss: 0.6100 - val_acc: 0.6560\n",
      "Epoch 3/150\n",
      "375165/375165 [==============================] - 16s 42us/step - loss: 0.6144 - acc: 0.6491 - val_loss: 0.6028 - val_acc: 0.6618\n",
      "Epoch 4/150\n",
      "375165/375165 [==============================] - 16s 41us/step - loss: 0.6066 - acc: 0.6565 - val_loss: 0.5948 - val_acc: 0.6689\n",
      "Epoch 5/150\n",
      "375165/375165 [==============================] - 16s 41us/step - loss: 0.6017 - acc: 0.6613 - val_loss: 0.5893 - val_acc: 0.6731\n",
      "Epoch 6/150\n",
      "  6656/375165 [..............................] - ETA: 15s - loss: 0.5953 - acc: 0.6644"
     ]
    }
   ],
   "source": [
    "# Set up TensorBoard callbacks\n",
    "tb_callback = TensorBoard(log_dir='./logs/', \n",
    "                                         histogram_freq=0,\n",
    "                                         write_graph=True, \n",
    "                                         write_images=True)\n",
    "\n",
    "# Set up Early Stopping for if loss begins to increase\n",
    "early_callback = EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=0,\n",
    "                              patience=2,\n",
    "                              verbose=0, mode='auto')\n",
    "\n",
    "logging.info(\"Initializing Model: Keras Sequential NN\\n\")\n",
    "# print(\"X_train shape: {}\\n\".format(X_train.shape))\n",
    "# print(\"y_train shape: {}\\n\".format(y_train.shape))\n",
    "# print(\"y_train_cat shape: {}\\n\".format(y_train_cat.shape))\n",
    "# print(\"y_train first row: {}\\n\".format(y_train[0]))\n",
    "\n",
    "# Creates a model with architecture 141 inputs -> [32 hidden nodes] -> 3 outputs\n",
    "# Create model.\n",
    "model = Sequential()\n",
    "model.add(Dense(141, \n",
    "                activation='relu', \n",
    "                input_dim=X_train.shape[1]))\n",
    "# Add dropout\n",
    "model.add(Dropout(0.5))\n",
    "#adding the second hidden layer\n",
    "#     model.add(Dense(50, \n",
    "#                    kernel_initializer ='uniform',\n",
    "#                    activation = 'relu'))\n",
    "# Adding output layer\n",
    "model.add(Dense(3,kernel_initializer=\"uniform\", \n",
    "                    activation=\"softmax\"))\n",
    "# Compile model \n",
    "model.compile(loss='sparse_categorical_crossentropy', \n",
    "                  optimizer='adam', \n",
    "                  metrics=['accuracy'])\n",
    "# Fit the model\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=150,\n",
    "                      batch_size=512,\n",
    "                      validation_split=0.2,\n",
    "                      shuffle=True,\n",
    "                   callbacks=[tb_callback, early_callback])\n",
    "\n",
    "# Score the model\n",
    "score = model.evaluate(X_test, y_test, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
