{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "from matplotlib.font_manager import _rebuild; _rebuild()\n",
    "#Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.io as spio\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_curve, auc\n",
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer, StandardScaler\n",
    "from yellowbrick.classifier import ROCAUC, PrecisionRecallCurve\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Animal chosen: ['fKH57']\n",
      "Iteration 1, loss = 0.65750467\n",
      "Iteration 2, loss = 0.45013103\n",
      "Iteration 3, loss = 0.37444231\n",
      "Iteration 4, loss = 0.31067258\n",
      "Iteration 5, loss = 0.27521384\n",
      "Iteration 6, loss = 0.24844033\n",
      "Iteration 7, loss = 0.23018489\n",
      "Iteration 8, loss = 0.21637280\n",
      "Iteration 9, loss = 0.20536754\n",
      "Iteration 10, loss = 0.19657076\n",
      "Iteration 11, loss = 0.18961847\n",
      "Iteration 12, loss = 0.18251589\n",
      "Iteration 13, loss = 0.17764845\n",
      "Iteration 14, loss = 0.17275601\n",
      "Iteration 15, loss = 0.16950659\n",
      "Iteration 16, loss = 0.16619124\n",
      "Iteration 17, loss = 0.16230164\n",
      "Iteration 18, loss = 0.15995792\n",
      "Iteration 19, loss = 0.15684458\n",
      "Iteration 20, loss = 0.15434485\n",
      "Iteration 21, loss = 0.15219670\n",
      "Iteration 22, loss = 0.14995774\n",
      "Iteration 23, loss = 0.14813810\n",
      "Iteration 24, loss = 0.14614845\n",
      "Iteration 25, loss = 0.14446575\n",
      "Iteration 26, loss = 0.14286856\n",
      "Iteration 27, loss = 0.14136312\n",
      "Iteration 28, loss = 0.14022607\n",
      "Iteration 29, loss = 0.13848927\n",
      "Iteration 30, loss = 0.13746857\n",
      "Iteration 31, loss = 0.13529212\n",
      "Iteration 32, loss = 0.13424693\n",
      "Iteration 33, loss = 0.13297187\n",
      "Iteration 34, loss = 0.13184000\n",
      "Iteration 35, loss = 0.13034078\n",
      "Iteration 36, loss = 0.12914409\n",
      "Iteration 37, loss = 0.12799247\n",
      "Iteration 38, loss = 0.12724213\n",
      "Iteration 39, loss = 0.12584126\n",
      "Iteration 40, loss = 0.12475985\n",
      "Iteration 41, loss = 0.12396145\n",
      "Iteration 42, loss = 0.12236108\n",
      "Iteration 43, loss = 0.12222462\n",
      "Iteration 44, loss = 0.12103476\n",
      "Iteration 45, loss = 0.11983513\n",
      "Iteration 46, loss = 0.12013152\n",
      "Iteration 47, loss = 0.11714165\n",
      "Iteration 48, loss = 0.11603021\n",
      "Iteration 49, loss = 0.11512583\n",
      "Iteration 50, loss = 0.11528224\n",
      "Iteration 51, loss = 0.11445686\n",
      "Iteration 52, loss = 0.11281088\n",
      "Iteration 53, loss = 0.11117802\n",
      "Iteration 54, loss = 0.11031804\n",
      "Iteration 55, loss = 0.11070189\n",
      "Iteration 56, loss = 0.10836845\n",
      "Iteration 57, loss = 0.10819257\n",
      "Iteration 58, loss = 0.10650794\n",
      "Iteration 59, loss = 0.10557721\n",
      "Iteration 60, loss = 0.10403810\n",
      "Iteration 61, loss = 0.10361678\n",
      "Iteration 62, loss = 0.10224857\n",
      "Iteration 63, loss = 0.10204531\n",
      "Iteration 64, loss = 0.10165476\n",
      "Iteration 65, loss = 0.10012858\n",
      "Iteration 66, loss = 0.09865905\n",
      "Iteration 67, loss = 0.09789626\n",
      "Iteration 68, loss = 0.09806492\n",
      "Iteration 69, loss = 0.09723287\n",
      "Iteration 70, loss = 0.09505353\n",
      "Iteration 71, loss = 0.09406014\n",
      "Iteration 72, loss = 0.09387878\n",
      "Iteration 73, loss = 0.09257638\n",
      "Iteration 74, loss = 0.09219863\n",
      "Iteration 75, loss = 0.09105127\n",
      "Iteration 76, loss = 0.09012360\n",
      "Iteration 77, loss = 0.08853325\n",
      "Iteration 78, loss = 0.08796527\n",
      "Iteration 79, loss = 0.08839115\n",
      "Iteration 80, loss = 0.08678296\n",
      "Iteration 81, loss = 0.08569840\n",
      "Iteration 82, loss = 0.08523004\n",
      "Iteration 83, loss = 0.08556719\n",
      "Iteration 84, loss = 0.08252457\n",
      "Iteration 85, loss = 0.08298608\n",
      "Iteration 86, loss = 0.08295300\n",
      "Iteration 87, loss = 0.08026213\n",
      "Iteration 88, loss = 0.08064315\n",
      "Iteration 89, loss = 0.07791074\n",
      "Iteration 90, loss = 0.07895147\n",
      "Iteration 91, loss = 0.07920419\n",
      "Iteration 92, loss = 0.07722136\n",
      "Iteration 93, loss = 0.07589054\n",
      "Iteration 94, loss = 0.07517814\n",
      "Iteration 95, loss = 0.07382119\n",
      "Iteration 96, loss = 0.07324182\n",
      "Iteration 97, loss = 0.07445657\n",
      "Iteration 98, loss = 0.07378882\n",
      "Iteration 99, loss = 0.07189501\n",
      "Iteration 100, loss = 0.07146257\n",
      "Iteration 101, loss = 0.07069474\n",
      "Iteration 102, loss = 0.06886607\n",
      "Iteration 103, loss = 0.06888652\n",
      "Iteration 104, loss = 0.06885912\n",
      "Iteration 105, loss = 0.06715159\n",
      "Iteration 106, loss = 0.06829489\n",
      "Iteration 107, loss = 0.06551917\n",
      "Iteration 108, loss = 0.06587092\n",
      "Iteration 109, loss = 0.06477367\n",
      "Iteration 110, loss = 0.06602969\n",
      "Iteration 111, loss = 0.06405978\n",
      "Iteration 112, loss = 0.06215767\n",
      "Iteration 113, loss = 0.06183482\n",
      "Iteration 114, loss = 0.06000635\n",
      "Iteration 115, loss = 0.05967760\n",
      "Iteration 116, loss = 0.05866165\n",
      "Iteration 117, loss = 0.05804571\n",
      "Iteration 118, loss = 0.05888120\n",
      "Iteration 119, loss = 0.05792286\n",
      "Iteration 120, loss = 0.05646744\n",
      "Iteration 121, loss = 0.05544246\n",
      "Iteration 122, loss = 0.05604267\n",
      "Iteration 123, loss = 0.05498897\n",
      "Iteration 124, loss = 0.05497103\n",
      "Iteration 125, loss = 0.05429676\n",
      "Iteration 126, loss = 0.05297052\n",
      "Iteration 127, loss = 0.05240724\n",
      "Iteration 128, loss = 0.05105409\n",
      "Iteration 129, loss = 0.05131937\n",
      "Iteration 130, loss = 0.05064723\n",
      "Iteration 131, loss = 0.05118718\n",
      "Iteration 132, loss = 0.05009091\n",
      "Iteration 133, loss = 0.04979489\n",
      "Iteration 134, loss = 0.04916175\n",
      "Iteration 135, loss = 0.04739201\n",
      "Iteration 136, loss = 0.04638876\n",
      "Iteration 137, loss = 0.04580434\n",
      "Iteration 138, loss = 0.04527611\n",
      "Iteration 139, loss = 0.04462607\n",
      "Iteration 140, loss = 0.04538008\n",
      "Iteration 141, loss = 0.04483748\n",
      "Iteration 142, loss = 0.04368258\n",
      "Iteration 143, loss = 0.04303847\n",
      "Iteration 144, loss = 0.04250892\n",
      "Iteration 145, loss = 0.04224330\n",
      "Iteration 146, loss = 0.04166119\n",
      "Iteration 147, loss = 0.04044288\n",
      "Iteration 148, loss = 0.03987322\n",
      "Iteration 149, loss = 0.03963847\n",
      "Iteration 150, loss = 0.03958333\n",
      "Iteration 151, loss = 0.03889080\n",
      "Iteration 152, loss = 0.03813183\n",
      "Iteration 153, loss = 0.03841584\n",
      "Iteration 154, loss = 0.03999288\n",
      "Iteration 155, loss = 0.03928898\n",
      "Iteration 156, loss = 0.03698123\n",
      "Iteration 157, loss = 0.03709357\n",
      "Iteration 158, loss = 0.03530771\n",
      "Iteration 159, loss = 0.03483718\n",
      "Iteration 160, loss = 0.03427324\n",
      "Iteration 161, loss = 0.03451734\n",
      "Iteration 162, loss = 0.03318923\n",
      "Iteration 163, loss = 0.03349896\n",
      "Iteration 164, loss = 0.03268955\n",
      "Iteration 165, loss = 0.03213236\n",
      "Iteration 166, loss = 0.03181938\n",
      "Iteration 167, loss = 0.03127447\n",
      "Iteration 168, loss = 0.03095160\n",
      "Iteration 169, loss = 0.03020149\n",
      "Iteration 170, loss = 0.02999809\n",
      "Iteration 171, loss = 0.02943018\n",
      "Iteration 172, loss = 0.02898856\n",
      "Iteration 173, loss = 0.02872878\n",
      "Iteration 174, loss = 0.02971592\n",
      "Iteration 175, loss = 0.02861046\n",
      "Iteration 176, loss = 0.02824322\n",
      "Iteration 177, loss = 0.02764159\n",
      "Iteration 178, loss = 0.02718204\n",
      "Iteration 179, loss = 0.02659558\n",
      "Iteration 180, loss = 0.02611809\n",
      "Iteration 181, loss = 0.02642109\n",
      "Iteration 182, loss = 0.02631760\n",
      "Iteration 183, loss = 0.02567380\n",
      "Iteration 184, loss = 0.02479403\n",
      "Iteration 185, loss = 0.02453715\n",
      "Iteration 186, loss = 0.02417370\n",
      "Iteration 187, loss = 0.02365995\n",
      "Iteration 188, loss = 0.02358199\n",
      "Iteration 189, loss = 0.02337892\n",
      "Iteration 190, loss = 0.02482898\n",
      "Iteration 191, loss = 0.02295769\n",
      "Iteration 192, loss = 0.02225068\n",
      "Iteration 193, loss = 0.02191275\n",
      "Iteration 194, loss = 0.02170445\n",
      "Iteration 195, loss = 0.02137701\n",
      "Iteration 196, loss = 0.02138430\n",
      "Iteration 197, loss = 0.02230158\n",
      "Iteration 198, loss = 0.02085749\n",
      "Iteration 199, loss = 0.02066802\n",
      "Iteration 200, loss = 0.02059058\n",
      "Iteration 201, loss = 0.01938734\n",
      "Iteration 202, loss = 0.01917774\n",
      "Iteration 203, loss = 0.01891118\n",
      "Iteration 204, loss = 0.01878418\n",
      "Iteration 205, loss = 0.01866190\n",
      "Iteration 206, loss = 0.01827241\n",
      "Iteration 207, loss = 0.01794632\n",
      "Iteration 208, loss = 0.01778777\n",
      "Iteration 209, loss = 0.01774006\n",
      "Iteration 210, loss = 0.01756794\n",
      "Iteration 211, loss = 0.01767225\n",
      "Iteration 212, loss = 0.01680138\n",
      "Iteration 213, loss = 0.01673714\n",
      "Iteration 214, loss = 0.01635097\n",
      "Iteration 215, loss = 0.01630429\n",
      "Iteration 216, loss = 0.01594885\n",
      "Iteration 217, loss = 0.01560937\n",
      "Iteration 218, loss = 0.01541532\n",
      "Iteration 219, loss = 0.01528442\n",
      "Iteration 220, loss = 0.01512204\n",
      "Iteration 221, loss = 0.01491769\n",
      "Iteration 222, loss = 0.01454127\n",
      "Iteration 223, loss = 0.01436026\n",
      "Iteration 224, loss = 0.01464674\n",
      "Iteration 225, loss = 0.01399372\n",
      "Iteration 226, loss = 0.01387194\n",
      "Iteration 227, loss = 0.01371988\n",
      "Iteration 228, loss = 0.01368506\n",
      "Iteration 229, loss = 0.01338227\n",
      "Iteration 230, loss = 0.01312895\n",
      "Iteration 231, loss = 0.01289868\n",
      "Iteration 232, loss = 0.01275048\n",
      "Iteration 233, loss = 0.01257704\n",
      "Iteration 234, loss = 0.01252227\n",
      "Iteration 235, loss = 0.01260073\n",
      "Iteration 236, loss = 0.01218957\n",
      "Iteration 237, loss = 0.01223462\n",
      "Iteration 238, loss = 0.01196424\n",
      "Iteration 239, loss = 0.01184057\n",
      "Iteration 240, loss = 0.01151772\n",
      "Iteration 241, loss = 0.01146251\n",
      "Iteration 242, loss = 0.01138033\n",
      "Iteration 243, loss = 0.01097526\n",
      "Iteration 244, loss = 0.01092173\n",
      "Iteration 245, loss = 0.01086543\n",
      "Iteration 246, loss = 0.01085131\n",
      "Iteration 247, loss = 0.01070298\n",
      "Iteration 248, loss = 0.01063781\n",
      "Iteration 249, loss = 0.01027450\n",
      "Iteration 250, loss = 0.01006699\n",
      "Iteration 251, loss = 0.00993489\n",
      "Iteration 252, loss = 0.00992901\n",
      "Iteration 253, loss = 0.00965555\n",
      "Iteration 254, loss = 0.00963498\n",
      "Iteration 255, loss = 0.00950082\n",
      "Iteration 256, loss = 0.00940052\n",
      "Iteration 257, loss = 0.00921920\n",
      "Iteration 258, loss = 0.00917827\n",
      "Iteration 259, loss = 0.00919121\n",
      "Iteration 260, loss = 0.00922352\n",
      "Iteration 261, loss = 0.00889488\n",
      "Iteration 262, loss = 0.00873504\n",
      "Iteration 263, loss = 0.00863438\n",
      "Iteration 264, loss = 0.00855687\n",
      "Iteration 265, loss = 0.00851719\n",
      "Iteration 266, loss = 0.00837898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 267, loss = 0.00832414\n",
      "Iteration 268, loss = 0.00829575\n",
      "Iteration 269, loss = 0.00857309\n",
      "Iteration 270, loss = 0.00812751\n",
      "Iteration 271, loss = 0.00803454\n",
      "Iteration 272, loss = 0.00769131\n",
      "Iteration 273, loss = 0.00763030\n",
      "Iteration 274, loss = 0.00754825\n",
      "Iteration 275, loss = 0.00744706\n",
      "Iteration 276, loss = 0.00758355\n",
      "Iteration 277, loss = 0.00724081\n",
      "Iteration 278, loss = 0.00723658\n",
      "Iteration 279, loss = 0.00706895\n",
      "Iteration 280, loss = 0.00708821\n",
      "Iteration 281, loss = 0.00687518\n",
      "Iteration 282, loss = 0.00689406\n",
      "Iteration 283, loss = 0.00683290\n",
      "Iteration 284, loss = 0.00666819\n",
      "Iteration 285, loss = 0.00658856\n",
      "Iteration 286, loss = 0.00658569\n",
      "Iteration 287, loss = 0.00644916\n",
      "Iteration 288, loss = 0.00640039\n",
      "Iteration 289, loss = 0.00631145\n",
      "Iteration 290, loss = 0.00637918\n",
      "Iteration 291, loss = 0.00623899\n",
      "Iteration 292, loss = 0.00612378\n",
      "Iteration 293, loss = 0.00601917\n",
      "Iteration 294, loss = 0.00595216\n",
      "Iteration 295, loss = 0.00610549\n",
      "Iteration 296, loss = 0.00595061\n",
      "Iteration 297, loss = 0.00576354\n",
      "Iteration 298, loss = 0.00571389\n",
      "Iteration 299, loss = 0.00565830\n",
      "Iteration 300, loss = 0.00561475\n",
      "Iteration 301, loss = 0.00558813\n",
      "Iteration 302, loss = 0.00549161\n",
      "Iteration 303, loss = 0.00543769\n",
      "Iteration 304, loss = 0.00540521\n",
      "Iteration 305, loss = 0.00539053\n",
      "Iteration 306, loss = 0.00525001\n",
      "Iteration 307, loss = 0.00517562\n",
      "Iteration 308, loss = 0.00517931\n",
      "Iteration 309, loss = 0.00511236\n",
      "Iteration 310, loss = 0.00503753\n",
      "Iteration 311, loss = 0.00498625\n",
      "Iteration 312, loss = 0.00492594\n",
      "Iteration 313, loss = 0.00487047\n",
      "Iteration 314, loss = 0.00486921\n",
      "Iteration 315, loss = 0.00477336\n",
      "Iteration 316, loss = 0.00499736\n",
      "Iteration 317, loss = 0.00472003\n",
      "Iteration 318, loss = 0.00463096\n",
      "Iteration 319, loss = 0.00460757\n",
      "Iteration 320, loss = 0.00454828\n",
      "Iteration 321, loss = 0.00453776\n",
      "Iteration 322, loss = 0.00447646\n",
      "Iteration 323, loss = 0.00449271\n",
      "Iteration 324, loss = 0.00437652\n",
      "Iteration 325, loss = 0.00433052\n",
      "Iteration 326, loss = 0.00429538\n",
      "Iteration 327, loss = 0.00422725\n",
      "Iteration 328, loss = 0.00419966\n",
      "Iteration 329, loss = 0.00428207\n",
      "Iteration 330, loss = 0.00416965\n",
      "Iteration 331, loss = 0.00406364\n",
      "Iteration 332, loss = 0.00403366\n",
      "Iteration 333, loss = 0.00399288\n",
      "Iteration 334, loss = 0.00394891\n",
      "Iteration 335, loss = 0.00395889\n",
      "Iteration 336, loss = 0.00390169\n",
      "Iteration 337, loss = 0.00384266\n",
      "Iteration 338, loss = 0.00382038\n",
      "Iteration 339, loss = 0.00388858\n",
      "Iteration 340, loss = 0.00374570\n",
      "Iteration 341, loss = 0.00371517\n",
      "Iteration 342, loss = 0.00371201\n",
      "Iteration 343, loss = 0.00364702\n",
      "Iteration 344, loss = 0.00363619\n",
      "Iteration 345, loss = 0.00368987\n",
      "Iteration 346, loss = 0.00355901\n",
      "Iteration 347, loss = 0.00356459\n",
      "Iteration 348, loss = 0.00353629\n",
      "Iteration 349, loss = 0.00350979\n",
      "Iteration 350, loss = 0.00346841\n",
      "Iteration 351, loss = 0.00341892\n",
      "Iteration 352, loss = 0.00343416\n",
      "Iteration 353, loss = 0.00335988\n",
      "Iteration 354, loss = 0.00331717\n",
      "Iteration 355, loss = 0.00330310\n",
      "Iteration 356, loss = 0.00326555\n",
      "Iteration 357, loss = 0.00330834\n",
      "Iteration 358, loss = 0.00328762\n",
      "Iteration 359, loss = 0.00319650\n",
      "Iteration 360, loss = 0.00316676\n",
      "Iteration 361, loss = 0.00313390\n",
      "Iteration 362, loss = 0.00314332\n",
      "Iteration 363, loss = 0.00306961\n",
      "Iteration 364, loss = 0.00305007\n",
      "Iteration 365, loss = 0.00304922\n",
      "Iteration 366, loss = 0.00300501\n",
      "Iteration 367, loss = 0.00299625\n",
      "Iteration 368, loss = 0.00298753\n",
      "Iteration 369, loss = 0.00297415\n",
      "Iteration 370, loss = 0.00290899\n",
      "Iteration 371, loss = 0.00289639\n",
      "Iteration 372, loss = 0.00285346\n",
      "Iteration 373, loss = 0.00287647\n",
      "Iteration 374, loss = 0.00288708\n",
      "Iteration 375, loss = 0.00281598\n",
      "Iteration 376, loss = 0.00275345\n",
      "Iteration 377, loss = 0.00276115\n",
      "Iteration 378, loss = 0.00278496\n",
      "Iteration 379, loss = 0.00278464\n",
      "Iteration 380, loss = 0.00274609\n",
      "Iteration 381, loss = 0.00273491\n",
      "Iteration 382, loss = 0.00267890\n",
      "Iteration 383, loss = 0.00264787\n",
      "Iteration 384, loss = 0.00258461\n",
      "Iteration 385, loss = 0.00257464\n",
      "Iteration 386, loss = 0.00257006\n",
      "Iteration 387, loss = 0.00253890\n",
      "Iteration 388, loss = 0.00251773\n",
      "Iteration 389, loss = 0.00250787\n",
      "Iteration 390, loss = 0.00248926\n",
      "Iteration 391, loss = 0.00246731\n",
      "Iteration 392, loss = 0.00244980\n",
      "Iteration 393, loss = 0.00243919\n",
      "Iteration 394, loss = 0.00240779\n",
      "Iteration 395, loss = 0.00238876\n",
      "Iteration 396, loss = 0.00238184\n",
      "Iteration 397, loss = 0.00237675\n",
      "Iteration 398, loss = 0.00234886\n",
      "Iteration 399, loss = 0.00232013\n",
      "Iteration 400, loss = 0.00235180\n",
      "Iteration 401, loss = 0.00231261\n",
      "Iteration 402, loss = 0.00228473\n",
      "Iteration 403, loss = 0.00226938\n",
      "Iteration 404, loss = 0.00224021\n",
      "Iteration 405, loss = 0.00224229\n",
      "Iteration 406, loss = 0.00221536\n",
      "Iteration 407, loss = 0.00221251\n",
      "Iteration 408, loss = 0.00219149\n",
      "Iteration 409, loss = 0.00218245\n",
      "Iteration 410, loss = 0.00216152\n",
      "Iteration 411, loss = 0.00214387\n",
      "Iteration 412, loss = 0.00212893\n",
      "Iteration 413, loss = 0.00211737\n",
      "Iteration 414, loss = 0.00210384\n",
      "Iteration 415, loss = 0.00209259\n",
      "Iteration 416, loss = 0.00207750\n",
      "Iteration 417, loss = 0.00207062\n",
      "Iteration 418, loss = 0.00208793\n",
      "Iteration 419, loss = 0.00204961\n",
      "Iteration 420, loss = 0.00202488\n",
      "Iteration 421, loss = 0.00202981\n",
      "Iteration 422, loss = 0.00201068\n",
      "Iteration 423, loss = 0.00199334\n",
      "Iteration 424, loss = 0.00197494\n",
      "Iteration 425, loss = 0.00198753\n",
      "Iteration 426, loss = 0.00195890\n",
      "Iteration 427, loss = 0.00196286\n",
      "Iteration 428, loss = 0.00193379\n",
      "Iteration 429, loss = 0.00191443\n",
      "Iteration 430, loss = 0.00190614\n",
      "Iteration 431, loss = 0.00190528\n",
      "Iteration 432, loss = 0.00188799\n",
      "Iteration 433, loss = 0.00187665\n",
      "Iteration 434, loss = 0.00186392\n",
      "Iteration 435, loss = 0.00185557\n",
      "Iteration 436, loss = 0.00185059\n",
      "Iteration 437, loss = 0.00184163\n",
      "Iteration 438, loss = 0.00182776\n",
      "Iteration 439, loss = 0.00180416\n",
      "Iteration 440, loss = 0.00180269\n",
      "Iteration 441, loss = 0.00180176\n",
      "Iteration 442, loss = 0.00177328\n",
      "Iteration 443, loss = 0.00175702\n",
      "Iteration 444, loss = 0.00175149\n",
      "Iteration 445, loss = 0.00174327\n",
      "Iteration 446, loss = 0.00173563\n",
      "Iteration 447, loss = 0.00172613\n",
      "Iteration 448, loss = 0.00171205\n",
      "Iteration 449, loss = 0.00170524\n",
      "Iteration 450, loss = 0.00168947\n",
      "Iteration 451, loss = 0.00168874\n",
      "Iteration 452, loss = 0.00167756\n",
      "Iteration 453, loss = 0.00167309\n",
      "Iteration 454, loss = 0.00165881\n",
      "Iteration 455, loss = 0.00164935\n",
      "Iteration 456, loss = 0.00163816\n",
      "Iteration 457, loss = 0.00163693\n",
      "Iteration 458, loss = 0.00163393\n",
      "Iteration 459, loss = 0.00161246\n",
      "Iteration 460, loss = 0.00160522\n",
      "Iteration 461, loss = 0.00160472\n",
      "Iteration 462, loss = 0.00159254\n",
      "Iteration 463, loss = 0.00157949\n",
      "Iteration 464, loss = 0.00158151\n",
      "Iteration 465, loss = 0.00157269\n",
      "Iteration 466, loss = 0.00156846\n",
      "Iteration 467, loss = 0.00154990\n",
      "Iteration 468, loss = 0.00154035\n",
      "Iteration 469, loss = 0.00153045\n",
      "Iteration 470, loss = 0.00152336\n",
      "Iteration 471, loss = 0.00152048\n",
      "Iteration 472, loss = 0.00150896\n",
      "Iteration 473, loss = 0.00150544\n",
      "Iteration 474, loss = 0.00149855\n",
      "Iteration 475, loss = 0.00149283\n",
      "Iteration 476, loss = 0.00148007\n",
      "Iteration 477, loss = 0.00147639\n",
      "Iteration 478, loss = 0.00146910\n",
      "Iteration 479, loss = 0.00145918\n",
      "Iteration 480, loss = 0.00145955\n",
      "Iteration 481, loss = 0.00144646\n",
      "Iteration 482, loss = 0.00144203\n",
      "Iteration 483, loss = 0.00143933\n",
      "Iteration 484, loss = 0.00142814\n",
      "Iteration 485, loss = 0.00142309\n",
      "Iteration 486, loss = 0.00142216\n",
      "Iteration 487, loss = 0.00141878\n",
      "Iteration 488, loss = 0.00140911\n",
      "Iteration 489, loss = 0.00139680\n",
      "Iteration 490, loss = 0.00139213\n",
      "Iteration 491, loss = 0.00138565\n",
      "Iteration 492, loss = 0.00137674\n",
      "Iteration 493, loss = 0.00137206\n",
      "Iteration 494, loss = 0.00136237\n",
      "Iteration 495, loss = 0.00136088\n",
      "Iteration 496, loss = 0.00135635\n",
      "Iteration 497, loss = 0.00136001\n",
      "Iteration 498, loss = 0.00134060\n",
      "Iteration 499, loss = 0.00133366\n",
      "Iteration 500, loss = 0.00132928\n",
      "Iteration 501, loss = 0.00133028\n",
      "Iteration 502, loss = 0.00131884\n",
      "Iteration 503, loss = 0.00131275\n",
      "Iteration 504, loss = 0.00130678\n",
      "Iteration 505, loss = 0.00130312\n",
      "Iteration 506, loss = 0.00129803\n",
      "Iteration 507, loss = 0.00129604\n",
      "Iteration 508, loss = 0.00129378\n",
      "Iteration 509, loss = 0.00128957\n",
      "Iteration 510, loss = 0.00127759\n",
      "Iteration 511, loss = 0.00127744\n",
      "Iteration 512, loss = 0.00127165\n",
      "Iteration 513, loss = 0.00126118\n",
      "Iteration 514, loss = 0.00126137\n",
      "Iteration 515, loss = 0.00125458\n",
      "Iteration 516, loss = 0.00124861\n",
      "Iteration 517, loss = 0.00123879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 518, loss = 0.00123867\n",
      "Iteration 519, loss = 0.00123818\n",
      "Iteration 520, loss = 0.00122463\n",
      "Iteration 521, loss = 0.00122260\n",
      "Iteration 522, loss = 0.00121979\n",
      "Iteration 523, loss = 0.00121114\n",
      "Iteration 524, loss = 0.00120757\n",
      "Iteration 525, loss = 0.00120970\n",
      "Iteration 526, loss = 0.00121083\n",
      "Iteration 527, loss = 0.00120173\n",
      "Iteration 528, loss = 0.00119001\n",
      "Iteration 529, loss = 0.00119337\n",
      "Iteration 530, loss = 0.00119030\n",
      "Iteration 531, loss = 0.00118244\n",
      "Iteration 532, loss = 0.00117388\n",
      "Iteration 533, loss = 0.00116976\n",
      "Iteration 534, loss = 0.00116544\n",
      "Iteration 535, loss = 0.00116656\n",
      "Iteration 536, loss = 0.00115953\n",
      "Iteration 537, loss = 0.00115789\n",
      "Iteration 538, loss = 0.00115370\n",
      "Iteration 539, loss = 0.00114684\n",
      "Iteration 540, loss = 0.00114687\n",
      "Iteration 541, loss = 0.00114290\n",
      "Iteration 542, loss = 0.00113400\n",
      "Iteration 543, loss = 0.00112897\n",
      "Iteration 544, loss = 0.00112716\n",
      "Iteration 545, loss = 0.00112357\n",
      "Iteration 546, loss = 0.00112089\n",
      "Iteration 547, loss = 0.00112332\n",
      "Iteration 548, loss = 0.00111838\n",
      "Iteration 549, loss = 0.00111089\n",
      "Iteration 550, loss = 0.00111002\n",
      "Iteration 551, loss = 0.00110246\n",
      "Iteration 552, loss = 0.00111085\n",
      "Iteration 553, loss = 0.00109825\n",
      "Iteration 554, loss = 0.00109217\n",
      "Iteration 555, loss = 0.00108686\n",
      "Iteration 556, loss = 0.00108406\n",
      "Iteration 557, loss = 0.00108160\n",
      "Iteration 558, loss = 0.00107738\n",
      "Iteration 559, loss = 0.00107783\n",
      "Iteration 560, loss = 0.00107112\n",
      "Iteration 561, loss = 0.00107217\n",
      "Iteration 562, loss = 0.00106934\n",
      "Iteration 563, loss = 0.00106230\n",
      "Iteration 564, loss = 0.00105821\n",
      "Iteration 565, loss = 0.00105479\n",
      "Iteration 566, loss = 0.00105112\n",
      "Iteration 567, loss = 0.00105202\n",
      "Iteration 568, loss = 0.00105102\n",
      "Iteration 569, loss = 0.00104125\n",
      "Iteration 570, loss = 0.00104120\n",
      "Iteration 571, loss = 0.00103820\n",
      "Iteration 572, loss = 0.00103286\n",
      "Iteration 573, loss = 0.00103003\n",
      "Iteration 574, loss = 0.00102644\n",
      "Iteration 575, loss = 0.00102450\n",
      "Iteration 576, loss = 0.00102405\n",
      "Iteration 577, loss = 0.00103015\n",
      "Iteration 578, loss = 0.00101817\n",
      "Iteration 579, loss = 0.00101405\n",
      "Iteration 580, loss = 0.00101091\n",
      "Iteration 581, loss = 0.00100867\n",
      "Iteration 582, loss = 0.00100601\n",
      "Iteration 583, loss = 0.00100327\n",
      "Iteration 584, loss = 0.00100211\n",
      "Iteration 585, loss = 0.00099613\n",
      "Iteration 586, loss = 0.00099511\n",
      "Iteration 587, loss = 0.00099697\n",
      "Iteration 588, loss = 0.00099540\n",
      "Iteration 589, loss = 0.00098751\n",
      "Iteration 590, loss = 0.00098580\n",
      "Iteration 591, loss = 0.00098328\n",
      "Iteration 592, loss = 0.00098329\n",
      "Iteration 593, loss = 0.00097910\n",
      "Iteration 594, loss = 0.00097642\n",
      "Iteration 595, loss = 0.00097681\n",
      "Iteration 596, loss = 0.00097676\n",
      "Iteration 597, loss = 0.00096594\n",
      "Iteration 598, loss = 0.00096491\n",
      "Iteration 599, loss = 0.00096252\n",
      "Iteration 600, loss = 0.00095938\n",
      "Iteration 601, loss = 0.00095865\n",
      "Iteration 602, loss = 0.00095423\n",
      "Iteration 603, loss = 0.00095503\n",
      "Iteration 604, loss = 0.00095297\n",
      "Iteration 605, loss = 0.00094811\n",
      "Iteration 606, loss = 0.00094616\n",
      "Iteration 607, loss = 0.00094377\n",
      "Iteration 608, loss = 0.00094293\n",
      "Iteration 609, loss = 0.00094441\n",
      "Iteration 610, loss = 0.00094271\n",
      "Iteration 611, loss = 0.00093673\n",
      "Iteration 612, loss = 0.00093903\n",
      "Iteration 613, loss = 0.00093223\n",
      "Iteration 614, loss = 0.00092889\n",
      "Iteration 615, loss = 0.00092612\n",
      "Iteration 616, loss = 0.00092390\n",
      "Iteration 617, loss = 0.00092184\n",
      "Iteration 618, loss = 0.00092046\n",
      "Iteration 619, loss = 0.00092164\n",
      "Iteration 620, loss = 0.00091697\n",
      "Iteration 621, loss = 0.00091890\n",
      "Iteration 622, loss = 0.00091896\n",
      "Iteration 623, loss = 0.00090905\n",
      "Iteration 624, loss = 0.00090745\n",
      "Iteration 625, loss = 0.00090651\n",
      "Iteration 626, loss = 0.00090551\n",
      "Iteration 627, loss = 0.00090231\n",
      "Iteration 628, loss = 0.00090132\n",
      "Iteration 629, loss = 0.00090417\n",
      "Iteration 630, loss = 0.00089830\n",
      "Iteration 631, loss = 0.00089838\n",
      "Iteration 632, loss = 0.00090063\n",
      "Iteration 633, loss = 0.00089302\n",
      "Iteration 634, loss = 0.00089776\n",
      "Iteration 635, loss = 0.00088926\n",
      "Iteration 636, loss = 0.00088613\n",
      "Iteration 637, loss = 0.00088518\n",
      "Iteration 638, loss = 0.00088667\n",
      "Iteration 639, loss = 0.00088909\n",
      "Iteration 640, loss = 0.00088092\n",
      "Iteration 641, loss = 0.00087897\n",
      "Iteration 642, loss = 0.00087580\n",
      "Iteration 643, loss = 0.00087515\n",
      "Iteration 644, loss = 0.00087620\n",
      "Iteration 645, loss = 0.00087469\n",
      "Iteration 646, loss = 0.00086973\n",
      "Iteration 647, loss = 0.00086906\n",
      "Iteration 648, loss = 0.00087075\n",
      "Iteration 649, loss = 0.00086687\n",
      "Iteration 650, loss = 0.00086448\n",
      "Iteration 651, loss = 0.00086376\n",
      "Iteration 652, loss = 0.00085907\n",
      "Iteration 653, loss = 0.00085774\n",
      "Iteration 654, loss = 0.00085778\n",
      "Iteration 655, loss = 0.00085576\n",
      "Iteration 656, loss = 0.00085515\n",
      "Iteration 657, loss = 0.00085673\n",
      "Iteration 658, loss = 0.00085180\n",
      "Iteration 659, loss = 0.00084933\n",
      "Iteration 660, loss = 0.00084703\n",
      "Iteration 661, loss = 0.00084618\n",
      "Iteration 662, loss = 0.00084477\n",
      "Iteration 663, loss = 0.00084249\n",
      "Iteration 664, loss = 0.00084232\n",
      "Iteration 665, loss = 0.00083967\n",
      "Iteration 666, loss = 0.00083924\n",
      "Iteration 667, loss = 0.00083679\n",
      "Iteration 668, loss = 0.00083658\n",
      "Iteration 669, loss = 0.00083703\n",
      "Iteration 670, loss = 0.00083721\n",
      "Iteration 671, loss = 0.00083328\n",
      "Iteration 672, loss = 0.00083404\n",
      "Iteration 673, loss = 0.00083038\n",
      "Iteration 674, loss = 0.00082972\n",
      "Iteration 675, loss = 0.00082669\n",
      "Iteration 676, loss = 0.00082501\n",
      "Iteration 677, loss = 0.00082619\n",
      "Iteration 678, loss = 0.00082203\n",
      "Iteration 679, loss = 0.00082531\n",
      "Iteration 680, loss = 0.00082735\n",
      "Iteration 681, loss = 0.00082844\n",
      "Iteration 682, loss = 0.00081855\n",
      "Iteration 683, loss = 0.00082282\n",
      "Iteration 684, loss = 0.00081382\n",
      "Iteration 685, loss = 0.00081370\n",
      "Iteration 686, loss = 0.00081208\n",
      "Iteration 687, loss = 0.00080990\n",
      "Iteration 688, loss = 0.00080968\n",
      "Iteration 689, loss = 0.00081101\n",
      "Iteration 690, loss = 0.00080942\n",
      "Iteration 691, loss = 0.00080661\n",
      "Iteration 692, loss = 0.00080426\n",
      "Iteration 693, loss = 0.00080548\n",
      "Iteration 694, loss = 0.00080422\n",
      "Iteration 695, loss = 0.00080303\n",
      "Iteration 696, loss = 0.00079959\n",
      "Iteration 697, loss = 0.00079898\n",
      "Iteration 698, loss = 0.00079721\n",
      "Iteration 699, loss = 0.00079595\n",
      "Iteration 700, loss = 0.00079522\n",
      "Iteration 701, loss = 0.00079384\n",
      "Iteration 702, loss = 0.00079300\n",
      "Iteration 703, loss = 0.00079197\n",
      "Iteration 704, loss = 0.00079177\n",
      "Iteration 705, loss = 0.00079066\n",
      "Iteration 706, loss = 0.00079131\n",
      "Iteration 707, loss = 0.00079090\n",
      "Iteration 708, loss = 0.00078603\n",
      "Iteration 709, loss = 0.00078502\n",
      "Iteration 710, loss = 0.00078509\n",
      "Iteration 711, loss = 0.00078243\n",
      "Iteration 712, loss = 0.00078207\n",
      "Iteration 713, loss = 0.00077994\n",
      "Iteration 714, loss = 0.00077839\n",
      "Iteration 715, loss = 0.00077866\n",
      "Iteration 716, loss = 0.00077926\n",
      "Iteration 717, loss = 0.00078013\n",
      "Iteration 718, loss = 0.00077589\n",
      "Iteration 719, loss = 0.00077468\n",
      "Iteration 720, loss = 0.00077419\n",
      "Iteration 721, loss = 0.00077164\n",
      "Iteration 722, loss = 0.00077048\n",
      "Iteration 723, loss = 0.00077013\n",
      "Iteration 724, loss = 0.00076965\n",
      "Iteration 725, loss = 0.00076966\n",
      "Iteration 726, loss = 0.00076870\n",
      "Iteration 727, loss = 0.00076738\n",
      "Iteration 728, loss = 0.00076482\n",
      "Iteration 729, loss = 0.00076335\n",
      "Iteration 730, loss = 0.00076301\n",
      "Iteration 731, loss = 0.00076255\n",
      "Iteration 732, loss = 0.00076525\n",
      "Iteration 733, loss = 0.00076884\n",
      "Iteration 734, loss = 0.00075917\n",
      "Iteration 735, loss = 0.00075926\n",
      "Iteration 736, loss = 0.00075736\n",
      "Iteration 737, loss = 0.00075677\n",
      "Iteration 738, loss = 0.00075452\n",
      "Iteration 739, loss = 0.00075367\n",
      "Iteration 740, loss = 0.00075386\n",
      "Iteration 741, loss = 0.00075288\n",
      "Iteration 742, loss = 0.00075070\n",
      "Iteration 743, loss = 0.00075055\n",
      "Iteration 744, loss = 0.00074892\n",
      "Iteration 745, loss = 0.00075185\n",
      "Iteration 746, loss = 0.00074748\n",
      "Iteration 747, loss = 0.00074604\n",
      "Iteration 748, loss = 0.00074589\n",
      "Iteration 749, loss = 0.00074447\n",
      "Iteration 750, loss = 0.00074358\n",
      "Iteration 751, loss = 0.00074205\n",
      "Iteration 752, loss = 0.00074253\n",
      "Iteration 753, loss = 0.00074095\n",
      "Iteration 754, loss = 0.00074047\n",
      "Iteration 755, loss = 0.00073908\n",
      "Iteration 756, loss = 0.00073805\n",
      "Iteration 757, loss = 0.00073891\n",
      "Iteration 758, loss = 0.00073661\n",
      "Iteration 759, loss = 0.00073567\n",
      "Iteration 760, loss = 0.00073383\n",
      "Iteration 761, loss = 0.00073473\n",
      "Iteration 762, loss = 0.00073591\n",
      "Iteration 763, loss = 0.00073373\n",
      "Iteration 764, loss = 0.00073153\n",
      "Iteration 765, loss = 0.00073190\n",
      "Iteration 766, loss = 0.00072925\n",
      "Iteration 767, loss = 0.00072878\n",
      "Iteration 768, loss = 0.00072735\n",
      "Iteration 769, loss = 0.00072682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 770, loss = 0.00072864\n",
      "Iteration 771, loss = 0.00072715\n",
      "Iteration 772, loss = 0.00072394\n",
      "Iteration 773, loss = 0.00072337\n",
      "Iteration 774, loss = 0.00072251\n",
      "Iteration 775, loss = 0.00072369\n",
      "Iteration 776, loss = 0.00072030\n",
      "Iteration 777, loss = 0.00071989\n",
      "Iteration 778, loss = 0.00071876\n",
      "Iteration 779, loss = 0.00071906\n",
      "Iteration 780, loss = 0.00071717\n",
      "Iteration 781, loss = 0.00071820\n",
      "Iteration 782, loss = 0.00071559\n",
      "Iteration 783, loss = 0.00071484\n",
      "Iteration 784, loss = 0.00071483\n",
      "Iteration 785, loss = 0.00071377\n",
      "Iteration 786, loss = 0.00071251\n",
      "Iteration 787, loss = 0.00071369\n",
      "Iteration 788, loss = 0.00071186\n",
      "Iteration 789, loss = 0.00071054\n",
      "Iteration 790, loss = 0.00070929\n",
      "Iteration 791, loss = 0.00070833\n",
      "Iteration 792, loss = 0.00070852\n",
      "Iteration 793, loss = 0.00070728\n",
      "Iteration 794, loss = 0.00070700\n",
      "Iteration 795, loss = 0.00070519\n",
      "Iteration 796, loss = 0.00070509\n",
      "Iteration 797, loss = 0.00070493\n",
      "Iteration 798, loss = 0.00070355\n",
      "Iteration 799, loss = 0.00070525\n",
      "Iteration 800, loss = 0.00070430\n",
      "Iteration 801, loss = 0.00070125\n",
      "Iteration 802, loss = 0.00069999\n",
      "Iteration 803, loss = 0.00069948\n",
      "Iteration 804, loss = 0.00069927\n",
      "Iteration 805, loss = 0.00069807\n",
      "Iteration 806, loss = 0.00069717\n",
      "Iteration 807, loss = 0.00069677\n",
      "Iteration 808, loss = 0.00069689\n",
      "Iteration 809, loss = 0.00069405\n",
      "Iteration 810, loss = 0.00069380\n",
      "Iteration 811, loss = 0.00069336\n",
      "Iteration 812, loss = 0.00069226\n",
      "Iteration 813, loss = 0.00069180\n",
      "Iteration 814, loss = 0.00069083\n",
      "Iteration 815, loss = 0.00069209\n",
      "Iteration 816, loss = 0.00068923\n",
      "Iteration 817, loss = 0.00068900\n",
      "Iteration 818, loss = 0.00068964\n",
      "Iteration 819, loss = 0.00068767\n",
      "Iteration 820, loss = 0.00068645\n",
      "Iteration 821, loss = 0.00068703\n",
      "Iteration 822, loss = 0.00068593\n",
      "Iteration 823, loss = 0.00068423\n",
      "Iteration 824, loss = 0.00068432\n",
      "Iteration 825, loss = 0.00068347\n",
      "Iteration 826, loss = 0.00068242\n",
      "Iteration 827, loss = 0.00068270\n",
      "Iteration 828, loss = 0.00068136\n",
      "Iteration 829, loss = 0.00068197\n",
      "Iteration 830, loss = 0.00068101\n",
      "Iteration 831, loss = 0.00068037\n",
      "Iteration 832, loss = 0.00067837\n",
      "Iteration 833, loss = 0.00067705\n",
      "Iteration 834, loss = 0.00067648\n",
      "Iteration 835, loss = 0.00067629\n",
      "Iteration 836, loss = 0.00067755\n",
      "Iteration 837, loss = 0.00067583\n",
      "Iteration 838, loss = 0.00067383\n",
      "Iteration 839, loss = 0.00067684\n",
      "Iteration 840, loss = 0.00067840\n",
      "Iteration 841, loss = 0.00067379\n",
      "Iteration 842, loss = 0.00067198\n",
      "Iteration 843, loss = 0.00066937\n",
      "Iteration 844, loss = 0.00066963\n",
      "Iteration 845, loss = 0.00067108\n",
      "Iteration 846, loss = 0.00066783\n",
      "Iteration 847, loss = 0.00066755\n",
      "Iteration 848, loss = 0.00066583\n",
      "Iteration 849, loss = 0.00066487\n",
      "Iteration 850, loss = 0.00066502\n",
      "Iteration 851, loss = 0.00066435\n",
      "Iteration 852, loss = 0.00066456\n",
      "Iteration 853, loss = 0.00066280\n",
      "Iteration 854, loss = 0.00066401\n",
      "Iteration 855, loss = 0.00066261\n",
      "Iteration 856, loss = 0.00066287\n",
      "Iteration 857, loss = 0.00065976\n",
      "Iteration 858, loss = 0.00065992\n",
      "Iteration 859, loss = 0.00065831\n",
      "Iteration 860, loss = 0.00065854\n",
      "Iteration 861, loss = 0.00065955\n",
      "Iteration 862, loss = 0.00065602\n",
      "Iteration 863, loss = 0.00065677\n",
      "Iteration 864, loss = 0.00065463\n",
      "Iteration 865, loss = 0.00065373\n",
      "Iteration 866, loss = 0.00065393\n",
      "Iteration 867, loss = 0.00065384\n",
      "Iteration 868, loss = 0.00065362\n",
      "Iteration 869, loss = 0.00065165\n",
      "Iteration 870, loss = 0.00065045\n",
      "Iteration 871, loss = 0.00064954\n",
      "Iteration 872, loss = 0.00064886\n",
      "Iteration 873, loss = 0.00064890\n",
      "Iteration 874, loss = 0.00064749\n",
      "Iteration 875, loss = 0.00064796\n",
      "Iteration 876, loss = 0.00064634\n",
      "Iteration 877, loss = 0.00064698\n",
      "Iteration 878, loss = 0.00064498\n",
      "Iteration 879, loss = 0.00064503\n",
      "Iteration 880, loss = 0.00065150\n",
      "Iteration 881, loss = 0.00064608\n",
      "Iteration 882, loss = 0.00064443\n",
      "Iteration 883, loss = 0.00064294\n",
      "Iteration 884, loss = 0.00064125\n",
      "Iteration 885, loss = 0.00064172\n",
      "Iteration 886, loss = 0.00064122\n",
      "Iteration 887, loss = 0.00064194\n",
      "Iteration 888, loss = 0.00063948\n",
      "Iteration 889, loss = 0.00063782\n",
      "Iteration 890, loss = 0.00063878\n",
      "Iteration 891, loss = 0.00063712\n",
      "Iteration 892, loss = 0.00063666\n",
      "Iteration 893, loss = 0.00063581\n",
      "Iteration 894, loss = 0.00063463\n",
      "Iteration 895, loss = 0.00063418\n",
      "Iteration 896, loss = 0.00063259\n",
      "Iteration 897, loss = 0.00063264\n",
      "Iteration 898, loss = 0.00063240\n",
      "Iteration 899, loss = 0.00063272\n",
      "Iteration 900, loss = 0.00063120\n",
      "Iteration 901, loss = 0.00062904\n",
      "Iteration 902, loss = 0.00062843\n",
      "Iteration 903, loss = 0.00062824\n",
      "Iteration 904, loss = 0.00062939\n",
      "Iteration 905, loss = 0.00062735\n",
      "Iteration 906, loss = 0.00062880\n",
      "Iteration 907, loss = 0.00062633\n",
      "Iteration 908, loss = 0.00062564\n",
      "Iteration 909, loss = 0.00062508\n",
      "Iteration 910, loss = 0.00062316\n",
      "Iteration 911, loss = 0.00062341\n",
      "Iteration 912, loss = 0.00062222\n",
      "Iteration 913, loss = 0.00062306\n",
      "Iteration 914, loss = 0.00062115\n",
      "Iteration 915, loss = 0.00062134\n",
      "Iteration 916, loss = 0.00061979\n",
      "Iteration 917, loss = 0.00061904\n",
      "Iteration 918, loss = 0.00061936\n",
      "Iteration 919, loss = 0.00061778\n",
      "Iteration 920, loss = 0.00061689\n",
      "Iteration 921, loss = 0.00061674\n",
      "Iteration 922, loss = 0.00061535\n",
      "Iteration 923, loss = 0.00061537\n",
      "Iteration 924, loss = 0.00061552\n",
      "Iteration 925, loss = 0.00061469\n",
      "Iteration 926, loss = 0.00061192\n",
      "Iteration 927, loss = 0.00061460\n",
      "Iteration 928, loss = 0.00061330\n",
      "Iteration 929, loss = 0.00061430\n",
      "Iteration 930, loss = 0.00061447\n",
      "Iteration 931, loss = 0.00061497\n",
      "Iteration 932, loss = 0.00061149\n",
      "Iteration 933, loss = 0.00060837\n",
      "Iteration 934, loss = 0.00060736\n",
      "Iteration 935, loss = 0.00060854\n",
      "Iteration 936, loss = 0.00060681\n",
      "Iteration 937, loss = 0.00060601\n",
      "Iteration 938, loss = 0.00060561\n",
      "Iteration 939, loss = 0.00060529\n",
      "Iteration 940, loss = 0.00060699\n",
      "Iteration 941, loss = 0.00060266\n",
      "Iteration 942, loss = 0.00060210\n",
      "Iteration 943, loss = 0.00060166\n",
      "Iteration 944, loss = 0.00060228\n",
      "Iteration 945, loss = 0.00060250\n",
      "Iteration 946, loss = 0.00060212\n",
      "Iteration 947, loss = 0.00059912\n",
      "Iteration 948, loss = 0.00059871\n",
      "Iteration 949, loss = 0.00059881\n",
      "Iteration 950, loss = 0.00059633\n",
      "Iteration 951, loss = 0.00059701\n",
      "Iteration 952, loss = 0.00059546\n",
      "Iteration 953, loss = 0.00059639\n",
      "Iteration 954, loss = 0.00059456\n",
      "Iteration 955, loss = 0.00059419\n",
      "Iteration 956, loss = 0.00059324\n",
      "Iteration 957, loss = 0.00059230\n",
      "Iteration 958, loss = 0.00059187\n",
      "Iteration 959, loss = 0.00059198\n",
      "Iteration 960, loss = 0.00059024\n",
      "Iteration 961, loss = 0.00059031\n",
      "Iteration 962, loss = 0.00058931\n",
      "Iteration 963, loss = 0.00058865\n",
      "Iteration 964, loss = 0.00058760\n",
      "Iteration 965, loss = 0.00058814\n",
      "Iteration 966, loss = 0.00058704\n",
      "Iteration 967, loss = 0.00058708\n",
      "Iteration 968, loss = 0.00058669\n",
      "Iteration 969, loss = 0.00058511\n",
      "Iteration 970, loss = 0.00058516\n",
      "Iteration 971, loss = 0.00058439\n",
      "Iteration 972, loss = 0.00058291\n",
      "Iteration 973, loss = 0.00058182\n",
      "Iteration 974, loss = 0.00058279\n",
      "Iteration 975, loss = 0.00058119\n",
      "Iteration 976, loss = 0.00058270\n",
      "Iteration 977, loss = 0.00058364\n",
      "Iteration 978, loss = 0.00057939\n",
      "Iteration 979, loss = 0.00057925\n",
      "Iteration 980, loss = 0.00057852\n",
      "Iteration 981, loss = 0.00057704\n",
      "Iteration 982, loss = 0.00057612\n",
      "Iteration 983, loss = 0.00057710\n",
      "Iteration 984, loss = 0.00057915\n",
      "Iteration 985, loss = 0.00057594\n",
      "Iteration 986, loss = 0.00057399\n",
      "Iteration 987, loss = 0.00057412\n",
      "Iteration 988, loss = 0.00057397\n",
      "Iteration 989, loss = 0.00057369\n",
      "Iteration 990, loss = 0.00057514\n",
      "Iteration 991, loss = 0.00057311\n",
      "Iteration 992, loss = 0.00057141\n",
      "Iteration 993, loss = 0.00057087\n",
      "Iteration 994, loss = 0.00056960\n",
      "Iteration 995, loss = 0.00057185\n",
      "Iteration 996, loss = 0.00056963\n",
      "Iteration 997, loss = 0.00057006\n",
      "Iteration 998, loss = 0.00056668\n",
      "Iteration 999, loss = 0.00056660\n",
      "Iteration 1000, loss = 0.00056933\n",
      "Iteration 1001, loss = 0.00056687\n",
      "Iteration 1002, loss = 0.00056506\n",
      "Iteration 1003, loss = 0.00056625\n",
      "Iteration 1004, loss = 0.00056434\n",
      "Iteration 1005, loss = 0.00056340\n",
      "Iteration 1006, loss = 0.00056424\n",
      "Iteration 1007, loss = 0.00056358\n",
      "Iteration 1008, loss = 0.00056435\n",
      "Iteration 1009, loss = 0.00056365\n",
      "Iteration 1010, loss = 0.00056232\n",
      "Iteration 1011, loss = 0.00056015\n",
      "Iteration 1012, loss = 0.00056218\n",
      "Iteration 1013, loss = 0.00055969\n",
      "Iteration 1014, loss = 0.00055795\n",
      "Iteration 1015, loss = 0.00055727\n",
      "Iteration 1016, loss = 0.00055632\n",
      "Iteration 1017, loss = 0.00055837\n",
      "Iteration 1018, loss = 0.00055806\n",
      "Iteration 1019, loss = 0.00055658\n",
      "Iteration 1020, loss = 0.00055512\n",
      "Iteration 1021, loss = 0.00055537\n",
      "Iteration 1022, loss = 0.00055653\n",
      "Iteration 1023, loss = 0.00055303\n",
      "Iteration 1024, loss = 0.00055221\n",
      "Iteration 1025, loss = 0.00055295\n",
      "Iteration 1026, loss = 0.00054966\n",
      "Iteration 1027, loss = 0.00055219\n",
      "Iteration 1028, loss = 0.00054869\n",
      "Iteration 1029, loss = 0.00054872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1030, loss = 0.00054814\n",
      "Iteration 1031, loss = 0.00054839\n",
      "Iteration 1032, loss = 0.00055483\n",
      "Iteration 1033, loss = 0.00055563\n",
      "Iteration 1034, loss = 0.00055024\n",
      "Iteration 1035, loss = 0.00054715\n",
      "Iteration 1036, loss = 0.00054529\n",
      "Iteration 1037, loss = 0.00054396\n",
      "Iteration 1038, loss = 0.00054300\n",
      "Iteration 1039, loss = 0.00054249\n",
      "Iteration 1040, loss = 0.00054327\n",
      "Iteration 1041, loss = 0.00054301\n",
      "Iteration 1042, loss = 0.00054182\n",
      "Iteration 1043, loss = 0.00054225\n",
      "Iteration 1044, loss = 0.00054104\n",
      "Iteration 1045, loss = 0.00053939\n",
      "Iteration 1046, loss = 0.00053842\n",
      "Iteration 1047, loss = 0.00053921\n",
      "Iteration 1048, loss = 0.00053961\n",
      "Iteration 1049, loss = 0.00054123\n",
      "Iteration 1050, loss = 0.00053886\n",
      "Iteration 1051, loss = 0.00053801\n",
      "Iteration 1052, loss = 0.00053748\n",
      "Iteration 1053, loss = 0.00053726\n",
      "Iteration 1054, loss = 0.00053371\n",
      "Iteration 1055, loss = 0.00053302\n",
      "Iteration 1056, loss = 0.00053313\n",
      "Iteration 1057, loss = 0.00053193\n",
      "Iteration 1058, loss = 0.00053204\n",
      "Iteration 1059, loss = 0.00053231\n",
      "Iteration 1060, loss = 0.00053187\n",
      "Iteration 1061, loss = 0.00053342\n",
      "Iteration 1062, loss = 0.00052940\n",
      "Iteration 1063, loss = 0.00052908\n",
      "Iteration 1064, loss = 0.00053020\n",
      "Iteration 1065, loss = 0.00052876\n",
      "Iteration 1066, loss = 0.00052707\n",
      "Iteration 1067, loss = 0.00052607\n",
      "Iteration 1068, loss = 0.00052652\n",
      "Iteration 1069, loss = 0.00052736\n",
      "Iteration 1070, loss = 0.00052693\n",
      "Iteration 1071, loss = 0.00052750\n",
      "Iteration 1072, loss = 0.00052601\n",
      "Iteration 1073, loss = 0.00052612\n",
      "Iteration 1074, loss = 0.00052368\n",
      "Iteration 1075, loss = 0.00052259\n",
      "Iteration 1076, loss = 0.00052181\n",
      "Iteration 1077, loss = 0.00052173\n",
      "Iteration 1078, loss = 0.00052145\n",
      "Iteration 1079, loss = 0.00052054\n",
      "Iteration 1080, loss = 0.00052150\n",
      "Iteration 1081, loss = 0.00052129\n",
      "Iteration 1082, loss = 0.00052131\n",
      "Iteration 1083, loss = 0.00052176\n",
      "Iteration 1084, loss = 0.00052126\n",
      "Iteration 1085, loss = 0.00052069\n",
      "Iteration 1086, loss = 0.00051754\n",
      "Iteration 1087, loss = 0.00051598\n",
      "Iteration 1088, loss = 0.00051688\n",
      "Iteration 1089, loss = 0.00052114\n",
      "Iteration 1090, loss = 0.00051621\n",
      "Iteration 1091, loss = 0.00051962\n",
      "Iteration 1092, loss = 0.00051581\n",
      "Iteration 1093, loss = 0.00051367\n",
      "Iteration 1094, loss = 0.00051281\n",
      "Iteration 1095, loss = 0.00051380\n",
      "Iteration 1096, loss = 0.00051184\n",
      "Iteration 1097, loss = 0.00051492\n",
      "Iteration 1098, loss = 0.00051057\n",
      "Iteration 1099, loss = 0.00051066\n",
      "Iteration 1100, loss = 0.00050962\n",
      "Iteration 1101, loss = 0.00050887\n",
      "Iteration 1102, loss = 0.00050969\n",
      "Iteration 1103, loss = 0.00050975\n",
      "Iteration 1104, loss = 0.00050838\n",
      "Iteration 1105, loss = 0.00050701\n",
      "Iteration 1106, loss = 0.00050655\n",
      "Iteration 1107, loss = 0.00050569\n",
      "Iteration 1108, loss = 0.00050565\n",
      "Iteration 1109, loss = 0.00050473\n",
      "Iteration 1110, loss = 0.00050651\n",
      "Iteration 1111, loss = 0.00050776\n",
      "Iteration 1112, loss = 0.00050862\n",
      "Iteration 1113, loss = 0.00051144\n",
      "Iteration 1114, loss = 0.00050310\n",
      "Iteration 1115, loss = 0.00050492\n",
      "Iteration 1116, loss = 0.00050205\n",
      "Iteration 1117, loss = 0.00050201\n",
      "Iteration 1118, loss = 0.00050061\n",
      "Iteration 1119, loss = 0.00050804\n",
      "Iteration 1120, loss = 0.00051757\n",
      "Iteration 1121, loss = 0.00050724\n",
      "Iteration 1122, loss = 0.00050388\n",
      "Iteration 1123, loss = 0.00050023\n",
      "Iteration 1124, loss = 0.00049758\n",
      "Iteration 1125, loss = 0.00049987\n",
      "Iteration 1126, loss = 0.00049711\n",
      "Iteration 1127, loss = 0.00049586\n",
      "Iteration 1128, loss = 0.00049522\n",
      "Iteration 1129, loss = 0.00049652\n",
      "Iteration 1130, loss = 0.00050020\n",
      "Iteration 1131, loss = 0.00049560\n",
      "Iteration 1132, loss = 0.00049629\n",
      "Iteration 1133, loss = 0.00049363\n",
      "Iteration 1134, loss = 0.00049621\n",
      "Iteration 1135, loss = 0.00049401\n",
      "Iteration 1136, loss = 0.00049773\n",
      "Iteration 1137, loss = 0.00049478\n",
      "Iteration 1138, loss = 0.00049720\n",
      "Iteration 1139, loss = 0.00050425\n",
      "Iteration 1140, loss = 0.00050648\n",
      "Iteration 1141, loss = 0.00051472\n",
      "Iteration 1142, loss = 0.02702880\n",
      "Iteration 1143, loss = 0.00829066\n",
      "Iteration 1144, loss = 0.00408893\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.49551978\n",
      "Iteration 2, loss = 0.36838386\n",
      "Iteration 3, loss = 0.29376411\n",
      "Iteration 4, loss = 0.25742296\n",
      "Iteration 5, loss = 0.22972486\n",
      "Iteration 6, loss = 0.21086765\n",
      "Iteration 7, loss = 0.19820497\n",
      "Iteration 8, loss = 0.18827884\n",
      "Iteration 9, loss = 0.18075779\n",
      "Iteration 10, loss = 0.17427368\n",
      "Iteration 11, loss = 0.16941815\n",
      "Iteration 12, loss = 0.16512772\n",
      "Iteration 13, loss = 0.16134223\n",
      "Iteration 14, loss = 0.15842442\n",
      "Iteration 15, loss = 0.15561000\n",
      "Iteration 16, loss = 0.15384424\n",
      "Iteration 17, loss = 0.15176565\n",
      "Iteration 18, loss = 0.14877583\n",
      "Iteration 19, loss = 0.14687233\n",
      "Iteration 20, loss = 0.14537815\n",
      "Iteration 21, loss = 0.14380889\n",
      "Iteration 22, loss = 0.14281781\n",
      "Iteration 23, loss = 0.14106766\n",
      "Iteration 24, loss = 0.13881925\n",
      "Iteration 25, loss = 0.13732147\n",
      "Iteration 26, loss = 0.13609380\n",
      "Iteration 27, loss = 0.13493448\n",
      "Iteration 28, loss = 0.13404994\n",
      "Iteration 29, loss = 0.13235177\n",
      "Iteration 30, loss = 0.13254079\n",
      "Iteration 31, loss = 0.13012100\n",
      "Iteration 32, loss = 0.12854158\n",
      "Iteration 33, loss = 0.12785649\n",
      "Iteration 34, loss = 0.12704663\n",
      "Iteration 35, loss = 0.12573181\n",
      "Iteration 36, loss = 0.12477667\n",
      "Iteration 37, loss = 0.12381050\n",
      "Iteration 38, loss = 0.12307091\n",
      "Iteration 39, loss = 0.12195045\n",
      "Iteration 40, loss = 0.12005119\n",
      "Iteration 41, loss = 0.11879359\n",
      "Iteration 42, loss = 0.11787404\n",
      "Iteration 43, loss = 0.11817463\n",
      "Iteration 44, loss = 0.11637387\n",
      "Iteration 45, loss = 0.11657953\n",
      "Iteration 46, loss = 0.11397254\n",
      "Iteration 47, loss = 0.11280929\n",
      "Iteration 48, loss = 0.11176133\n",
      "Iteration 49, loss = 0.11176321\n",
      "Iteration 50, loss = 0.10992907\n",
      "Iteration 51, loss = 0.10843943\n",
      "Iteration 52, loss = 0.10926194\n",
      "Iteration 53, loss = 0.10711842\n",
      "Iteration 54, loss = 0.10722722\n",
      "Iteration 55, loss = 0.10564078\n",
      "Iteration 56, loss = 0.10367420\n",
      "Iteration 57, loss = 0.10228935\n",
      "Iteration 58, loss = 0.10189357\n",
      "Iteration 59, loss = 0.10092017\n",
      "Iteration 60, loss = 0.10071415\n",
      "Iteration 61, loss = 0.09794115\n",
      "Iteration 62, loss = 0.09763260\n",
      "Iteration 63, loss = 0.09625128\n",
      "Iteration 64, loss = 0.09547561\n",
      "Iteration 65, loss = 0.09506997\n",
      "Iteration 66, loss = 0.09555978\n",
      "Iteration 67, loss = 0.09360271\n",
      "Iteration 68, loss = 0.09202421\n",
      "Iteration 69, loss = 0.09126935\n",
      "Iteration 70, loss = 0.09024477\n",
      "Iteration 71, loss = 0.09012213\n",
      "Iteration 72, loss = 0.08690714\n",
      "Iteration 73, loss = 0.08687953\n",
      "Iteration 74, loss = 0.08668081\n",
      "Iteration 75, loss = 0.08506778\n",
      "Iteration 76, loss = 0.08557215\n",
      "Iteration 77, loss = 0.08595820\n",
      "Iteration 78, loss = 0.08395697\n",
      "Iteration 79, loss = 0.08118670\n",
      "Iteration 80, loss = 0.08052164\n",
      "Iteration 81, loss = 0.07961508\n",
      "Iteration 82, loss = 0.07925379\n",
      "Iteration 83, loss = 0.07835736\n",
      "Iteration 84, loss = 0.07769264\n",
      "Iteration 85, loss = 0.07673964\n",
      "Iteration 86, loss = 0.07561835\n",
      "Iteration 87, loss = 0.07473327\n",
      "Iteration 88, loss = 0.07402923\n",
      "Iteration 89, loss = 0.07288505\n",
      "Iteration 90, loss = 0.07180209\n",
      "Iteration 91, loss = 0.07140165\n",
      "Iteration 92, loss = 0.07097800\n",
      "Iteration 93, loss = 0.07080348\n",
      "Iteration 94, loss = 0.06994459\n",
      "Iteration 95, loss = 0.06911921\n",
      "Iteration 96, loss = 0.06779569\n",
      "Iteration 97, loss = 0.06727913\n",
      "Iteration 98, loss = 0.06771650\n",
      "Iteration 99, loss = 0.06625638\n",
      "Iteration 100, loss = 0.06438935\n",
      "Iteration 101, loss = 0.06365677\n",
      "Iteration 102, loss = 0.06237460\n",
      "Iteration 103, loss = 0.06243481\n",
      "Iteration 104, loss = 0.06189048\n",
      "Iteration 105, loss = 0.06083384\n",
      "Iteration 106, loss = 0.06105485\n",
      "Iteration 107, loss = 0.05953729\n",
      "Iteration 108, loss = 0.05906748\n",
      "Iteration 109, loss = 0.05800685\n",
      "Iteration 110, loss = 0.05758594\n",
      "Iteration 111, loss = 0.05795293\n",
      "Iteration 112, loss = 0.05756209\n",
      "Iteration 113, loss = 0.05570969\n",
      "Iteration 114, loss = 0.05629360\n",
      "Iteration 115, loss = 0.05532080\n",
      "Iteration 116, loss = 0.05352847\n",
      "Iteration 117, loss = 0.05307606\n",
      "Iteration 118, loss = 0.05251334\n",
      "Iteration 119, loss = 0.05158074\n",
      "Iteration 120, loss = 0.05179298\n",
      "Iteration 121, loss = 0.05204452\n",
      "Iteration 122, loss = 0.04932782\n",
      "Iteration 123, loss = 0.04965198\n",
      "Iteration 124, loss = 0.04958900\n",
      "Iteration 125, loss = 0.04821737\n",
      "Iteration 126, loss = 0.04777331\n",
      "Iteration 127, loss = 0.04695446\n",
      "Iteration 128, loss = 0.04579156\n",
      "Iteration 129, loss = 0.04512278\n",
      "Iteration 130, loss = 0.04577784\n",
      "Iteration 131, loss = 0.04419465\n",
      "Iteration 132, loss = 0.04360178\n",
      "Iteration 133, loss = 0.04380861\n",
      "Iteration 134, loss = 0.04237319\n",
      "Iteration 135, loss = 0.04259527\n",
      "Iteration 136, loss = 0.04174406\n",
      "Iteration 137, loss = 0.04237311\n",
      "Iteration 138, loss = 0.04153547\n",
      "Iteration 139, loss = 0.04080355\n",
      "Iteration 140, loss = 0.03969248\n",
      "Iteration 141, loss = 0.03899347\n",
      "Iteration 142, loss = 0.03870835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 143, loss = 0.03852628\n",
      "Iteration 144, loss = 0.03890553\n",
      "Iteration 145, loss = 0.03702095\n",
      "Iteration 146, loss = 0.03671668\n",
      "Iteration 147, loss = 0.03623218\n",
      "Iteration 148, loss = 0.03521440\n",
      "Iteration 149, loss = 0.03503082\n",
      "Iteration 150, loss = 0.03445161\n",
      "Iteration 151, loss = 0.03414684\n",
      "Iteration 152, loss = 0.03327410\n",
      "Iteration 153, loss = 0.03290343\n",
      "Iteration 154, loss = 0.03282064\n",
      "Iteration 155, loss = 0.03281546\n",
      "Iteration 156, loss = 0.03217228\n",
      "Iteration 157, loss = 0.03233113\n",
      "Iteration 158, loss = 0.03189179\n",
      "Iteration 159, loss = 0.03116238\n",
      "Iteration 160, loss = 0.03073410\n",
      "Iteration 161, loss = 0.03009825\n",
      "Iteration 162, loss = 0.03020302\n",
      "Iteration 163, loss = 0.02895436\n",
      "Iteration 164, loss = 0.02905408\n",
      "Iteration 165, loss = 0.02961977\n",
      "Iteration 166, loss = 0.02776198\n",
      "Iteration 167, loss = 0.02735336\n",
      "Iteration 168, loss = 0.02703611\n",
      "Iteration 169, loss = 0.02661069\n",
      "Iteration 170, loss = 0.02626851\n",
      "Iteration 171, loss = 0.02564003\n",
      "Iteration 172, loss = 0.02609780\n",
      "Iteration 173, loss = 0.02529291\n",
      "Iteration 174, loss = 0.02501393\n",
      "Iteration 175, loss = 0.02444524\n",
      "Iteration 176, loss = 0.02427616\n",
      "Iteration 177, loss = 0.02392490\n",
      "Iteration 178, loss = 0.02405871\n",
      "Iteration 179, loss = 0.02305069\n",
      "Iteration 180, loss = 0.02322789\n",
      "Iteration 181, loss = 0.02266740\n",
      "Iteration 182, loss = 0.02280258\n",
      "Iteration 183, loss = 0.02170985\n",
      "Iteration 184, loss = 0.02157621\n",
      "Iteration 185, loss = 0.02275632\n",
      "Iteration 186, loss = 0.02143444\n",
      "Iteration 187, loss = 0.02096608\n",
      "Iteration 188, loss = 0.02049022\n",
      "Iteration 189, loss = 0.02058031\n",
      "Iteration 190, loss = 0.02007294\n",
      "Iteration 191, loss = 0.01968641\n",
      "Iteration 192, loss = 0.01916798\n",
      "Iteration 193, loss = 0.01890545\n",
      "Iteration 194, loss = 0.01854058\n",
      "Iteration 195, loss = 0.01817314\n",
      "Iteration 196, loss = 0.01870313\n",
      "Iteration 197, loss = 0.01859534\n",
      "Iteration 198, loss = 0.01845989\n",
      "Iteration 199, loss = 0.01781705\n",
      "Iteration 200, loss = 0.01767820\n",
      "Iteration 201, loss = 0.01709906\n",
      "Iteration 202, loss = 0.01668868\n",
      "Iteration 203, loss = 0.01637445\n",
      "Iteration 204, loss = 0.01609888\n",
      "Iteration 205, loss = 0.01595101\n",
      "Iteration 206, loss = 0.01606785\n",
      "Iteration 207, loss = 0.01559836\n",
      "Iteration 208, loss = 0.01526660\n",
      "Iteration 209, loss = 0.01512031\n",
      "Iteration 210, loss = 0.01523262\n",
      "Iteration 211, loss = 0.01534175\n",
      "Iteration 212, loss = 0.01457047\n",
      "Iteration 213, loss = 0.01428840\n",
      "Iteration 214, loss = 0.01489327\n",
      "Iteration 215, loss = 0.01428937\n",
      "Iteration 216, loss = 0.01394190\n",
      "Iteration 217, loss = 0.01363806\n",
      "Iteration 218, loss = 0.01346055\n",
      "Iteration 219, loss = 0.01314770\n",
      "Iteration 220, loss = 0.01300832\n",
      "Iteration 221, loss = 0.01297751\n",
      "Iteration 222, loss = 0.01281360\n",
      "Iteration 223, loss = 0.01267921\n",
      "Iteration 224, loss = 0.01244581\n",
      "Iteration 225, loss = 0.01257348\n",
      "Iteration 226, loss = 0.01209078\n",
      "Iteration 227, loss = 0.01203876\n",
      "Iteration 228, loss = 0.01178277\n",
      "Iteration 229, loss = 0.01150989\n",
      "Iteration 230, loss = 0.01134185\n",
      "Iteration 231, loss = 0.01131380\n",
      "Iteration 232, loss = 0.01115157\n",
      "Iteration 233, loss = 0.01103948\n",
      "Iteration 234, loss = 0.01090123\n",
      "Iteration 235, loss = 0.01065379\n",
      "Iteration 236, loss = 0.01051068\n",
      "Iteration 237, loss = 0.01032230\n",
      "Iteration 238, loss = 0.01029581\n",
      "Iteration 239, loss = 0.01010986\n",
      "Iteration 240, loss = 0.01015195\n",
      "Iteration 241, loss = 0.00996750\n",
      "Iteration 242, loss = 0.01007660\n",
      "Iteration 243, loss = 0.00968590\n",
      "Iteration 244, loss = 0.00958663\n",
      "Iteration 245, loss = 0.00933124\n",
      "Iteration 246, loss = 0.00927610\n",
      "Iteration 247, loss = 0.00927210\n",
      "Iteration 248, loss = 0.00909405\n",
      "Iteration 249, loss = 0.00936322\n",
      "Iteration 250, loss = 0.00882806\n",
      "Iteration 251, loss = 0.00902440\n",
      "Iteration 252, loss = 0.00857747\n",
      "Iteration 253, loss = 0.00844250\n",
      "Iteration 254, loss = 0.00835084\n",
      "Iteration 255, loss = 0.00831298\n",
      "Iteration 256, loss = 0.00818253\n",
      "Iteration 257, loss = 0.00808704\n",
      "Iteration 258, loss = 0.00800823\n",
      "Iteration 259, loss = 0.00800099\n",
      "Iteration 260, loss = 0.00779355\n",
      "Iteration 261, loss = 0.00768938\n",
      "Iteration 262, loss = 0.00758708\n",
      "Iteration 263, loss = 0.00764410\n",
      "Iteration 264, loss = 0.00741080\n",
      "Iteration 265, loss = 0.00743825\n",
      "Iteration 266, loss = 0.00726414\n",
      "Iteration 267, loss = 0.00712174\n",
      "Iteration 268, loss = 0.00703312\n",
      "Iteration 269, loss = 0.00710429\n",
      "Iteration 270, loss = 0.00704783\n",
      "Iteration 271, loss = 0.00689971\n",
      "Iteration 272, loss = 0.00704116\n",
      "Iteration 273, loss = 0.00671241\n",
      "Iteration 274, loss = 0.00671529\n",
      "Iteration 275, loss = 0.00658030\n",
      "Iteration 276, loss = 0.00649957\n",
      "Iteration 277, loss = 0.00637085\n",
      "Iteration 278, loss = 0.00634600\n",
      "Iteration 279, loss = 0.00639480\n",
      "Iteration 280, loss = 0.00627619\n",
      "Iteration 281, loss = 0.00612116\n",
      "Iteration 282, loss = 0.00600820\n",
      "Iteration 283, loss = 0.00597810\n",
      "Iteration 284, loss = 0.00594918\n",
      "Iteration 285, loss = 0.00605697\n",
      "Iteration 286, loss = 0.00580885\n",
      "Iteration 287, loss = 0.00570428\n",
      "Iteration 288, loss = 0.00571869\n",
      "Iteration 289, loss = 0.00564069\n",
      "Iteration 290, loss = 0.00554408\n",
      "Iteration 291, loss = 0.00558143\n",
      "Iteration 292, loss = 0.00553663\n",
      "Iteration 293, loss = 0.00554247\n",
      "Iteration 294, loss = 0.00531486\n",
      "Iteration 295, loss = 0.00532966\n",
      "Iteration 296, loss = 0.00524999\n",
      "Iteration 297, loss = 0.00523632\n",
      "Iteration 298, loss = 0.00518376\n",
      "Iteration 299, loss = 0.00502712\n",
      "Iteration 300, loss = 0.00501037\n",
      "Iteration 301, loss = 0.00500244\n",
      "Iteration 302, loss = 0.00486065\n",
      "Iteration 303, loss = 0.00490327\n",
      "Iteration 304, loss = 0.00486715\n",
      "Iteration 305, loss = 0.00486572\n",
      "Iteration 306, loss = 0.00472441\n",
      "Iteration 307, loss = 0.00464947\n",
      "Iteration 308, loss = 0.00460856\n",
      "Iteration 309, loss = 0.00457938\n",
      "Iteration 310, loss = 0.00451921\n",
      "Iteration 311, loss = 0.00446359\n",
      "Iteration 312, loss = 0.00439401\n",
      "Iteration 313, loss = 0.00435021\n",
      "Iteration 314, loss = 0.00432784\n",
      "Iteration 315, loss = 0.00439043\n",
      "Iteration 316, loss = 0.00432588\n",
      "Iteration 317, loss = 0.00423387\n",
      "Iteration 318, loss = 0.00428272\n",
      "Iteration 319, loss = 0.00421795\n",
      "Iteration 320, loss = 0.00417377\n",
      "Iteration 321, loss = 0.00412601\n",
      "Iteration 322, loss = 0.00401336\n",
      "Iteration 323, loss = 0.00401885\n",
      "Iteration 324, loss = 0.00406126\n",
      "Iteration 325, loss = 0.00406185\n",
      "Iteration 326, loss = 0.00393061\n",
      "Iteration 327, loss = 0.00381002\n",
      "Iteration 328, loss = 0.00381025\n",
      "Iteration 329, loss = 0.00382165\n",
      "Iteration 330, loss = 0.00373327\n",
      "Iteration 331, loss = 0.00376694\n",
      "Iteration 332, loss = 0.00366214\n",
      "Iteration 333, loss = 0.00363061\n",
      "Iteration 334, loss = 0.00364391\n",
      "Iteration 335, loss = 0.00361237\n",
      "Iteration 336, loss = 0.00354723\n",
      "Iteration 337, loss = 0.00349150\n",
      "Iteration 338, loss = 0.00346278\n",
      "Iteration 339, loss = 0.00349082\n",
      "Iteration 340, loss = 0.00341930\n",
      "Iteration 341, loss = 0.00350832\n",
      "Iteration 342, loss = 0.00350965\n",
      "Iteration 343, loss = 0.00332889\n",
      "Iteration 344, loss = 0.00330840\n",
      "Iteration 345, loss = 0.00328420\n",
      "Iteration 346, loss = 0.00332853\n",
      "Iteration 347, loss = 0.00345474\n",
      "Iteration 348, loss = 0.00328879\n",
      "Iteration 349, loss = 0.00318603\n",
      "Iteration 350, loss = 0.00314816\n",
      "Iteration 351, loss = 0.00311056\n",
      "Iteration 352, loss = 0.00316298\n",
      "Iteration 353, loss = 0.00308249\n",
      "Iteration 354, loss = 0.00303161\n",
      "Iteration 355, loss = 0.00309284\n",
      "Iteration 356, loss = 0.00305302\n",
      "Iteration 357, loss = 0.00295315\n",
      "Iteration 358, loss = 0.00293127\n",
      "Iteration 359, loss = 0.00291339\n",
      "Iteration 360, loss = 0.00288809\n",
      "Iteration 361, loss = 0.00286028\n",
      "Iteration 362, loss = 0.00283583\n",
      "Iteration 363, loss = 0.00281907\n",
      "Iteration 364, loss = 0.00280327\n",
      "Iteration 365, loss = 0.00282027\n",
      "Iteration 366, loss = 0.00274965\n",
      "Iteration 367, loss = 0.00273154\n",
      "Iteration 368, loss = 0.00275205\n",
      "Iteration 369, loss = 0.00269093\n",
      "Iteration 370, loss = 0.00267453\n",
      "Iteration 371, loss = 0.00272301\n",
      "Iteration 372, loss = 0.00264897\n",
      "Iteration 373, loss = 0.00261194\n",
      "Iteration 374, loss = 0.00261692\n",
      "Iteration 375, loss = 0.00257091\n",
      "Iteration 376, loss = 0.00254579\n",
      "Iteration 377, loss = 0.00255710\n",
      "Iteration 378, loss = 0.00256226\n",
      "Iteration 379, loss = 0.00253046\n",
      "Iteration 380, loss = 0.00249960\n",
      "Iteration 381, loss = 0.00245933\n",
      "Iteration 382, loss = 0.00246397\n",
      "Iteration 383, loss = 0.00245150\n",
      "Iteration 384, loss = 0.00240615\n",
      "Iteration 385, loss = 0.00238970\n",
      "Iteration 386, loss = 0.00238018\n",
      "Iteration 387, loss = 0.00235621\n",
      "Iteration 388, loss = 0.00235543\n",
      "Iteration 389, loss = 0.00232503\n",
      "Iteration 390, loss = 0.00231087\n",
      "Iteration 391, loss = 0.00229544\n",
      "Iteration 392, loss = 0.00228631\n",
      "Iteration 393, loss = 0.00226433\n",
      "Iteration 394, loss = 0.00234686\n",
      "Iteration 395, loss = 0.00227757\n",
      "Iteration 396, loss = 0.00224409\n",
      "Iteration 397, loss = 0.00222562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 398, loss = 0.00220587\n",
      "Iteration 399, loss = 0.00220115\n",
      "Iteration 400, loss = 0.00220613\n",
      "Iteration 401, loss = 0.00215868\n",
      "Iteration 402, loss = 0.00213148\n",
      "Iteration 403, loss = 0.00213165\n",
      "Iteration 404, loss = 0.00212902\n",
      "Iteration 405, loss = 0.00213095\n",
      "Iteration 406, loss = 0.00208787\n",
      "Iteration 407, loss = 0.00206408\n",
      "Iteration 408, loss = 0.00205007\n",
      "Iteration 409, loss = 0.00204923\n",
      "Iteration 410, loss = 0.00204726\n",
      "Iteration 411, loss = 0.00202101\n",
      "Iteration 412, loss = 0.00201313\n",
      "Iteration 413, loss = 0.00202885\n",
      "Iteration 414, loss = 0.00199702\n",
      "Iteration 415, loss = 0.00197293\n",
      "Iteration 416, loss = 0.00196094\n",
      "Iteration 417, loss = 0.00195142\n",
      "Iteration 418, loss = 0.00198590\n",
      "Iteration 419, loss = 0.00194204\n",
      "Iteration 420, loss = 0.00191136\n",
      "Iteration 421, loss = 0.00192861\n",
      "Iteration 422, loss = 0.00189121\n",
      "Iteration 423, loss = 0.00188264\n",
      "Iteration 424, loss = 0.00185672\n",
      "Iteration 425, loss = 0.00186189\n",
      "Iteration 426, loss = 0.00184180\n",
      "Iteration 427, loss = 0.00184004\n",
      "Iteration 428, loss = 0.00182058\n",
      "Iteration 429, loss = 0.00181350\n",
      "Iteration 430, loss = 0.00180933\n",
      "Iteration 431, loss = 0.00179271\n",
      "Iteration 432, loss = 0.00178393\n",
      "Iteration 433, loss = 0.00177649\n",
      "Iteration 434, loss = 0.00176281\n",
      "Iteration 435, loss = 0.00175742\n",
      "Iteration 436, loss = 0.00174729\n",
      "Iteration 437, loss = 0.00173321\n",
      "Iteration 438, loss = 0.00171730\n",
      "Iteration 439, loss = 0.00171826\n",
      "Iteration 440, loss = 0.00170233\n",
      "Iteration 441, loss = 0.00170413\n",
      "Iteration 442, loss = 0.00170269\n",
      "Iteration 443, loss = 0.00169061\n",
      "Iteration 444, loss = 0.00167696\n",
      "Iteration 445, loss = 0.00166980\n",
      "Iteration 446, loss = 0.00165346\n",
      "Iteration 447, loss = 0.00164505\n",
      "Iteration 448, loss = 0.00162972\n",
      "Iteration 449, loss = 0.00163017\n",
      "Iteration 450, loss = 0.00162421\n",
      "Iteration 451, loss = 0.00162685\n",
      "Iteration 452, loss = 0.00160722\n",
      "Iteration 453, loss = 0.00159112\n",
      "Iteration 454, loss = 0.00158526\n",
      "Iteration 455, loss = 0.00158422\n",
      "Iteration 456, loss = 0.00158394\n",
      "Iteration 457, loss = 0.00156173\n",
      "Iteration 458, loss = 0.00156064\n",
      "Iteration 459, loss = 0.00154460\n",
      "Iteration 460, loss = 0.00153615\n",
      "Iteration 461, loss = 0.00154076\n",
      "Iteration 462, loss = 0.00153446\n",
      "Iteration 463, loss = 0.00151740\n",
      "Iteration 464, loss = 0.00150843\n",
      "Iteration 465, loss = 0.00149966\n",
      "Iteration 466, loss = 0.00150228\n",
      "Iteration 467, loss = 0.00148879\n",
      "Iteration 468, loss = 0.00148611\n",
      "Iteration 469, loss = 0.00147573\n",
      "Iteration 470, loss = 0.00146412\n",
      "Iteration 471, loss = 0.00146103\n",
      "Iteration 472, loss = 0.00145922\n",
      "Iteration 473, loss = 0.00144790\n",
      "Iteration 474, loss = 0.00144273\n",
      "Iteration 475, loss = 0.00143596\n",
      "Iteration 476, loss = 0.00144883\n",
      "Iteration 477, loss = 0.00143141\n",
      "Iteration 478, loss = 0.00141560\n",
      "Iteration 479, loss = 0.00140955\n",
      "Iteration 480, loss = 0.00140936\n",
      "Iteration 481, loss = 0.00140477\n",
      "Iteration 482, loss = 0.00139515\n",
      "Iteration 483, loss = 0.00138934\n",
      "Iteration 484, loss = 0.00138159\n",
      "Iteration 485, loss = 0.00137084\n",
      "Iteration 486, loss = 0.00137260\n",
      "Iteration 487, loss = 0.00136603\n",
      "Iteration 488, loss = 0.00135615\n",
      "Iteration 489, loss = 0.00136107\n",
      "Iteration 490, loss = 0.00135911\n",
      "Iteration 491, loss = 0.00135111\n",
      "Iteration 492, loss = 0.00133885\n",
      "Iteration 493, loss = 0.00133603\n",
      "Iteration 494, loss = 0.00132910\n",
      "Iteration 495, loss = 0.00132111\n",
      "Iteration 496, loss = 0.00131280\n",
      "Iteration 497, loss = 0.00130708\n",
      "Iteration 498, loss = 0.00129937\n",
      "Iteration 499, loss = 0.00130165\n",
      "Iteration 500, loss = 0.00129254\n",
      "Iteration 501, loss = 0.00128732\n",
      "Iteration 502, loss = 0.00128348\n",
      "Iteration 503, loss = 0.00127567\n",
      "Iteration 504, loss = 0.00128284\n",
      "Iteration 505, loss = 0.00126649\n",
      "Iteration 506, loss = 0.00125781\n",
      "Iteration 507, loss = 0.00126056\n",
      "Iteration 508, loss = 0.00126361\n",
      "Iteration 509, loss = 0.00125045\n",
      "Iteration 510, loss = 0.00125380\n",
      "Iteration 511, loss = 0.00124143\n",
      "Iteration 512, loss = 0.00123764\n",
      "Iteration 513, loss = 0.00122658\n",
      "Iteration 514, loss = 0.00122426\n",
      "Iteration 515, loss = 0.00121977\n",
      "Iteration 516, loss = 0.00122053\n",
      "Iteration 517, loss = 0.00121125\n",
      "Iteration 518, loss = 0.00120782\n",
      "Iteration 519, loss = 0.00120221\n",
      "Iteration 520, loss = 0.00119672\n",
      "Iteration 521, loss = 0.00119714\n",
      "Iteration 522, loss = 0.00118780\n",
      "Iteration 523, loss = 0.00118932\n",
      "Iteration 524, loss = 0.00119088\n",
      "Iteration 525, loss = 0.00118261\n",
      "Iteration 526, loss = 0.00117426\n",
      "Iteration 527, loss = 0.00116846\n",
      "Iteration 528, loss = 0.00116826\n",
      "Iteration 529, loss = 0.00116524\n",
      "Iteration 530, loss = 0.00116002\n",
      "Iteration 531, loss = 0.00115609\n",
      "Iteration 532, loss = 0.00115291\n",
      "Iteration 533, loss = 0.00114831\n",
      "Iteration 534, loss = 0.00114760\n",
      "Iteration 535, loss = 0.00114383\n",
      "Iteration 536, loss = 0.00114796\n",
      "Iteration 537, loss = 0.00113222\n",
      "Iteration 538, loss = 0.00113242\n",
      "Iteration 539, loss = 0.00112536\n",
      "Iteration 540, loss = 0.00112056\n",
      "Iteration 541, loss = 0.00111826\n",
      "Iteration 542, loss = 0.00111558\n",
      "Iteration 543, loss = 0.00111311\n",
      "Iteration 544, loss = 0.00111013\n",
      "Iteration 545, loss = 0.00110803\n",
      "Iteration 546, loss = 0.00110420\n",
      "Iteration 547, loss = 0.00110556\n",
      "Iteration 548, loss = 0.00110212\n",
      "Iteration 549, loss = 0.00109176\n",
      "Iteration 550, loss = 0.00110620\n",
      "Iteration 551, loss = 0.00108500\n",
      "Iteration 552, loss = 0.00108397\n",
      "Iteration 553, loss = 0.00107868\n",
      "Iteration 554, loss = 0.00107201\n",
      "Iteration 555, loss = 0.00107395\n",
      "Iteration 556, loss = 0.00107275\n",
      "Iteration 557, loss = 0.00106668\n",
      "Iteration 558, loss = 0.00106974\n",
      "Iteration 559, loss = 0.00106151\n",
      "Iteration 560, loss = 0.00105763\n",
      "Iteration 561, loss = 0.00105905\n",
      "Iteration 562, loss = 0.00105299\n",
      "Iteration 563, loss = 0.00104553\n",
      "Iteration 564, loss = 0.00104496\n",
      "Iteration 565, loss = 0.00104079\n",
      "Iteration 566, loss = 0.00104111\n",
      "Iteration 567, loss = 0.00103375\n",
      "Iteration 568, loss = 0.00104143\n",
      "Iteration 569, loss = 0.00103631\n",
      "Iteration 570, loss = 0.00102984\n",
      "Iteration 571, loss = 0.00102713\n",
      "Iteration 572, loss = 0.00102507\n",
      "Iteration 573, loss = 0.00102701\n",
      "Iteration 574, loss = 0.00101767\n",
      "Iteration 575, loss = 0.00101616\n",
      "Iteration 576, loss = 0.00101135\n",
      "Iteration 577, loss = 0.00100856\n",
      "Iteration 578, loss = 0.00100782\n",
      "Iteration 579, loss = 0.00100532\n",
      "Iteration 580, loss = 0.00100057\n",
      "Iteration 581, loss = 0.00099973\n",
      "Iteration 582, loss = 0.00099661\n",
      "Iteration 583, loss = 0.00099706\n",
      "Iteration 584, loss = 0.00099084\n",
      "Iteration 585, loss = 0.00099107\n",
      "Iteration 586, loss = 0.00098664\n",
      "Iteration 587, loss = 0.00098615\n",
      "Iteration 588, loss = 0.00098341\n",
      "Iteration 589, loss = 0.00098743\n",
      "Iteration 590, loss = 0.00098130\n",
      "Iteration 591, loss = 0.00097454\n",
      "Iteration 592, loss = 0.00097447\n",
      "Iteration 593, loss = 0.00096968\n",
      "Iteration 594, loss = 0.00096952\n",
      "Iteration 595, loss = 0.00096782\n",
      "Iteration 596, loss = 0.00096669\n",
      "Iteration 597, loss = 0.00096489\n",
      "Iteration 598, loss = 0.00096817\n",
      "Iteration 599, loss = 0.00095916\n",
      "Iteration 600, loss = 0.00095729\n",
      "Iteration 601, loss = 0.00095641\n",
      "Iteration 602, loss = 0.00095267\n",
      "Iteration 603, loss = 0.00095018\n",
      "Iteration 604, loss = 0.00095283\n",
      "Iteration 605, loss = 0.00095068\n",
      "Iteration 606, loss = 0.00094346\n",
      "Iteration 607, loss = 0.00094485\n",
      "Iteration 608, loss = 0.00093788\n",
      "Iteration 609, loss = 0.00094110\n",
      "Iteration 610, loss = 0.00093384\n",
      "Iteration 611, loss = 0.00093248\n",
      "Iteration 612, loss = 0.00093218\n",
      "Iteration 613, loss = 0.00092824\n",
      "Iteration 614, loss = 0.00092929\n",
      "Iteration 615, loss = 0.00092336\n",
      "Iteration 616, loss = 0.00092605\n",
      "Iteration 617, loss = 0.00092155\n",
      "Iteration 618, loss = 0.00091945\n",
      "Iteration 619, loss = 0.00091638\n",
      "Iteration 620, loss = 0.00091622\n",
      "Iteration 621, loss = 0.00091405\n",
      "Iteration 622, loss = 0.00091153\n",
      "Iteration 623, loss = 0.00091080\n",
      "Iteration 624, loss = 0.00090960\n",
      "Iteration 625, loss = 0.00090559\n",
      "Iteration 626, loss = 0.00090731\n",
      "Iteration 627, loss = 0.00090190\n",
      "Iteration 628, loss = 0.00090001\n",
      "Iteration 629, loss = 0.00089810\n",
      "Iteration 630, loss = 0.00090082\n",
      "Iteration 631, loss = 0.00089890\n",
      "Iteration 632, loss = 0.00089612\n",
      "Iteration 633, loss = 0.00089232\n",
      "Iteration 634, loss = 0.00089424\n",
      "Iteration 635, loss = 0.00089143\n",
      "Iteration 636, loss = 0.00089272\n",
      "Iteration 637, loss = 0.00088898\n",
      "Iteration 638, loss = 0.00088866\n",
      "Iteration 639, loss = 0.00088560\n",
      "Iteration 640, loss = 0.00088128\n",
      "Iteration 641, loss = 0.00088014\n",
      "Iteration 642, loss = 0.00087851\n",
      "Iteration 643, loss = 0.00087910\n",
      "Iteration 644, loss = 0.00087314\n",
      "Iteration 645, loss = 0.00087328\n",
      "Iteration 646, loss = 0.00087347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 647, loss = 0.00087136\n",
      "Iteration 648, loss = 0.00087268\n",
      "Iteration 649, loss = 0.00086675\n",
      "Iteration 650, loss = 0.00086583\n",
      "Iteration 651, loss = 0.00086370\n",
      "Iteration 652, loss = 0.00086397\n",
      "Iteration 653, loss = 0.00086145\n",
      "Iteration 654, loss = 0.00085894\n",
      "Iteration 655, loss = 0.00085797\n",
      "Iteration 656, loss = 0.00085625\n",
      "Iteration 657, loss = 0.00085503\n",
      "Iteration 658, loss = 0.00085241\n",
      "Iteration 659, loss = 0.00085335\n",
      "Iteration 660, loss = 0.00085041\n",
      "Iteration 661, loss = 0.00084933\n",
      "Iteration 662, loss = 0.00084715\n",
      "Iteration 663, loss = 0.00084608\n",
      "Iteration 664, loss = 0.00084637\n",
      "Iteration 665, loss = 0.00084411\n",
      "Iteration 666, loss = 0.00084269\n",
      "Iteration 667, loss = 0.00084135\n",
      "Iteration 668, loss = 0.00083869\n",
      "Iteration 669, loss = 0.00083806\n",
      "Iteration 670, loss = 0.00083778\n",
      "Iteration 671, loss = 0.00083683\n",
      "Iteration 672, loss = 0.00083438\n",
      "Iteration 673, loss = 0.00083487\n",
      "Iteration 674, loss = 0.00083510\n",
      "Iteration 675, loss = 0.00083103\n",
      "Iteration 676, loss = 0.00082816\n",
      "Iteration 677, loss = 0.00082832\n",
      "Iteration 678, loss = 0.00082716\n",
      "Iteration 679, loss = 0.00082661\n",
      "Iteration 680, loss = 0.00082438\n",
      "Iteration 681, loss = 0.00082201\n",
      "Iteration 682, loss = 0.00082131\n",
      "Iteration 683, loss = 0.00081994\n",
      "Iteration 684, loss = 0.00081875\n",
      "Iteration 685, loss = 0.00081820\n",
      "Iteration 686, loss = 0.00081633\n",
      "Iteration 687, loss = 0.00081545\n",
      "Iteration 688, loss = 0.00081486\n",
      "Iteration 689, loss = 0.00081370\n",
      "Iteration 690, loss = 0.00081157\n",
      "Iteration 691, loss = 0.00081069\n",
      "Iteration 692, loss = 0.00080964\n",
      "Iteration 693, loss = 0.00081007\n",
      "Iteration 694, loss = 0.00081123\n",
      "Iteration 695, loss = 0.00080836\n",
      "Iteration 696, loss = 0.00080587\n",
      "Iteration 697, loss = 0.00080442\n",
      "Iteration 698, loss = 0.00080259\n",
      "Iteration 699, loss = 0.00080066\n",
      "Iteration 700, loss = 0.00080109\n",
      "Iteration 701, loss = 0.00079874\n",
      "Iteration 702, loss = 0.00080078\n",
      "Iteration 703, loss = 0.00080210\n",
      "Iteration 704, loss = 0.00080562\n",
      "Iteration 705, loss = 0.00079971\n",
      "Iteration 706, loss = 0.00079435\n",
      "Iteration 707, loss = 0.00079206\n",
      "Iteration 708, loss = 0.00079136\n",
      "Iteration 709, loss = 0.00079002\n",
      "Iteration 710, loss = 0.00078827\n",
      "Iteration 711, loss = 0.00078800\n",
      "Iteration 712, loss = 0.00078666\n",
      "Iteration 713, loss = 0.00078597\n",
      "Iteration 714, loss = 0.00078668\n",
      "Iteration 715, loss = 0.00078400\n",
      "Iteration 716, loss = 0.00078203\n",
      "Iteration 717, loss = 0.00078361\n",
      "Iteration 718, loss = 0.00078017\n",
      "Iteration 719, loss = 0.00078062\n",
      "Iteration 720, loss = 0.00078130\n",
      "Iteration 721, loss = 0.00077873\n",
      "Iteration 722, loss = 0.00077590\n",
      "Iteration 723, loss = 0.00077561\n",
      "Iteration 724, loss = 0.00077525\n",
      "Iteration 725, loss = 0.00077559\n",
      "Iteration 726, loss = 0.00077632\n",
      "Iteration 727, loss = 0.00077098\n",
      "Iteration 728, loss = 0.00077094\n",
      "Iteration 729, loss = 0.00076942\n",
      "Iteration 730, loss = 0.00076770\n",
      "Iteration 731, loss = 0.00077024\n",
      "Iteration 732, loss = 0.00076809\n",
      "Iteration 733, loss = 0.00076952\n",
      "Iteration 734, loss = 0.00076519\n",
      "Iteration 735, loss = 0.00076517\n",
      "Iteration 736, loss = 0.00076313\n",
      "Iteration 737, loss = 0.00076320\n",
      "Iteration 738, loss = 0.00076303\n",
      "Iteration 739, loss = 0.00076061\n",
      "Iteration 740, loss = 0.00075895\n",
      "Iteration 741, loss = 0.00075860\n",
      "Iteration 742, loss = 0.00075713\n",
      "Iteration 743, loss = 0.00075606\n",
      "Iteration 744, loss = 0.00075582\n",
      "Iteration 745, loss = 0.00075416\n",
      "Iteration 746, loss = 0.00075293\n",
      "Iteration 747, loss = 0.00075208\n",
      "Iteration 748, loss = 0.00075160\n",
      "Iteration 749, loss = 0.00075089\n",
      "Iteration 750, loss = 0.00074971\n",
      "Iteration 751, loss = 0.00074854\n",
      "Iteration 752, loss = 0.00074850\n",
      "Iteration 753, loss = 0.00074656\n",
      "Iteration 754, loss = 0.00074559\n",
      "Iteration 755, loss = 0.00074560\n",
      "Iteration 756, loss = 0.00074375\n",
      "Iteration 757, loss = 0.00074488\n",
      "Iteration 758, loss = 0.00074203\n",
      "Iteration 759, loss = 0.00074135\n",
      "Iteration 760, loss = 0.00074108\n",
      "Iteration 761, loss = 0.00074042\n",
      "Iteration 762, loss = 0.00073903\n",
      "Iteration 763, loss = 0.00073846\n",
      "Iteration 764, loss = 0.00073679\n",
      "Iteration 765, loss = 0.00073749\n",
      "Iteration 766, loss = 0.00073490\n",
      "Iteration 767, loss = 0.00073463\n",
      "Iteration 768, loss = 0.00073392\n",
      "Iteration 769, loss = 0.00073272\n",
      "Iteration 770, loss = 0.00073191\n",
      "Iteration 771, loss = 0.00073092\n",
      "Iteration 772, loss = 0.00073230\n",
      "Iteration 773, loss = 0.00072974\n",
      "Iteration 774, loss = 0.00072837\n",
      "Iteration 775, loss = 0.00072815\n",
      "Iteration 776, loss = 0.00072720\n",
      "Iteration 777, loss = 0.00072697\n",
      "Iteration 778, loss = 0.00072515\n",
      "Iteration 779, loss = 0.00072511\n",
      "Iteration 780, loss = 0.00072368\n",
      "Iteration 781, loss = 0.00072476\n",
      "Iteration 782, loss = 0.00072389\n",
      "Iteration 783, loss = 0.00072339\n",
      "Iteration 784, loss = 0.00072446\n",
      "Iteration 785, loss = 0.00071822\n",
      "Iteration 786, loss = 0.00071969\n",
      "Iteration 787, loss = 0.00071812\n",
      "Iteration 788, loss = 0.00071655\n",
      "Iteration 789, loss = 0.00071747\n",
      "Iteration 790, loss = 0.00071583\n",
      "Iteration 791, loss = 0.00071564\n",
      "Iteration 792, loss = 0.00071422\n",
      "Iteration 793, loss = 0.00071227\n",
      "Iteration 794, loss = 0.00071136\n",
      "Iteration 795, loss = 0.00071102\n",
      "Iteration 796, loss = 0.00071001\n",
      "Iteration 797, loss = 0.00070931\n",
      "Iteration 798, loss = 0.00070846\n",
      "Iteration 799, loss = 0.00070782\n",
      "Iteration 800, loss = 0.00070698\n",
      "Iteration 801, loss = 0.00070605\n",
      "Iteration 802, loss = 0.00070566\n",
      "Iteration 803, loss = 0.00070487\n",
      "Iteration 804, loss = 0.00070410\n",
      "Iteration 805, loss = 0.00070631\n",
      "Iteration 806, loss = 0.00070468\n",
      "Iteration 807, loss = 0.00070285\n",
      "Iteration 808, loss = 0.00070060\n",
      "Iteration 809, loss = 0.00070079\n",
      "Iteration 810, loss = 0.00070138\n",
      "Iteration 811, loss = 0.00069813\n",
      "Iteration 812, loss = 0.00069871\n",
      "Iteration 813, loss = 0.00069675\n",
      "Iteration 814, loss = 0.00069601\n",
      "Iteration 815, loss = 0.00069522\n",
      "Iteration 816, loss = 0.00069463\n",
      "Iteration 817, loss = 0.00069516\n",
      "Iteration 818, loss = 0.00069532\n",
      "Iteration 819, loss = 0.00069499\n",
      "Iteration 820, loss = 0.00069350\n",
      "Iteration 821, loss = 0.00069333\n",
      "Iteration 822, loss = 0.00069146\n",
      "Iteration 823, loss = 0.00068880\n",
      "Iteration 824, loss = 0.00068795\n",
      "Iteration 825, loss = 0.00068954\n",
      "Iteration 826, loss = 0.00068751\n",
      "Iteration 827, loss = 0.00068600\n",
      "Iteration 828, loss = 0.00068551\n",
      "Iteration 829, loss = 0.00068518\n",
      "Iteration 830, loss = 0.00068402\n",
      "Iteration 831, loss = 0.00068311\n",
      "Iteration 832, loss = 0.00068267\n",
      "Iteration 833, loss = 0.00068250\n",
      "Iteration 834, loss = 0.00068292\n",
      "Iteration 835, loss = 0.00068222\n",
      "Iteration 836, loss = 0.00068132\n",
      "Iteration 837, loss = 0.00068041\n",
      "Iteration 838, loss = 0.00067840\n",
      "Iteration 839, loss = 0.00067704\n",
      "Iteration 840, loss = 0.00067791\n",
      "Iteration 841, loss = 0.00067574\n",
      "Iteration 842, loss = 0.00067490\n",
      "Iteration 843, loss = 0.00067426\n",
      "Iteration 844, loss = 0.00067279\n",
      "Iteration 845, loss = 0.00067298\n",
      "Iteration 846, loss = 0.00067166\n",
      "Iteration 847, loss = 0.00067162\n",
      "Iteration 848, loss = 0.00067093\n",
      "Iteration 849, loss = 0.00067129\n",
      "Iteration 850, loss = 0.00066888\n",
      "Iteration 851, loss = 0.00066957\n",
      "Iteration 852, loss = 0.00067030\n",
      "Iteration 853, loss = 0.00066812\n",
      "Iteration 854, loss = 0.00066905\n",
      "Iteration 855, loss = 0.00066806\n",
      "Iteration 856, loss = 0.00066874\n",
      "Iteration 857, loss = 0.00066877\n",
      "Iteration 858, loss = 0.00066562\n",
      "Iteration 859, loss = 0.00066615\n",
      "Iteration 860, loss = 0.00066188\n",
      "Iteration 861, loss = 0.00066285\n",
      "Iteration 862, loss = 0.00066155\n",
      "Iteration 863, loss = 0.00066041\n",
      "Iteration 864, loss = 0.00065870\n",
      "Iteration 865, loss = 0.00065845\n",
      "Iteration 866, loss = 0.00065757\n",
      "Iteration 867, loss = 0.00065825\n",
      "Iteration 868, loss = 0.00065677\n",
      "Iteration 869, loss = 0.00065557\n",
      "Iteration 870, loss = 0.00065500\n",
      "Iteration 871, loss = 0.00065426\n",
      "Iteration 872, loss = 0.00065262\n",
      "Iteration 873, loss = 0.00065269\n",
      "Iteration 874, loss = 0.00065220\n",
      "Iteration 875, loss = 0.00065212\n",
      "Iteration 876, loss = 0.00065083\n",
      "Iteration 877, loss = 0.00065024\n",
      "Iteration 878, loss = 0.00064913\n",
      "Iteration 879, loss = 0.00064772\n",
      "Iteration 880, loss = 0.00064763\n",
      "Iteration 881, loss = 0.00064836\n",
      "Iteration 882, loss = 0.00064633\n",
      "Iteration 883, loss = 0.00064640\n",
      "Iteration 884, loss = 0.00064521\n",
      "Iteration 885, loss = 0.00064414\n",
      "Iteration 886, loss = 0.00064442\n",
      "Iteration 887, loss = 0.00064238\n",
      "Iteration 888, loss = 0.00064161\n",
      "Iteration 889, loss = 0.00064291\n",
      "Iteration 890, loss = 0.00064029\n",
      "Iteration 891, loss = 0.00063985\n",
      "Iteration 892, loss = 0.00064032\n",
      "Iteration 893, loss = 0.00063826\n",
      "Iteration 894, loss = 0.00063831\n",
      "Iteration 895, loss = 0.00063887\n",
      "Iteration 896, loss = 0.00063753\n",
      "Iteration 897, loss = 0.00063883\n",
      "Iteration 898, loss = 0.00063669\n",
      "Iteration 899, loss = 0.00063752\n",
      "Iteration 900, loss = 0.00064281\n",
      "Iteration 901, loss = 0.00063614\n",
      "Iteration 902, loss = 0.00063208\n",
      "Iteration 903, loss = 0.00063169\n",
      "Iteration 904, loss = 0.00063212\n",
      "Iteration 905, loss = 0.00063203\n",
      "Iteration 906, loss = 0.00062905\n",
      "Iteration 907, loss = 0.00062924\n",
      "Iteration 908, loss = 0.00062873\n",
      "Iteration 909, loss = 0.00062801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 910, loss = 0.00062657\n",
      "Iteration 911, loss = 0.00062541\n",
      "Iteration 912, loss = 0.00062614\n",
      "Iteration 913, loss = 0.00062540\n",
      "Iteration 914, loss = 0.00062378\n",
      "Iteration 915, loss = 0.00062352\n",
      "Iteration 916, loss = 0.00062327\n",
      "Iteration 917, loss = 0.00062226\n",
      "Iteration 918, loss = 0.00062143\n",
      "Iteration 919, loss = 0.00062043\n",
      "Iteration 920, loss = 0.00062010\n",
      "Iteration 921, loss = 0.00062024\n",
      "Iteration 922, loss = 0.00061974\n",
      "Iteration 923, loss = 0.00062374\n",
      "Iteration 924, loss = 0.00062281\n",
      "Iteration 925, loss = 0.00062048\n",
      "Iteration 926, loss = 0.00061640\n",
      "Iteration 927, loss = 0.00061718\n",
      "Iteration 928, loss = 0.00061472\n",
      "Iteration 929, loss = 0.00061522\n",
      "Iteration 930, loss = 0.00061370\n",
      "Iteration 931, loss = 0.00061249\n",
      "Iteration 932, loss = 0.00061118\n",
      "Iteration 933, loss = 0.00061124\n",
      "Iteration 934, loss = 0.00061148\n",
      "Iteration 935, loss = 0.00061029\n",
      "Iteration 936, loss = 0.00060888\n",
      "Iteration 937, loss = 0.00060939\n",
      "Iteration 938, loss = 0.00060901\n",
      "Iteration 939, loss = 0.00060900\n",
      "Iteration 940, loss = 0.00061427\n",
      "Iteration 941, loss = 0.00061662\n",
      "Iteration 942, loss = 0.00061765\n",
      "Iteration 943, loss = 0.00060839\n",
      "Iteration 944, loss = 0.00060490\n",
      "Iteration 945, loss = 0.00060461\n",
      "Iteration 946, loss = 0.00060339\n",
      "Iteration 947, loss = 0.00060127\n",
      "Iteration 948, loss = 0.00060105\n",
      "Iteration 949, loss = 0.00060034\n",
      "Iteration 950, loss = 0.00059983\n",
      "Iteration 951, loss = 0.00059847\n",
      "Iteration 952, loss = 0.00059948\n",
      "Iteration 953, loss = 0.00059816\n",
      "Iteration 954, loss = 0.00059741\n",
      "Iteration 955, loss = 0.00060027\n",
      "Iteration 956, loss = 0.00060018\n",
      "Iteration 957, loss = 0.00060015\n",
      "Iteration 958, loss = 0.00059563\n",
      "Iteration 959, loss = 0.00059441\n",
      "Iteration 960, loss = 0.00059407\n",
      "Iteration 961, loss = 0.00059344\n",
      "Iteration 962, loss = 0.00059380\n",
      "Iteration 963, loss = 0.00059687\n",
      "Iteration 964, loss = 0.00059118\n",
      "Iteration 965, loss = 0.00059102\n",
      "Iteration 966, loss = 0.00059092\n",
      "Iteration 967, loss = 0.00059431\n",
      "Iteration 968, loss = 0.00058872\n",
      "Iteration 969, loss = 0.00058746\n",
      "Iteration 970, loss = 0.00058779\n",
      "Iteration 971, loss = 0.00058709\n",
      "Iteration 972, loss = 0.00058737\n",
      "Iteration 973, loss = 0.00058472\n",
      "Iteration 974, loss = 0.00058565\n",
      "Iteration 975, loss = 0.00058723\n",
      "Iteration 976, loss = 0.00058482\n",
      "Iteration 977, loss = 0.00058673\n",
      "Iteration 978, loss = 0.00058351\n",
      "Iteration 979, loss = 0.00058143\n",
      "Iteration 980, loss = 0.00058069\n",
      "Iteration 981, loss = 0.00058093\n",
      "Iteration 982, loss = 0.00057977\n",
      "Iteration 983, loss = 0.00058135\n",
      "Iteration 984, loss = 0.00058200\n",
      "Iteration 985, loss = 0.00057984\n",
      "Iteration 986, loss = 0.00057791\n",
      "Iteration 987, loss = 0.00057723\n",
      "Iteration 988, loss = 0.00057688\n",
      "Iteration 989, loss = 0.00057466\n",
      "Iteration 990, loss = 0.00057420\n",
      "Iteration 991, loss = 0.00057503\n",
      "Iteration 992, loss = 0.00057365\n",
      "Iteration 993, loss = 0.00057377\n",
      "Iteration 994, loss = 0.00057460\n",
      "Iteration 995, loss = 0.00057493\n",
      "Iteration 996, loss = 0.00057225\n",
      "Iteration 997, loss = 0.00057048\n",
      "Iteration 998, loss = 0.00056980\n",
      "Iteration 999, loss = 0.00056910\n",
      "Iteration 1000, loss = 0.00056810\n",
      "Iteration 1001, loss = 0.00057250\n",
      "Iteration 1002, loss = 0.00056989\n",
      "Iteration 1003, loss = 0.00056925\n",
      "Iteration 1004, loss = 0.00056635\n",
      "Iteration 1005, loss = 0.00056972\n",
      "Iteration 1006, loss = 0.00056803\n",
      "Iteration 1007, loss = 0.00056787\n",
      "Iteration 1008, loss = 0.00056331\n",
      "Iteration 1009, loss = 0.00056184\n",
      "Iteration 1010, loss = 0.00056093\n",
      "Iteration 1011, loss = 0.00056186\n",
      "Iteration 1012, loss = 0.00056135\n",
      "Iteration 1013, loss = 0.00056058\n",
      "Iteration 1014, loss = 0.00056336\n",
      "Iteration 1015, loss = 0.00056872\n",
      "Iteration 1016, loss = 0.00056506\n",
      "Iteration 1017, loss = 0.00056110\n",
      "Iteration 1018, loss = 0.00055745\n",
      "Iteration 1019, loss = 0.00055723\n",
      "Iteration 1020, loss = 0.00055597\n",
      "Iteration 1021, loss = 0.00055552\n",
      "Iteration 1022, loss = 0.00055504\n",
      "Iteration 1023, loss = 0.00055619\n",
      "Iteration 1024, loss = 0.00055406\n",
      "Iteration 1025, loss = 0.00055235\n",
      "Iteration 1026, loss = 0.00055232\n",
      "Iteration 1027, loss = 0.00055165\n",
      "Iteration 1028, loss = 0.00055181\n",
      "Iteration 1029, loss = 0.00055135\n",
      "Iteration 1030, loss = 0.00055001\n",
      "Iteration 1031, loss = 0.00054900\n",
      "Iteration 1032, loss = 0.00054984\n",
      "Iteration 1033, loss = 0.00054824\n",
      "Iteration 1034, loss = 0.00054937\n",
      "Iteration 1035, loss = 0.00055692\n",
      "Iteration 1036, loss = 0.00055095\n",
      "Iteration 1037, loss = 0.00055225\n",
      "Iteration 1038, loss = 0.00054980\n",
      "Iteration 1039, loss = 0.00054918\n",
      "Iteration 1040, loss = 0.00054631\n",
      "Iteration 1041, loss = 0.00054632\n",
      "Iteration 1042, loss = 0.00054277\n",
      "Iteration 1043, loss = 0.00054234\n",
      "Iteration 1044, loss = 0.00054294\n",
      "Iteration 1045, loss = 0.00054742\n",
      "Iteration 1046, loss = 0.00054566\n",
      "Iteration 1047, loss = 0.00054548\n",
      "Iteration 1048, loss = 0.00054070\n",
      "Iteration 1049, loss = 0.00053949\n",
      "Iteration 1050, loss = 0.00053936\n",
      "Iteration 1051, loss = 0.00053759\n",
      "Iteration 1052, loss = 0.00053718\n",
      "Iteration 1053, loss = 0.00054162\n",
      "Iteration 1054, loss = 0.00053851\n",
      "Iteration 1055, loss = 0.00053652\n",
      "Iteration 1056, loss = 0.00053619\n",
      "Iteration 1057, loss = 0.00053532\n",
      "Iteration 1058, loss = 0.00053446\n",
      "Iteration 1059, loss = 0.00053434\n",
      "Iteration 1060, loss = 0.00053385\n",
      "Iteration 1061, loss = 0.00053469\n",
      "Iteration 1062, loss = 0.00054046\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.60405620\n",
      "Iteration 2, loss = 0.44849330\n",
      "Iteration 3, loss = 0.34071603\n",
      "Iteration 4, loss = 0.29390127\n",
      "Iteration 5, loss = 0.25405130\n",
      "Iteration 6, loss = 0.22929105\n",
      "Iteration 7, loss = 0.21305753\n",
      "Iteration 8, loss = 0.20052356\n",
      "Iteration 9, loss = 0.19073466\n",
      "Iteration 10, loss = 0.18279092\n",
      "Iteration 11, loss = 0.17647671\n",
      "Iteration 12, loss = 0.17128120\n",
      "Iteration 13, loss = 0.16719952\n",
      "Iteration 14, loss = 0.16332243\n",
      "Iteration 15, loss = 0.15956837\n",
      "Iteration 16, loss = 0.15609199\n",
      "Iteration 17, loss = 0.15366099\n",
      "Iteration 18, loss = 0.15136297\n",
      "Iteration 19, loss = 0.14887873\n",
      "Iteration 20, loss = 0.14699101\n",
      "Iteration 21, loss = 0.14433313\n",
      "Iteration 22, loss = 0.14252630\n",
      "Iteration 23, loss = 0.14106795\n",
      "Iteration 24, loss = 0.14019455\n",
      "Iteration 25, loss = 0.13721306\n",
      "Iteration 26, loss = 0.13734538\n",
      "Iteration 27, loss = 0.13509302\n",
      "Iteration 28, loss = 0.13461630\n",
      "Iteration 29, loss = 0.13362450\n",
      "Iteration 30, loss = 0.13097586\n",
      "Iteration 31, loss = 0.12971760\n",
      "Iteration 32, loss = 0.12877305\n",
      "Iteration 33, loss = 0.12721400\n",
      "Iteration 34, loss = 0.12721789\n",
      "Iteration 35, loss = 0.12653259\n",
      "Iteration 36, loss = 0.12418564\n",
      "Iteration 37, loss = 0.12235412\n",
      "Iteration 38, loss = 0.12156031\n",
      "Iteration 39, loss = 0.12078938\n",
      "Iteration 40, loss = 0.11988584\n",
      "Iteration 41, loss = 0.11835502\n",
      "Iteration 42, loss = 0.11709592\n",
      "Iteration 43, loss = 0.11691900\n",
      "Iteration 44, loss = 0.11645830\n",
      "Iteration 45, loss = 0.11454369\n",
      "Iteration 46, loss = 0.11310164\n",
      "Iteration 47, loss = 0.11223736\n",
      "Iteration 48, loss = 0.11156743\n",
      "Iteration 49, loss = 0.11240902\n",
      "Iteration 50, loss = 0.11083708\n",
      "Iteration 51, loss = 0.10844956\n",
      "Iteration 52, loss = 0.10747910\n",
      "Iteration 53, loss = 0.10672576\n",
      "Iteration 54, loss = 0.10652049\n",
      "Iteration 55, loss = 0.10615373\n",
      "Iteration 56, loss = 0.10439049\n",
      "Iteration 57, loss = 0.10384303\n",
      "Iteration 58, loss = 0.10295158\n",
      "Iteration 59, loss = 0.10181992\n",
      "Iteration 60, loss = 0.10046059\n",
      "Iteration 61, loss = 0.09937015\n",
      "Iteration 62, loss = 0.09821949\n",
      "Iteration 63, loss = 0.09832612\n",
      "Iteration 64, loss = 0.09693653\n",
      "Iteration 65, loss = 0.09544338\n",
      "Iteration 66, loss = 0.09526778\n",
      "Iteration 67, loss = 0.09418561\n",
      "Iteration 68, loss = 0.09368347\n",
      "Iteration 69, loss = 0.09578523\n",
      "Iteration 70, loss = 0.09310092\n",
      "Iteration 71, loss = 0.09034631\n",
      "Iteration 72, loss = 0.09083544\n",
      "Iteration 73, loss = 0.08862112\n",
      "Iteration 74, loss = 0.08755510\n",
      "Iteration 75, loss = 0.08701150\n",
      "Iteration 76, loss = 0.08656978\n",
      "Iteration 77, loss = 0.08556044\n",
      "Iteration 78, loss = 0.08512454\n",
      "Iteration 79, loss = 0.08413431\n",
      "Iteration 80, loss = 0.08260979\n",
      "Iteration 81, loss = 0.08279513\n",
      "Iteration 82, loss = 0.08249417\n",
      "Iteration 83, loss = 0.08051360\n",
      "Iteration 84, loss = 0.08020044\n",
      "Iteration 85, loss = 0.07921481\n",
      "Iteration 86, loss = 0.07809662\n",
      "Iteration 87, loss = 0.07728230\n",
      "Iteration 88, loss = 0.07616527\n",
      "Iteration 89, loss = 0.07532796\n",
      "Iteration 90, loss = 0.07454895\n",
      "Iteration 91, loss = 0.07452342\n",
      "Iteration 92, loss = 0.07324251\n",
      "Iteration 93, loss = 0.07319409\n",
      "Iteration 94, loss = 0.07102824\n",
      "Iteration 95, loss = 0.07045154\n",
      "Iteration 96, loss = 0.06918426\n",
      "Iteration 97, loss = 0.07171052\n",
      "Iteration 98, loss = 0.06884847\n",
      "Iteration 99, loss = 0.06908162\n",
      "Iteration 100, loss = 0.06789859\n",
      "Iteration 101, loss = 0.06745991\n",
      "Iteration 102, loss = 0.06561832\n",
      "Iteration 103, loss = 0.06386140\n",
      "Iteration 104, loss = 0.06563727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 105, loss = 0.06431865\n",
      "Iteration 106, loss = 0.06486588\n",
      "Iteration 107, loss = 0.06383262\n",
      "Iteration 108, loss = 0.06247233\n",
      "Iteration 109, loss = 0.06190643\n",
      "Iteration 110, loss = 0.05994538\n",
      "Iteration 111, loss = 0.05847207\n",
      "Iteration 112, loss = 0.05804708\n",
      "Iteration 113, loss = 0.05846212\n",
      "Iteration 114, loss = 0.05762748\n",
      "Iteration 115, loss = 0.05583888\n",
      "Iteration 116, loss = 0.05562566\n",
      "Iteration 117, loss = 0.05410894\n",
      "Iteration 118, loss = 0.05490112\n",
      "Iteration 119, loss = 0.05336821\n",
      "Iteration 120, loss = 0.05239716\n",
      "Iteration 121, loss = 0.05152057\n",
      "Iteration 122, loss = 0.05108064\n",
      "Iteration 123, loss = 0.05053540\n",
      "Iteration 124, loss = 0.04995985\n",
      "Iteration 125, loss = 0.04930151\n",
      "Iteration 126, loss = 0.04821023\n",
      "Iteration 127, loss = 0.04803984\n",
      "Iteration 128, loss = 0.04707882\n",
      "Iteration 129, loss = 0.04798560\n",
      "Iteration 130, loss = 0.04776577\n",
      "Iteration 131, loss = 0.04634265\n",
      "Iteration 132, loss = 0.04539942\n",
      "Iteration 133, loss = 0.04495711\n",
      "Iteration 134, loss = 0.04434729\n",
      "Iteration 135, loss = 0.04352978\n",
      "Iteration 136, loss = 0.04303287\n",
      "Iteration 137, loss = 0.04201012\n",
      "Iteration 138, loss = 0.04119401\n",
      "Iteration 139, loss = 0.04091336\n",
      "Iteration 140, loss = 0.04114754\n",
      "Iteration 141, loss = 0.04029197\n",
      "Iteration 142, loss = 0.04052913\n",
      "Iteration 143, loss = 0.04052174\n",
      "Iteration 144, loss = 0.04028323\n",
      "Iteration 145, loss = 0.03865040\n",
      "Iteration 146, loss = 0.03830376\n",
      "Iteration 147, loss = 0.03814435\n",
      "Iteration 148, loss = 0.03700992\n",
      "Iteration 149, loss = 0.03567246\n",
      "Iteration 150, loss = 0.03517240\n",
      "Iteration 151, loss = 0.03548237\n",
      "Iteration 152, loss = 0.03554695\n",
      "Iteration 153, loss = 0.03364297\n",
      "Iteration 154, loss = 0.03310434\n",
      "Iteration 155, loss = 0.03314227\n",
      "Iteration 156, loss = 0.03243829\n",
      "Iteration 157, loss = 0.03191852\n",
      "Iteration 158, loss = 0.03149429\n",
      "Iteration 159, loss = 0.03074089\n",
      "Iteration 160, loss = 0.03030304\n",
      "Iteration 161, loss = 0.02999736\n",
      "Iteration 162, loss = 0.02974199\n",
      "Iteration 163, loss = 0.02924566\n",
      "Iteration 164, loss = 0.02874848\n",
      "Iteration 165, loss = 0.02812889\n",
      "Iteration 166, loss = 0.02773787\n",
      "Iteration 167, loss = 0.02778197\n",
      "Iteration 168, loss = 0.02723553\n",
      "Iteration 169, loss = 0.02727917\n",
      "Iteration 170, loss = 0.02652735\n",
      "Iteration 171, loss = 0.02586018\n",
      "Iteration 172, loss = 0.02593258\n",
      "Iteration 173, loss = 0.02525825\n",
      "Iteration 174, loss = 0.02533295\n",
      "Iteration 175, loss = 0.02465308\n",
      "Iteration 176, loss = 0.02487282\n",
      "Iteration 177, loss = 0.02402748\n",
      "Iteration 178, loss = 0.02437050\n",
      "Iteration 179, loss = 0.02340888\n",
      "Iteration 180, loss = 0.02297368\n",
      "Iteration 181, loss = 0.02229215\n",
      "Iteration 182, loss = 0.02195790\n",
      "Iteration 183, loss = 0.02178177\n",
      "Iteration 184, loss = 0.02157775\n",
      "Iteration 185, loss = 0.02162529\n",
      "Iteration 186, loss = 0.02147681\n",
      "Iteration 187, loss = 0.02059115\n",
      "Iteration 188, loss = 0.02085450\n",
      "Iteration 189, loss = 0.02018467\n",
      "Iteration 190, loss = 0.01969862\n",
      "Iteration 191, loss = 0.01974859\n",
      "Iteration 192, loss = 0.01933171\n",
      "Iteration 193, loss = 0.01899024\n",
      "Iteration 194, loss = 0.01919671\n",
      "Iteration 195, loss = 0.01856830\n",
      "Iteration 196, loss = 0.01793858\n",
      "Iteration 197, loss = 0.01798608\n",
      "Iteration 198, loss = 0.01798680\n",
      "Iteration 199, loss = 0.01804047\n",
      "Iteration 200, loss = 0.01707741\n",
      "Iteration 201, loss = 0.01673694\n",
      "Iteration 202, loss = 0.01640270\n",
      "Iteration 203, loss = 0.01617127\n",
      "Iteration 204, loss = 0.01649564\n",
      "Iteration 205, loss = 0.01608561\n",
      "Iteration 206, loss = 0.01594008\n",
      "Iteration 207, loss = 0.01736003\n",
      "Iteration 208, loss = 0.01582127\n",
      "Iteration 209, loss = 0.01530522\n",
      "Iteration 210, loss = 0.01501346\n",
      "Iteration 211, loss = 0.01459719\n",
      "Iteration 212, loss = 0.01435097\n",
      "Iteration 213, loss = 0.01397236\n",
      "Iteration 214, loss = 0.01388859\n",
      "Iteration 215, loss = 0.01350126\n",
      "Iteration 216, loss = 0.01356648\n",
      "Iteration 217, loss = 0.01346887\n",
      "Iteration 218, loss = 0.01352379\n",
      "Iteration 219, loss = 0.01274207\n",
      "Iteration 220, loss = 0.01272101\n",
      "Iteration 221, loss = 0.01245213\n",
      "Iteration 222, loss = 0.01251260\n",
      "Iteration 223, loss = 0.01240604\n",
      "Iteration 224, loss = 0.01204587\n",
      "Iteration 225, loss = 0.01177818\n",
      "Iteration 226, loss = 0.01194372\n",
      "Iteration 227, loss = 0.01145549\n",
      "Iteration 228, loss = 0.01128673\n",
      "Iteration 229, loss = 0.01123788\n",
      "Iteration 230, loss = 0.01096316\n",
      "Iteration 231, loss = 0.01098327\n",
      "Iteration 232, loss = 0.01134716\n",
      "Iteration 233, loss = 0.01086464\n",
      "Iteration 234, loss = 0.01062615\n",
      "Iteration 235, loss = 0.01029042\n",
      "Iteration 236, loss = 0.01010289\n",
      "Iteration 237, loss = 0.01024043\n",
      "Iteration 238, loss = 0.00991257\n",
      "Iteration 239, loss = 0.00970071\n",
      "Iteration 240, loss = 0.00972870\n",
      "Iteration 241, loss = 0.00954748\n",
      "Iteration 242, loss = 0.00931112\n",
      "Iteration 243, loss = 0.00927582\n",
      "Iteration 244, loss = 0.00915940\n",
      "Iteration 245, loss = 0.00900934\n",
      "Iteration 246, loss = 0.00901540\n",
      "Iteration 247, loss = 0.00893600\n",
      "Iteration 248, loss = 0.00876869\n",
      "Iteration 249, loss = 0.00864366\n",
      "Iteration 250, loss = 0.00851717\n",
      "Iteration 251, loss = 0.00840749\n",
      "Iteration 252, loss = 0.00828649\n",
      "Iteration 253, loss = 0.00812944\n",
      "Iteration 254, loss = 0.00812451\n",
      "Iteration 255, loss = 0.00791588\n",
      "Iteration 256, loss = 0.00785255\n",
      "Iteration 257, loss = 0.00779193\n",
      "Iteration 258, loss = 0.00771048\n",
      "Iteration 259, loss = 0.00781777\n",
      "Iteration 260, loss = 0.00777313\n",
      "Iteration 261, loss = 0.00760891\n",
      "Iteration 262, loss = 0.00745831\n",
      "Iteration 263, loss = 0.00719988\n",
      "Iteration 264, loss = 0.00712099\n",
      "Iteration 265, loss = 0.00699206\n",
      "Iteration 266, loss = 0.00688566\n",
      "Iteration 267, loss = 0.00679091\n",
      "Iteration 268, loss = 0.00673560\n",
      "Iteration 269, loss = 0.00664645\n",
      "Iteration 270, loss = 0.00659073\n",
      "Iteration 271, loss = 0.00653273\n",
      "Iteration 272, loss = 0.00659955\n",
      "Iteration 273, loss = 0.00642740\n",
      "Iteration 274, loss = 0.00627403\n",
      "Iteration 275, loss = 0.00628061\n",
      "Iteration 276, loss = 0.00623083\n",
      "Iteration 277, loss = 0.00618324\n",
      "Iteration 278, loss = 0.00609714\n",
      "Iteration 279, loss = 0.00598324\n",
      "Iteration 280, loss = 0.00604571\n",
      "Iteration 281, loss = 0.00591047\n",
      "Iteration 282, loss = 0.00606466\n",
      "Iteration 283, loss = 0.00576051\n",
      "Iteration 284, loss = 0.00575333\n",
      "Iteration 285, loss = 0.00561520\n",
      "Iteration 286, loss = 0.00549340\n",
      "Iteration 287, loss = 0.00546657\n",
      "Iteration 288, loss = 0.00555176\n",
      "Iteration 289, loss = 0.00546353\n",
      "Iteration 290, loss = 0.00536436\n",
      "Iteration 291, loss = 0.00522665\n",
      "Iteration 292, loss = 0.00525853\n",
      "Iteration 293, loss = 0.00513550\n",
      "Iteration 294, loss = 0.00529894\n",
      "Iteration 295, loss = 0.00499542\n",
      "Iteration 296, loss = 0.00492936\n",
      "Iteration 297, loss = 0.00485233\n",
      "Iteration 298, loss = 0.00484646\n",
      "Iteration 299, loss = 0.00478907\n",
      "Iteration 300, loss = 0.00480575\n",
      "Iteration 301, loss = 0.00470220\n",
      "Iteration 302, loss = 0.00463480\n",
      "Iteration 303, loss = 0.00456057\n",
      "Iteration 304, loss = 0.00454608\n",
      "Iteration 305, loss = 0.00454875\n",
      "Iteration 306, loss = 0.00449459\n",
      "Iteration 307, loss = 0.00447704\n",
      "Iteration 308, loss = 0.00437458\n",
      "Iteration 309, loss = 0.00433254\n",
      "Iteration 310, loss = 0.00428095\n",
      "Iteration 311, loss = 0.00431634\n",
      "Iteration 312, loss = 0.00420361\n",
      "Iteration 313, loss = 0.00423140\n",
      "Iteration 314, loss = 0.00411589\n",
      "Iteration 315, loss = 0.00413166\n",
      "Iteration 316, loss = 0.00405570\n",
      "Iteration 317, loss = 0.00401167\n",
      "Iteration 318, loss = 0.00396093\n",
      "Iteration 319, loss = 0.00394609\n",
      "Iteration 320, loss = 0.00389288\n",
      "Iteration 321, loss = 0.00388385\n",
      "Iteration 322, loss = 0.00390422\n",
      "Iteration 323, loss = 0.00391709\n",
      "Iteration 324, loss = 0.00380932\n",
      "Iteration 325, loss = 0.00371837\n",
      "Iteration 326, loss = 0.00369143\n",
      "Iteration 327, loss = 0.00362884\n",
      "Iteration 328, loss = 0.00358703\n",
      "Iteration 329, loss = 0.00356292\n",
      "Iteration 330, loss = 0.00352595\n",
      "Iteration 331, loss = 0.00348937\n",
      "Iteration 332, loss = 0.00350386\n",
      "Iteration 333, loss = 0.00345324\n",
      "Iteration 334, loss = 0.00341508\n",
      "Iteration 335, loss = 0.00338525\n",
      "Iteration 336, loss = 0.00336407\n",
      "Iteration 337, loss = 0.00332693\n",
      "Iteration 338, loss = 0.00329145\n",
      "Iteration 339, loss = 0.00327483\n",
      "Iteration 340, loss = 0.00323433\n",
      "Iteration 341, loss = 0.00322459\n",
      "Iteration 342, loss = 0.00317790\n",
      "Iteration 343, loss = 0.00316676\n",
      "Iteration 344, loss = 0.00313881\n",
      "Iteration 345, loss = 0.00312102\n",
      "Iteration 346, loss = 0.00307944\n",
      "Iteration 347, loss = 0.00306539\n",
      "Iteration 348, loss = 0.00303947\n",
      "Iteration 349, loss = 0.00299477\n",
      "Iteration 350, loss = 0.00297151\n",
      "Iteration 351, loss = 0.00294115\n",
      "Iteration 352, loss = 0.00293586\n",
      "Iteration 353, loss = 0.00291127\n",
      "Iteration 354, loss = 0.00288434\n",
      "Iteration 355, loss = 0.00286886\n",
      "Iteration 356, loss = 0.00284328\n",
      "Iteration 357, loss = 0.00281848\n",
      "Iteration 358, loss = 0.00280433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 359, loss = 0.00277841\n",
      "Iteration 360, loss = 0.00274764\n",
      "Iteration 361, loss = 0.00277648\n",
      "Iteration 362, loss = 0.00271633\n",
      "Iteration 363, loss = 0.00267524\n",
      "Iteration 364, loss = 0.00266716\n",
      "Iteration 365, loss = 0.00267331\n",
      "Iteration 366, loss = 0.00262774\n",
      "Iteration 367, loss = 0.00260924\n",
      "Iteration 368, loss = 0.00259344\n",
      "Iteration 369, loss = 0.00258914\n",
      "Iteration 370, loss = 0.00254514\n",
      "Iteration 371, loss = 0.00253350\n",
      "Iteration 372, loss = 0.00250469\n",
      "Iteration 373, loss = 0.00250971\n",
      "Iteration 374, loss = 0.00247176\n",
      "Iteration 375, loss = 0.00245487\n",
      "Iteration 376, loss = 0.00244609\n",
      "Iteration 377, loss = 0.00241014\n",
      "Iteration 378, loss = 0.00241935\n",
      "Iteration 379, loss = 0.00239852\n",
      "Iteration 380, loss = 0.00239171\n",
      "Iteration 381, loss = 0.00235586\n",
      "Iteration 382, loss = 0.00233502\n",
      "Iteration 383, loss = 0.00231446\n",
      "Iteration 384, loss = 0.00230045\n",
      "Iteration 385, loss = 0.00229049\n",
      "Iteration 386, loss = 0.00226780\n",
      "Iteration 387, loss = 0.00225216\n",
      "Iteration 388, loss = 0.00225363\n",
      "Iteration 389, loss = 0.00222388\n",
      "Iteration 390, loss = 0.00220577\n",
      "Iteration 391, loss = 0.00219574\n",
      "Iteration 392, loss = 0.00217729\n",
      "Iteration 393, loss = 0.00217063\n",
      "Iteration 394, loss = 0.00215152\n",
      "Iteration 395, loss = 0.00214217\n",
      "Iteration 396, loss = 0.00214720\n",
      "Iteration 397, loss = 0.00212656\n",
      "Iteration 398, loss = 0.00209162\n",
      "Iteration 399, loss = 0.00212155\n",
      "Iteration 400, loss = 0.00207884\n",
      "Iteration 401, loss = 0.00206707\n",
      "Iteration 402, loss = 0.00203336\n",
      "Iteration 403, loss = 0.00203654\n",
      "Iteration 404, loss = 0.00200922\n",
      "Iteration 405, loss = 0.00199472\n",
      "Iteration 406, loss = 0.00198326\n",
      "Iteration 407, loss = 0.00200342\n",
      "Iteration 408, loss = 0.00197476\n",
      "Iteration 409, loss = 0.00195741\n",
      "Iteration 410, loss = 0.00193231\n",
      "Iteration 411, loss = 0.00192464\n",
      "Iteration 412, loss = 0.00191827\n",
      "Iteration 413, loss = 0.00190742\n",
      "Iteration 414, loss = 0.00189533\n",
      "Iteration 415, loss = 0.00189628\n",
      "Iteration 416, loss = 0.00186863\n",
      "Iteration 417, loss = 0.00187292\n",
      "Iteration 418, loss = 0.00186728\n",
      "Iteration 419, loss = 0.00185187\n",
      "Iteration 420, loss = 0.00182882\n",
      "Iteration 421, loss = 0.00182663\n",
      "Iteration 422, loss = 0.00182849\n",
      "Iteration 423, loss = 0.00180527\n",
      "Iteration 424, loss = 0.00180954\n",
      "Iteration 425, loss = 0.00178840\n",
      "Iteration 426, loss = 0.00176235\n",
      "Iteration 427, loss = 0.00175282\n",
      "Iteration 428, loss = 0.00174679\n",
      "Iteration 429, loss = 0.00173028\n",
      "Iteration 430, loss = 0.00172619\n",
      "Iteration 431, loss = 0.00170741\n",
      "Iteration 432, loss = 0.00170659\n",
      "Iteration 433, loss = 0.00169723\n",
      "Iteration 434, loss = 0.00169744\n",
      "Iteration 435, loss = 0.00168241\n",
      "Iteration 436, loss = 0.00169039\n",
      "Iteration 437, loss = 0.00165691\n",
      "Iteration 438, loss = 0.00164754\n",
      "Iteration 439, loss = 0.00164914\n",
      "Iteration 440, loss = 0.00165599\n",
      "Iteration 441, loss = 0.00164526\n",
      "Iteration 442, loss = 0.00161085\n",
      "Iteration 443, loss = 0.00160289\n",
      "Iteration 444, loss = 0.00159219\n",
      "Iteration 445, loss = 0.00158668\n",
      "Iteration 446, loss = 0.00157871\n",
      "Iteration 447, loss = 0.00157931\n",
      "Iteration 448, loss = 0.00156435\n",
      "Iteration 449, loss = 0.00156026\n",
      "Iteration 450, loss = 0.00155649\n",
      "Iteration 451, loss = 0.00154234\n",
      "Iteration 452, loss = 0.00153853\n",
      "Iteration 453, loss = 0.00153822\n",
      "Iteration 454, loss = 0.00151331\n",
      "Iteration 455, loss = 0.00151515\n",
      "Iteration 456, loss = 0.00150000\n",
      "Iteration 457, loss = 0.00149888\n",
      "Iteration 458, loss = 0.00149474\n",
      "Iteration 459, loss = 0.00148700\n",
      "Iteration 460, loss = 0.00147223\n",
      "Iteration 461, loss = 0.00146381\n",
      "Iteration 462, loss = 0.00146005\n",
      "Iteration 463, loss = 0.00145237\n",
      "Iteration 464, loss = 0.00144454\n",
      "Iteration 465, loss = 0.00143565\n",
      "Iteration 466, loss = 0.00142792\n",
      "Iteration 467, loss = 0.00142551\n",
      "Iteration 468, loss = 0.00141895\n",
      "Iteration 469, loss = 0.00140986\n",
      "Iteration 470, loss = 0.00140277\n",
      "Iteration 471, loss = 0.00139754\n",
      "Iteration 472, loss = 0.00139262\n",
      "Iteration 473, loss = 0.00138738\n",
      "Iteration 474, loss = 0.00138490\n",
      "Iteration 475, loss = 0.00137453\n",
      "Iteration 476, loss = 0.00137426\n",
      "Iteration 477, loss = 0.00136353\n",
      "Iteration 478, loss = 0.00136679\n",
      "Iteration 479, loss = 0.00135356\n",
      "Iteration 480, loss = 0.00134273\n",
      "Iteration 481, loss = 0.00134354\n",
      "Iteration 482, loss = 0.00133978\n",
      "Iteration 483, loss = 0.00134787\n",
      "Iteration 484, loss = 0.00132883\n",
      "Iteration 485, loss = 0.00131660\n",
      "Iteration 486, loss = 0.00131171\n",
      "Iteration 487, loss = 0.00131886\n",
      "Iteration 488, loss = 0.00129898\n",
      "Iteration 489, loss = 0.00129409\n",
      "Iteration 490, loss = 0.00128926\n",
      "Iteration 491, loss = 0.00128516\n",
      "Iteration 492, loss = 0.00127883\n",
      "Iteration 493, loss = 0.00127513\n",
      "Iteration 494, loss = 0.00127458\n",
      "Iteration 495, loss = 0.00126483\n",
      "Iteration 496, loss = 0.00126116\n",
      "Iteration 497, loss = 0.00125502\n",
      "Iteration 498, loss = 0.00124698\n",
      "Iteration 499, loss = 0.00124397\n",
      "Iteration 500, loss = 0.00123820\n",
      "Iteration 501, loss = 0.00123770\n",
      "Iteration 502, loss = 0.00123244\n",
      "Iteration 503, loss = 0.00122592\n",
      "Iteration 504, loss = 0.00121950\n",
      "Iteration 505, loss = 0.00121434\n",
      "Iteration 506, loss = 0.00121015\n",
      "Iteration 507, loss = 0.00120847\n",
      "Iteration 508, loss = 0.00120302\n",
      "Iteration 509, loss = 0.00120239\n",
      "Iteration 510, loss = 0.00119216\n",
      "Iteration 511, loss = 0.00118931\n",
      "Iteration 512, loss = 0.00118432\n",
      "Iteration 513, loss = 0.00118195\n",
      "Iteration 514, loss = 0.00117827\n",
      "Iteration 515, loss = 0.00117088\n",
      "Iteration 516, loss = 0.00116804\n",
      "Iteration 517, loss = 0.00116280\n",
      "Iteration 518, loss = 0.00115902\n",
      "Iteration 519, loss = 0.00115884\n",
      "Iteration 520, loss = 0.00115345\n",
      "Iteration 521, loss = 0.00114856\n",
      "Iteration 522, loss = 0.00114384\n",
      "Iteration 523, loss = 0.00114037\n",
      "Iteration 524, loss = 0.00113749\n",
      "Iteration 525, loss = 0.00113527\n",
      "Iteration 526, loss = 0.00112743\n",
      "Iteration 527, loss = 0.00112655\n",
      "Iteration 528, loss = 0.00112460\n",
      "Iteration 529, loss = 0.00111774\n",
      "Iteration 530, loss = 0.00112118\n",
      "Iteration 531, loss = 0.00111071\n",
      "Iteration 532, loss = 0.00110850\n",
      "Iteration 533, loss = 0.00110549\n",
      "Iteration 534, loss = 0.00110217\n",
      "Iteration 535, loss = 0.00110037\n",
      "Iteration 536, loss = 0.00109310\n",
      "Iteration 537, loss = 0.00109075\n",
      "Iteration 538, loss = 0.00108513\n",
      "Iteration 539, loss = 0.00108328\n",
      "Iteration 540, loss = 0.00107898\n",
      "Iteration 541, loss = 0.00107427\n",
      "Iteration 542, loss = 0.00107917\n",
      "Iteration 543, loss = 0.00108220\n",
      "Iteration 544, loss = 0.00106733\n",
      "Iteration 545, loss = 0.00106229\n",
      "Iteration 546, loss = 0.00106312\n",
      "Iteration 547, loss = 0.00105983\n",
      "Iteration 548, loss = 0.00105723\n",
      "Iteration 549, loss = 0.00105126\n",
      "Iteration 550, loss = 0.00104708\n",
      "Iteration 551, loss = 0.00104562\n",
      "Iteration 552, loss = 0.00104225\n",
      "Iteration 553, loss = 0.00104271\n",
      "Iteration 554, loss = 0.00103699\n",
      "Iteration 555, loss = 0.00103511\n",
      "Iteration 556, loss = 0.00102873\n",
      "Iteration 557, loss = 0.00102891\n",
      "Iteration 558, loss = 0.00102362\n",
      "Iteration 559, loss = 0.00102080\n",
      "Iteration 560, loss = 0.00101835\n",
      "Iteration 561, loss = 0.00102113\n",
      "Iteration 562, loss = 0.00101450\n",
      "Iteration 563, loss = 0.00100983\n",
      "Iteration 564, loss = 0.00100847\n",
      "Iteration 565, loss = 0.00100548\n",
      "Iteration 566, loss = 0.00100177\n",
      "Iteration 567, loss = 0.00099898\n",
      "Iteration 568, loss = 0.00099473\n",
      "Iteration 569, loss = 0.00099478\n",
      "Iteration 570, loss = 0.00099479\n",
      "Iteration 571, loss = 0.00099591\n",
      "Iteration 572, loss = 0.00098728\n",
      "Iteration 573, loss = 0.00098348\n",
      "Iteration 574, loss = 0.00098400\n",
      "Iteration 575, loss = 0.00097858\n",
      "Iteration 576, loss = 0.00097574\n",
      "Iteration 577, loss = 0.00097292\n",
      "Iteration 578, loss = 0.00097233\n",
      "Iteration 579, loss = 0.00096758\n",
      "Iteration 580, loss = 0.00096706\n",
      "Iteration 581, loss = 0.00096385\n",
      "Iteration 582, loss = 0.00096090\n",
      "Iteration 583, loss = 0.00095858\n",
      "Iteration 584, loss = 0.00095998\n",
      "Iteration 585, loss = 0.00096075\n",
      "Iteration 586, loss = 0.00095197\n",
      "Iteration 587, loss = 0.00095047\n",
      "Iteration 588, loss = 0.00095237\n",
      "Iteration 589, loss = 0.00094667\n",
      "Iteration 590, loss = 0.00094450\n",
      "Iteration 591, loss = 0.00094804\n",
      "Iteration 592, loss = 0.00094193\n",
      "Iteration 593, loss = 0.00093657\n",
      "Iteration 594, loss = 0.00093418\n",
      "Iteration 595, loss = 0.00093133\n",
      "Iteration 596, loss = 0.00093176\n",
      "Iteration 597, loss = 0.00092985\n",
      "Iteration 598, loss = 0.00092689\n",
      "Iteration 599, loss = 0.00092489\n",
      "Iteration 600, loss = 0.00092278\n",
      "Iteration 601, loss = 0.00092013\n",
      "Iteration 602, loss = 0.00091941\n",
      "Iteration 603, loss = 0.00091752\n",
      "Iteration 604, loss = 0.00091448\n",
      "Iteration 605, loss = 0.00091299\n",
      "Iteration 606, loss = 0.00091013\n",
      "Iteration 607, loss = 0.00091122\n",
      "Iteration 608, loss = 0.00090634\n",
      "Iteration 609, loss = 0.00090511\n",
      "Iteration 610, loss = 0.00090317\n",
      "Iteration 611, loss = 0.00090229\n",
      "Iteration 612, loss = 0.00090215\n",
      "Iteration 613, loss = 0.00090177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 614, loss = 0.00089728\n",
      "Iteration 615, loss = 0.00089341\n",
      "Iteration 616, loss = 0.00089223\n",
      "Iteration 617, loss = 0.00089072\n",
      "Iteration 618, loss = 0.00089255\n",
      "Iteration 619, loss = 0.00089036\n",
      "Iteration 620, loss = 0.00088989\n",
      "Iteration 621, loss = 0.00088488\n",
      "Iteration 622, loss = 0.00088119\n",
      "Iteration 623, loss = 0.00088162\n",
      "Iteration 624, loss = 0.00087929\n",
      "Iteration 625, loss = 0.00087629\n",
      "Iteration 626, loss = 0.00087581\n",
      "Iteration 627, loss = 0.00087413\n",
      "Iteration 628, loss = 0.00087198\n",
      "Iteration 629, loss = 0.00087191\n",
      "Iteration 630, loss = 0.00086989\n",
      "Iteration 631, loss = 0.00086628\n",
      "Iteration 632, loss = 0.00086504\n",
      "Iteration 633, loss = 0.00086668\n",
      "Iteration 634, loss = 0.00086294\n",
      "Iteration 635, loss = 0.00086490\n",
      "Iteration 636, loss = 0.00086146\n",
      "Iteration 637, loss = 0.00086256\n",
      "Iteration 638, loss = 0.00086028\n",
      "Iteration 639, loss = 0.00085544\n",
      "Iteration 640, loss = 0.00085260\n",
      "Iteration 641, loss = 0.00085383\n",
      "Iteration 642, loss = 0.00085146\n",
      "Iteration 643, loss = 0.00084990\n",
      "Iteration 644, loss = 0.00084779\n",
      "Iteration 645, loss = 0.00084693\n",
      "Iteration 646, loss = 0.00085076\n",
      "Iteration 647, loss = 0.00084158\n",
      "Iteration 648, loss = 0.00084074\n",
      "Iteration 649, loss = 0.00083924\n",
      "Iteration 650, loss = 0.00084050\n",
      "Iteration 651, loss = 0.00083819\n",
      "Iteration 652, loss = 0.00083625\n",
      "Iteration 653, loss = 0.00083711\n",
      "Iteration 654, loss = 0.00083711\n",
      "Iteration 655, loss = 0.00083404\n",
      "Iteration 656, loss = 0.00082944\n",
      "Iteration 657, loss = 0.00082842\n",
      "Iteration 658, loss = 0.00082872\n",
      "Iteration 659, loss = 0.00082567\n",
      "Iteration 660, loss = 0.00082399\n",
      "Iteration 661, loss = 0.00082206\n",
      "Iteration 662, loss = 0.00082119\n",
      "Iteration 663, loss = 0.00082014\n",
      "Iteration 664, loss = 0.00081846\n",
      "Iteration 665, loss = 0.00081745\n",
      "Iteration 666, loss = 0.00081576\n",
      "Iteration 667, loss = 0.00081599\n",
      "Iteration 668, loss = 0.00081335\n",
      "Iteration 669, loss = 0.00081362\n",
      "Iteration 670, loss = 0.00081013\n",
      "Iteration 671, loss = 0.00080916\n",
      "Iteration 672, loss = 0.00080749\n",
      "Iteration 673, loss = 0.00080790\n",
      "Iteration 674, loss = 0.00080609\n",
      "Iteration 675, loss = 0.00080520\n",
      "Iteration 676, loss = 0.00080484\n",
      "Iteration 677, loss = 0.00080153\n",
      "Iteration 678, loss = 0.00080229\n",
      "Iteration 679, loss = 0.00080294\n",
      "Iteration 680, loss = 0.00080274\n",
      "Iteration 681, loss = 0.00079768\n",
      "Iteration 682, loss = 0.00079687\n",
      "Iteration 683, loss = 0.00079607\n",
      "Iteration 684, loss = 0.00079589\n",
      "Iteration 685, loss = 0.00079636\n",
      "Iteration 686, loss = 0.00079098\n",
      "Iteration 687, loss = 0.00079200\n",
      "Iteration 688, loss = 0.00079226\n",
      "Iteration 689, loss = 0.00079206\n",
      "Iteration 690, loss = 0.00078753\n",
      "Iteration 691, loss = 0.00078790\n",
      "Iteration 692, loss = 0.00078597\n",
      "Iteration 693, loss = 0.00078450\n",
      "Iteration 694, loss = 0.00078404\n",
      "Iteration 695, loss = 0.00078145\n",
      "Iteration 696, loss = 0.00078055\n",
      "Iteration 697, loss = 0.00077947\n",
      "Iteration 698, loss = 0.00077894\n",
      "Iteration 699, loss = 0.00077899\n",
      "Iteration 700, loss = 0.00077755\n",
      "Iteration 701, loss = 0.00077486\n",
      "Iteration 702, loss = 0.00077578\n",
      "Iteration 703, loss = 0.00077414\n",
      "Iteration 704, loss = 0.00077244\n",
      "Iteration 705, loss = 0.00077125\n",
      "Iteration 706, loss = 0.00077139\n",
      "Iteration 707, loss = 0.00077226\n",
      "Iteration 708, loss = 0.00076948\n",
      "Iteration 709, loss = 0.00076700\n",
      "Iteration 710, loss = 0.00076540\n",
      "Iteration 711, loss = 0.00076526\n",
      "Iteration 712, loss = 0.00076502\n",
      "Iteration 713, loss = 0.00076360\n",
      "Iteration 714, loss = 0.00076506\n",
      "Iteration 715, loss = 0.00076158\n",
      "Iteration 716, loss = 0.00076053\n",
      "Iteration 717, loss = 0.00075909\n",
      "Iteration 718, loss = 0.00075876\n",
      "Iteration 719, loss = 0.00075790\n",
      "Iteration 720, loss = 0.00075643\n",
      "Iteration 721, loss = 0.00075572\n",
      "Iteration 722, loss = 0.00075401\n",
      "Iteration 723, loss = 0.00075279\n",
      "Iteration 724, loss = 0.00075216\n",
      "Iteration 725, loss = 0.00075203\n",
      "Iteration 726, loss = 0.00075390\n",
      "Iteration 727, loss = 0.00075175\n",
      "Iteration 728, loss = 0.00075265\n",
      "Iteration 729, loss = 0.00074797\n",
      "Iteration 730, loss = 0.00075040\n",
      "Iteration 731, loss = 0.00074720\n",
      "Iteration 732, loss = 0.00074489\n",
      "Iteration 733, loss = 0.00074370\n",
      "Iteration 734, loss = 0.00074230\n",
      "Iteration 735, loss = 0.00074141\n",
      "Iteration 736, loss = 0.00074207\n",
      "Iteration 737, loss = 0.00074189\n",
      "Iteration 738, loss = 0.00074075\n",
      "Iteration 739, loss = 0.00073922\n",
      "Iteration 740, loss = 0.00073654\n",
      "Iteration 741, loss = 0.00073618\n",
      "Iteration 742, loss = 0.00073592\n",
      "Iteration 743, loss = 0.00073714\n",
      "Iteration 744, loss = 0.00073522\n",
      "Iteration 745, loss = 0.00073322\n",
      "Iteration 746, loss = 0.00073183\n",
      "Iteration 747, loss = 0.00073211\n",
      "Iteration 748, loss = 0.00073121\n",
      "Iteration 749, loss = 0.00072968\n",
      "Iteration 750, loss = 0.00072882\n",
      "Iteration 751, loss = 0.00072718\n",
      "Iteration 752, loss = 0.00072706\n",
      "Iteration 753, loss = 0.00072583\n",
      "Iteration 754, loss = 0.00072482\n",
      "Iteration 755, loss = 0.00072402\n",
      "Iteration 756, loss = 0.00072341\n",
      "Iteration 757, loss = 0.00072329\n",
      "Iteration 758, loss = 0.00072165\n",
      "Iteration 759, loss = 0.00072033\n",
      "Iteration 760, loss = 0.00071982\n",
      "Iteration 761, loss = 0.00071917\n",
      "Iteration 762, loss = 0.00071913\n",
      "Iteration 763, loss = 0.00071867\n",
      "Iteration 764, loss = 0.00071737\n",
      "Iteration 765, loss = 0.00071604\n",
      "Iteration 766, loss = 0.00071453\n",
      "Iteration 767, loss = 0.00071534\n",
      "Iteration 768, loss = 0.00071316\n",
      "Iteration 769, loss = 0.00071330\n",
      "Iteration 770, loss = 0.00071199\n",
      "Iteration 771, loss = 0.00071092\n",
      "Iteration 772, loss = 0.00071093\n",
      "Iteration 773, loss = 0.00070987\n",
      "Iteration 774, loss = 0.00071044\n",
      "Iteration 775, loss = 0.00070814\n",
      "Iteration 776, loss = 0.00070744\n",
      "Iteration 777, loss = 0.00070585\n",
      "Iteration 778, loss = 0.00070650\n",
      "Iteration 779, loss = 0.00070671\n",
      "Iteration 780, loss = 0.00070356\n",
      "Iteration 781, loss = 0.00070301\n",
      "Iteration 782, loss = 0.00070214\n",
      "Iteration 783, loss = 0.00070252\n",
      "Iteration 784, loss = 0.00070149\n",
      "Iteration 785, loss = 0.00070200\n",
      "Iteration 786, loss = 0.00069892\n",
      "Iteration 787, loss = 0.00069828\n",
      "Iteration 788, loss = 0.00069698\n",
      "Iteration 789, loss = 0.00069690\n",
      "Iteration 790, loss = 0.00069598\n",
      "Iteration 791, loss = 0.00069484\n",
      "Iteration 792, loss = 0.00069477\n",
      "Iteration 793, loss = 0.00069451\n",
      "Iteration 794, loss = 0.00069381\n",
      "Iteration 795, loss = 0.00069336\n",
      "Iteration 796, loss = 0.00069538\n",
      "Iteration 797, loss = 0.00069377\n",
      "Iteration 798, loss = 0.00069121\n",
      "Iteration 799, loss = 0.00068925\n",
      "Iteration 800, loss = 0.00068855\n",
      "Iteration 801, loss = 0.00068709\n",
      "Iteration 802, loss = 0.00068827\n",
      "Iteration 803, loss = 0.00068622\n",
      "Iteration 804, loss = 0.00068528\n",
      "Iteration 805, loss = 0.00068532\n",
      "Iteration 806, loss = 0.00068608\n",
      "Iteration 807, loss = 0.00068398\n",
      "Iteration 808, loss = 0.00068264\n",
      "Iteration 809, loss = 0.00068270\n",
      "Iteration 810, loss = 0.00068139\n",
      "Iteration 811, loss = 0.00068006\n",
      "Iteration 812, loss = 0.00068028\n",
      "Iteration 813, loss = 0.00068035\n",
      "Iteration 814, loss = 0.00067858\n",
      "Iteration 815, loss = 0.00067742\n",
      "Iteration 816, loss = 0.00067665\n",
      "Iteration 817, loss = 0.00067854\n",
      "Iteration 818, loss = 0.00067665\n",
      "Iteration 819, loss = 0.00067510\n",
      "Iteration 820, loss = 0.00067450\n",
      "Iteration 821, loss = 0.00067277\n",
      "Iteration 822, loss = 0.00067256\n",
      "Iteration 823, loss = 0.00067176\n",
      "Iteration 824, loss = 0.00067049\n",
      "Iteration 825, loss = 0.00066991\n",
      "Iteration 826, loss = 0.00066932\n",
      "Iteration 827, loss = 0.00066879\n",
      "Iteration 828, loss = 0.00066828\n",
      "Iteration 829, loss = 0.00066679\n",
      "Iteration 830, loss = 0.00066651\n",
      "Iteration 831, loss = 0.00066727\n",
      "Iteration 832, loss = 0.00066474\n",
      "Iteration 833, loss = 0.00066406\n",
      "Iteration 834, loss = 0.00066338\n",
      "Iteration 835, loss = 0.00066251\n",
      "Iteration 836, loss = 0.00066232\n",
      "Iteration 837, loss = 0.00066149\n",
      "Iteration 838, loss = 0.00066036\n",
      "Iteration 839, loss = 0.00066073\n",
      "Iteration 840, loss = 0.00066082\n",
      "Iteration 841, loss = 0.00066283\n",
      "Iteration 842, loss = 0.00066156\n",
      "Iteration 843, loss = 0.00065938\n",
      "Iteration 844, loss = 0.00065740\n",
      "Iteration 845, loss = 0.00065878\n",
      "Iteration 846, loss = 0.00065473\n",
      "Iteration 847, loss = 0.00065446\n",
      "Iteration 848, loss = 0.00065355\n",
      "Iteration 849, loss = 0.00065281\n",
      "Iteration 850, loss = 0.00065304\n",
      "Iteration 851, loss = 0.00065364\n",
      "Iteration 852, loss = 0.00065184\n",
      "Iteration 853, loss = 0.00065177\n",
      "Iteration 854, loss = 0.00064934\n",
      "Iteration 855, loss = 0.00065075\n",
      "Iteration 856, loss = 0.00065002\n",
      "Iteration 857, loss = 0.00064752\n",
      "Iteration 858, loss = 0.00064643\n",
      "Iteration 859, loss = 0.00064613\n",
      "Iteration 860, loss = 0.00064598\n",
      "Iteration 861, loss = 0.00064480\n",
      "Iteration 862, loss = 0.00064451\n",
      "Iteration 863, loss = 0.00064384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 864, loss = 0.00064239\n",
      "Iteration 865, loss = 0.00064274\n",
      "Iteration 866, loss = 0.00064262\n",
      "Iteration 867, loss = 0.00064081\n",
      "Iteration 868, loss = 0.00064101\n",
      "Iteration 869, loss = 0.00063891\n",
      "Iteration 870, loss = 0.00063872\n",
      "Iteration 871, loss = 0.00063992\n",
      "Iteration 872, loss = 0.00063757\n",
      "Iteration 873, loss = 0.00063777\n",
      "Iteration 874, loss = 0.00063759\n",
      "Iteration 875, loss = 0.00063563\n",
      "Iteration 876, loss = 0.00063536\n",
      "Iteration 877, loss = 0.00063490\n",
      "Iteration 878, loss = 0.00063380\n",
      "Iteration 879, loss = 0.00063223\n",
      "Iteration 880, loss = 0.00063255\n",
      "Iteration 881, loss = 0.00063171\n",
      "Iteration 882, loss = 0.00063009\n",
      "Iteration 883, loss = 0.00062943\n",
      "Iteration 884, loss = 0.00062838\n",
      "Iteration 885, loss = 0.00062791\n",
      "Iteration 886, loss = 0.00062752\n",
      "Iteration 887, loss = 0.00062725\n",
      "Iteration 888, loss = 0.00062604\n",
      "Iteration 889, loss = 0.00062587\n",
      "Iteration 890, loss = 0.00062461\n",
      "Iteration 891, loss = 0.00062396\n",
      "Iteration 892, loss = 0.00062354\n",
      "Iteration 893, loss = 0.00062398\n",
      "Iteration 894, loss = 0.00062176\n",
      "Iteration 895, loss = 0.00062361\n",
      "Iteration 896, loss = 0.00062396\n",
      "Iteration 897, loss = 0.00062166\n",
      "Iteration 898, loss = 0.00062134\n",
      "Iteration 899, loss = 0.00062031\n",
      "Iteration 900, loss = 0.00061839\n",
      "Iteration 901, loss = 0.00061856\n",
      "Iteration 902, loss = 0.00061825\n",
      "Iteration 903, loss = 0.00061631\n",
      "Iteration 904, loss = 0.00061673\n",
      "Iteration 905, loss = 0.00061437\n",
      "Iteration 906, loss = 0.00061365\n",
      "Iteration 907, loss = 0.00061361\n",
      "Iteration 908, loss = 0.00061236\n",
      "Iteration 909, loss = 0.00061280\n",
      "Iteration 910, loss = 0.00061245\n",
      "Iteration 911, loss = 0.00061028\n",
      "Iteration 912, loss = 0.00061061\n",
      "Iteration 913, loss = 0.00061161\n",
      "Iteration 914, loss = 0.00061076\n",
      "Iteration 915, loss = 0.00060959\n",
      "Iteration 916, loss = 0.00060788\n",
      "Iteration 917, loss = 0.00060911\n",
      "Iteration 918, loss = 0.00060820\n",
      "Iteration 919, loss = 0.00060566\n",
      "Iteration 920, loss = 0.00060531\n",
      "Iteration 921, loss = 0.00060563\n",
      "Iteration 922, loss = 0.00060503\n",
      "Iteration 923, loss = 0.00060348\n",
      "Iteration 924, loss = 0.00060158\n",
      "Iteration 925, loss = 0.00060174\n",
      "Iteration 926, loss = 0.00060060\n",
      "Iteration 927, loss = 0.00059998\n",
      "Iteration 928, loss = 0.00059908\n",
      "Iteration 929, loss = 0.00059863\n",
      "Iteration 930, loss = 0.00059846\n",
      "Iteration 931, loss = 0.00059758\n",
      "Iteration 932, loss = 0.00059663\n",
      "Iteration 933, loss = 0.00059760\n",
      "Iteration 934, loss = 0.00059729\n",
      "Iteration 935, loss = 0.00059616\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70905376\n",
      "Iteration 2, loss = 0.37093448\n",
      "Iteration 3, loss = 0.31867239\n",
      "Iteration 4, loss = 0.25565645\n",
      "Iteration 5, loss = 0.23415344\n",
      "Iteration 6, loss = 0.21533572\n",
      "Iteration 7, loss = 0.20335704\n",
      "Iteration 8, loss = 0.19341487\n",
      "Iteration 9, loss = 0.18556727\n",
      "Iteration 10, loss = 0.17902634\n",
      "Iteration 11, loss = 0.17375132\n",
      "Iteration 12, loss = 0.16864299\n",
      "Iteration 13, loss = 0.16479661\n",
      "Iteration 14, loss = 0.16115395\n",
      "Iteration 15, loss = 0.15799212\n",
      "Iteration 16, loss = 0.15527066\n",
      "Iteration 17, loss = 0.15362738\n",
      "Iteration 18, loss = 0.15123712\n",
      "Iteration 19, loss = 0.14918120\n",
      "Iteration 20, loss = 0.14634127\n",
      "Iteration 21, loss = 0.14435661\n",
      "Iteration 22, loss = 0.14331864\n",
      "Iteration 23, loss = 0.14152976\n",
      "Iteration 24, loss = 0.13941180\n",
      "Iteration 25, loss = 0.13824817\n",
      "Iteration 26, loss = 0.13718027\n",
      "Iteration 27, loss = 0.13556659\n",
      "Iteration 28, loss = 0.13465857\n",
      "Iteration 29, loss = 0.13310320\n",
      "Iteration 30, loss = 0.13179507\n",
      "Iteration 31, loss = 0.13158083\n",
      "Iteration 32, loss = 0.12966624\n",
      "Iteration 33, loss = 0.12925118\n",
      "Iteration 34, loss = 0.12738703\n",
      "Iteration 35, loss = 0.12617282\n",
      "Iteration 36, loss = 0.12535498\n",
      "Iteration 37, loss = 0.12499171\n",
      "Iteration 38, loss = 0.12359675\n",
      "Iteration 39, loss = 0.12250486\n",
      "Iteration 40, loss = 0.12179508\n",
      "Iteration 41, loss = 0.12043269\n",
      "Iteration 42, loss = 0.11921619\n",
      "Iteration 43, loss = 0.11852789\n",
      "Iteration 44, loss = 0.11779073\n",
      "Iteration 45, loss = 0.11674734\n",
      "Iteration 46, loss = 0.11592795\n",
      "Iteration 47, loss = 0.11611037\n",
      "Iteration 48, loss = 0.11408107\n",
      "Iteration 49, loss = 0.11275849\n",
      "Iteration 50, loss = 0.11145583\n",
      "Iteration 51, loss = 0.11113218\n",
      "Iteration 52, loss = 0.11090081\n",
      "Iteration 53, loss = 0.10957696\n",
      "Iteration 54, loss = 0.11014393\n",
      "Iteration 55, loss = 0.10808760\n",
      "Iteration 56, loss = 0.10691434\n",
      "Iteration 57, loss = 0.10667406\n",
      "Iteration 58, loss = 0.10422633\n",
      "Iteration 59, loss = 0.10353243\n",
      "Iteration 60, loss = 0.10314765\n",
      "Iteration 61, loss = 0.10161481\n",
      "Iteration 62, loss = 0.10069463\n",
      "Iteration 63, loss = 0.09963561\n",
      "Iteration 64, loss = 0.09891941\n",
      "Iteration 65, loss = 0.09756066\n",
      "Iteration 66, loss = 0.09678043\n",
      "Iteration 67, loss = 0.09667570\n",
      "Iteration 68, loss = 0.09484085\n",
      "Iteration 69, loss = 0.09545915\n",
      "Iteration 70, loss = 0.09384133\n",
      "Iteration 71, loss = 0.09532872\n",
      "Iteration 72, loss = 0.09233942\n",
      "Iteration 73, loss = 0.09086093\n",
      "Iteration 74, loss = 0.08991131\n",
      "Iteration 75, loss = 0.08876778\n",
      "Iteration 76, loss = 0.08800028\n",
      "Iteration 77, loss = 0.08766993\n",
      "Iteration 78, loss = 0.08694840\n",
      "Iteration 79, loss = 0.08535688\n",
      "Iteration 80, loss = 0.08541374\n",
      "Iteration 81, loss = 0.08399558\n",
      "Iteration 82, loss = 0.08337574\n",
      "Iteration 83, loss = 0.08258899\n",
      "Iteration 84, loss = 0.08121019\n",
      "Iteration 85, loss = 0.08065021\n",
      "Iteration 86, loss = 0.07996870\n",
      "Iteration 87, loss = 0.07896397\n",
      "Iteration 88, loss = 0.07771501\n",
      "Iteration 89, loss = 0.07689793\n",
      "Iteration 90, loss = 0.07691535\n",
      "Iteration 91, loss = 0.07568345\n",
      "Iteration 92, loss = 0.07459198\n",
      "Iteration 93, loss = 0.07456368\n",
      "Iteration 94, loss = 0.07299762\n",
      "Iteration 95, loss = 0.07266916\n",
      "Iteration 96, loss = 0.07160734\n",
      "Iteration 97, loss = 0.07103198\n",
      "Iteration 98, loss = 0.06997058\n",
      "Iteration 99, loss = 0.06974958\n",
      "Iteration 100, loss = 0.06958513\n",
      "Iteration 101, loss = 0.06945489\n",
      "Iteration 102, loss = 0.06838090\n",
      "Iteration 103, loss = 0.06656138\n",
      "Iteration 104, loss = 0.06601128\n",
      "Iteration 105, loss = 0.06573815\n",
      "Iteration 106, loss = 0.06606272\n",
      "Iteration 107, loss = 0.06326868\n",
      "Iteration 108, loss = 0.06309297\n",
      "Iteration 109, loss = 0.06261962\n",
      "Iteration 110, loss = 0.06134443\n",
      "Iteration 111, loss = 0.06133719\n",
      "Iteration 112, loss = 0.06047752\n",
      "Iteration 113, loss = 0.05953554\n",
      "Iteration 114, loss = 0.06021477\n",
      "Iteration 115, loss = 0.05888625\n",
      "Iteration 116, loss = 0.05732539\n",
      "Iteration 117, loss = 0.05677430\n",
      "Iteration 118, loss = 0.05598829\n",
      "Iteration 119, loss = 0.05545386\n",
      "Iteration 120, loss = 0.05543074\n",
      "Iteration 121, loss = 0.05409157\n",
      "Iteration 122, loss = 0.05344685\n",
      "Iteration 123, loss = 0.05328228\n",
      "Iteration 124, loss = 0.05333511\n",
      "Iteration 125, loss = 0.05199542\n",
      "Iteration 126, loss = 0.05147285\n",
      "Iteration 127, loss = 0.05015623\n",
      "Iteration 128, loss = 0.04981442\n",
      "Iteration 129, loss = 0.04946859\n",
      "Iteration 130, loss = 0.04889055\n",
      "Iteration 131, loss = 0.04812333\n",
      "Iteration 132, loss = 0.04709297\n",
      "Iteration 133, loss = 0.04693836\n",
      "Iteration 134, loss = 0.04651772\n",
      "Iteration 135, loss = 0.04623395\n",
      "Iteration 136, loss = 0.04635717\n",
      "Iteration 137, loss = 0.04550821\n",
      "Iteration 138, loss = 0.04421077\n",
      "Iteration 139, loss = 0.04415468\n",
      "Iteration 140, loss = 0.04315770\n",
      "Iteration 141, loss = 0.04262807\n",
      "Iteration 142, loss = 0.04201217\n",
      "Iteration 143, loss = 0.04142776\n",
      "Iteration 144, loss = 0.04107093\n",
      "Iteration 145, loss = 0.04043590\n",
      "Iteration 146, loss = 0.04045744\n",
      "Iteration 147, loss = 0.03941260\n",
      "Iteration 148, loss = 0.03938599\n",
      "Iteration 149, loss = 0.03914901\n",
      "Iteration 150, loss = 0.03809553\n",
      "Iteration 151, loss = 0.03745230\n",
      "Iteration 152, loss = 0.03691164\n",
      "Iteration 153, loss = 0.03695869\n",
      "Iteration 154, loss = 0.03637973\n",
      "Iteration 155, loss = 0.03568377\n",
      "Iteration 156, loss = 0.03549621\n",
      "Iteration 157, loss = 0.03474171\n",
      "Iteration 158, loss = 0.03547023\n",
      "Iteration 159, loss = 0.03440210\n",
      "Iteration 160, loss = 0.03381978\n",
      "Iteration 161, loss = 0.03298711\n",
      "Iteration 162, loss = 0.03239870\n",
      "Iteration 163, loss = 0.03203433\n",
      "Iteration 164, loss = 0.03156821\n",
      "Iteration 165, loss = 0.03154177\n",
      "Iteration 166, loss = 0.03101476\n",
      "Iteration 167, loss = 0.03044251\n",
      "Iteration 168, loss = 0.03041127\n",
      "Iteration 169, loss = 0.02987228\n",
      "Iteration 170, loss = 0.03033851\n",
      "Iteration 171, loss = 0.02918679\n",
      "Iteration 172, loss = 0.02865515\n",
      "Iteration 173, loss = 0.02857147\n",
      "Iteration 174, loss = 0.02896601\n",
      "Iteration 175, loss = 0.02737118\n",
      "Iteration 176, loss = 0.02768902\n",
      "Iteration 177, loss = 0.02671727\n",
      "Iteration 178, loss = 0.02656006\n",
      "Iteration 179, loss = 0.02719175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 180, loss = 0.02753616\n",
      "Iteration 181, loss = 0.02653608\n",
      "Iteration 182, loss = 0.02470175\n",
      "Iteration 183, loss = 0.02420012\n",
      "Iteration 184, loss = 0.02434938\n",
      "Iteration 185, loss = 0.02394363\n",
      "Iteration 186, loss = 0.02349735\n",
      "Iteration 187, loss = 0.02318145\n",
      "Iteration 188, loss = 0.02288253\n",
      "Iteration 189, loss = 0.02321045\n",
      "Iteration 190, loss = 0.02269338\n",
      "Iteration 191, loss = 0.02275055\n",
      "Iteration 192, loss = 0.02217335\n",
      "Iteration 193, loss = 0.02131117\n",
      "Iteration 194, loss = 0.02137600\n",
      "Iteration 195, loss = 0.02123860\n",
      "Iteration 196, loss = 0.02061065\n",
      "Iteration 197, loss = 0.02045263\n",
      "Iteration 198, loss = 0.02031549\n",
      "Iteration 199, loss = 0.02013024\n",
      "Iteration 200, loss = 0.01953952\n",
      "Iteration 201, loss = 0.01941791\n",
      "Iteration 202, loss = 0.01906588\n",
      "Iteration 203, loss = 0.01851155\n",
      "Iteration 204, loss = 0.01859150\n",
      "Iteration 205, loss = 0.01825484\n",
      "Iteration 206, loss = 0.01843618\n",
      "Iteration 207, loss = 0.01818402\n",
      "Iteration 208, loss = 0.01760432\n",
      "Iteration 209, loss = 0.01727961\n",
      "Iteration 210, loss = 0.01730320\n",
      "Iteration 211, loss = 0.01777565\n",
      "Iteration 212, loss = 0.01680843\n",
      "Iteration 213, loss = 0.01637122\n",
      "Iteration 214, loss = 0.01626649\n",
      "Iteration 215, loss = 0.01612129\n",
      "Iteration 216, loss = 0.01564278\n",
      "Iteration 217, loss = 0.01543590\n",
      "Iteration 218, loss = 0.01533934\n",
      "Iteration 219, loss = 0.01527095\n",
      "Iteration 220, loss = 0.01548519\n",
      "Iteration 221, loss = 0.01489125\n",
      "Iteration 222, loss = 0.01469123\n",
      "Iteration 223, loss = 0.01449151\n",
      "Iteration 224, loss = 0.01478182\n",
      "Iteration 225, loss = 0.01393543\n",
      "Iteration 226, loss = 0.01371948\n",
      "Iteration 227, loss = 0.01351573\n",
      "Iteration 228, loss = 0.01415535\n",
      "Iteration 229, loss = 0.01335751\n",
      "Iteration 230, loss = 0.01386949\n",
      "Iteration 231, loss = 0.01370416\n",
      "Iteration 232, loss = 0.01330447\n",
      "Iteration 233, loss = 0.01292688\n",
      "Iteration 234, loss = 0.01293520\n",
      "Iteration 235, loss = 0.01259536\n",
      "Iteration 236, loss = 0.01200744\n",
      "Iteration 237, loss = 0.01207571\n",
      "Iteration 238, loss = 0.01217274\n",
      "Iteration 239, loss = 0.01196064\n",
      "Iteration 240, loss = 0.01181761\n",
      "Iteration 241, loss = 0.01199160\n",
      "Iteration 242, loss = 0.01168234\n",
      "Iteration 243, loss = 0.01153752\n",
      "Iteration 244, loss = 0.01132729\n",
      "Iteration 245, loss = 0.01098460\n",
      "Iteration 246, loss = 0.01073237\n",
      "Iteration 247, loss = 0.01081999\n",
      "Iteration 248, loss = 0.01043123\n",
      "Iteration 249, loss = 0.01047063\n",
      "Iteration 250, loss = 0.01012343\n",
      "Iteration 251, loss = 0.00996870\n",
      "Iteration 252, loss = 0.00988055\n",
      "Iteration 253, loss = 0.00978941\n",
      "Iteration 254, loss = 0.00964281\n",
      "Iteration 255, loss = 0.00944372\n",
      "Iteration 256, loss = 0.00935683\n",
      "Iteration 257, loss = 0.00930563\n",
      "Iteration 258, loss = 0.00915710\n",
      "Iteration 259, loss = 0.00902783\n",
      "Iteration 260, loss = 0.00892382\n",
      "Iteration 261, loss = 0.00878348\n",
      "Iteration 262, loss = 0.00879335\n",
      "Iteration 263, loss = 0.00877931\n",
      "Iteration 264, loss = 0.00894845\n",
      "Iteration 265, loss = 0.00867826\n",
      "Iteration 266, loss = 0.00843585\n",
      "Iteration 267, loss = 0.00828404\n",
      "Iteration 268, loss = 0.00825587\n",
      "Iteration 269, loss = 0.00831090\n",
      "Iteration 270, loss = 0.00799279\n",
      "Iteration 271, loss = 0.00791724\n",
      "Iteration 272, loss = 0.00773489\n",
      "Iteration 273, loss = 0.00761247\n",
      "Iteration 274, loss = 0.00764083\n",
      "Iteration 275, loss = 0.00747745\n",
      "Iteration 276, loss = 0.00744681\n",
      "Iteration 277, loss = 0.00725643\n",
      "Iteration 278, loss = 0.00726911\n",
      "Iteration 279, loss = 0.00711685\n",
      "Iteration 280, loss = 0.00705849\n",
      "Iteration 281, loss = 0.00697161\n",
      "Iteration 282, loss = 0.00691222\n",
      "Iteration 283, loss = 0.00701199\n",
      "Iteration 284, loss = 0.00676838\n",
      "Iteration 285, loss = 0.00678099\n",
      "Iteration 286, loss = 0.00664548\n",
      "Iteration 287, loss = 0.00659035\n",
      "Iteration 288, loss = 0.00673035\n",
      "Iteration 289, loss = 0.00662882\n",
      "Iteration 290, loss = 0.00647382\n",
      "Iteration 291, loss = 0.00628252\n",
      "Iteration 292, loss = 0.00626888\n",
      "Iteration 293, loss = 0.00609576\n",
      "Iteration 294, loss = 0.00612168\n",
      "Iteration 295, loss = 0.00602988\n",
      "Iteration 296, loss = 0.00600686\n",
      "Iteration 297, loss = 0.00590709\n",
      "Iteration 298, loss = 0.00588960\n",
      "Iteration 299, loss = 0.00589021\n",
      "Iteration 300, loss = 0.00574804\n",
      "Iteration 301, loss = 0.00560712\n",
      "Iteration 302, loss = 0.00570209\n",
      "Iteration 303, loss = 0.00555920\n",
      "Iteration 304, loss = 0.00543443\n",
      "Iteration 305, loss = 0.00545104\n",
      "Iteration 306, loss = 0.00543598\n",
      "Iteration 307, loss = 0.00531274\n",
      "Iteration 308, loss = 0.00523623\n",
      "Iteration 309, loss = 0.00517604\n",
      "Iteration 310, loss = 0.00513460\n",
      "Iteration 311, loss = 0.00508955\n",
      "Iteration 312, loss = 0.00513125\n",
      "Iteration 313, loss = 0.00503445\n",
      "Iteration 314, loss = 0.00499876\n",
      "Iteration 315, loss = 0.00493929\n",
      "Iteration 316, loss = 0.00484897\n",
      "Iteration 317, loss = 0.00479204\n",
      "Iteration 318, loss = 0.00474183\n",
      "Iteration 319, loss = 0.00471101\n",
      "Iteration 320, loss = 0.00471582\n",
      "Iteration 321, loss = 0.00464375\n",
      "Iteration 322, loss = 0.00472851\n",
      "Iteration 323, loss = 0.00466879\n",
      "Iteration 324, loss = 0.00461375\n",
      "Iteration 325, loss = 0.00445629\n",
      "Iteration 326, loss = 0.00437115\n",
      "Iteration 327, loss = 0.00436452\n",
      "Iteration 328, loss = 0.00435805\n",
      "Iteration 329, loss = 0.00434944\n",
      "Iteration 330, loss = 0.00448239\n",
      "Iteration 331, loss = 0.00424141\n",
      "Iteration 332, loss = 0.00412623\n",
      "Iteration 333, loss = 0.00413193\n",
      "Iteration 334, loss = 0.00408469\n",
      "Iteration 335, loss = 0.00406888\n",
      "Iteration 336, loss = 0.00403370\n",
      "Iteration 337, loss = 0.00397957\n",
      "Iteration 338, loss = 0.00392594\n",
      "Iteration 339, loss = 0.00389383\n",
      "Iteration 340, loss = 0.00387761\n",
      "Iteration 341, loss = 0.00384651\n",
      "Iteration 342, loss = 0.00385623\n",
      "Iteration 343, loss = 0.00374383\n",
      "Iteration 344, loss = 0.00377688\n",
      "Iteration 345, loss = 0.00370033\n",
      "Iteration 346, loss = 0.00368932\n",
      "Iteration 347, loss = 0.00365301\n",
      "Iteration 348, loss = 0.00364549\n",
      "Iteration 349, loss = 0.00359355\n",
      "Iteration 350, loss = 0.00353282\n",
      "Iteration 351, loss = 0.00350494\n",
      "Iteration 352, loss = 0.00349763\n",
      "Iteration 353, loss = 0.00350178\n",
      "Iteration 354, loss = 0.00344855\n",
      "Iteration 355, loss = 0.00341563\n",
      "Iteration 356, loss = 0.00341276\n",
      "Iteration 357, loss = 0.00331466\n",
      "Iteration 358, loss = 0.00330879\n",
      "Iteration 359, loss = 0.00331724\n",
      "Iteration 360, loss = 0.00324608\n",
      "Iteration 361, loss = 0.00322233\n",
      "Iteration 362, loss = 0.00323157\n",
      "Iteration 363, loss = 0.00315539\n",
      "Iteration 364, loss = 0.00321425\n",
      "Iteration 365, loss = 0.00322353\n",
      "Iteration 366, loss = 0.00308018\n",
      "Iteration 367, loss = 0.00310661\n",
      "Iteration 368, loss = 0.00305326\n",
      "Iteration 369, loss = 0.00303550\n",
      "Iteration 370, loss = 0.00300414\n",
      "Iteration 371, loss = 0.00297951\n",
      "Iteration 372, loss = 0.00293777\n",
      "Iteration 373, loss = 0.00291954\n",
      "Iteration 374, loss = 0.00291341\n",
      "Iteration 375, loss = 0.00288757\n",
      "Iteration 376, loss = 0.00288008\n",
      "Iteration 377, loss = 0.00284593\n",
      "Iteration 378, loss = 0.00284762\n",
      "Iteration 379, loss = 0.00280168\n",
      "Iteration 380, loss = 0.00277641\n",
      "Iteration 381, loss = 0.00280361\n",
      "Iteration 382, loss = 0.00275048\n",
      "Iteration 383, loss = 0.00272036\n",
      "Iteration 384, loss = 0.00270789\n",
      "Iteration 385, loss = 0.00268938\n",
      "Iteration 386, loss = 0.00264679\n",
      "Iteration 387, loss = 0.00264858\n",
      "Iteration 388, loss = 0.00260452\n",
      "Iteration 389, loss = 0.00261008\n",
      "Iteration 390, loss = 0.00264131\n",
      "Iteration 391, loss = 0.00264051\n",
      "Iteration 392, loss = 0.00264828\n",
      "Iteration 393, loss = 0.00252038\n",
      "Iteration 394, loss = 0.00252238\n",
      "Iteration 395, loss = 0.00250765\n",
      "Iteration 396, loss = 0.00247748\n",
      "Iteration 397, loss = 0.00247846\n",
      "Iteration 398, loss = 0.00243099\n",
      "Iteration 399, loss = 0.00243844\n",
      "Iteration 400, loss = 0.00241404\n",
      "Iteration 401, loss = 0.00238200\n",
      "Iteration 402, loss = 0.00239828\n",
      "Iteration 403, loss = 0.00236089\n",
      "Iteration 404, loss = 0.00237045\n",
      "Iteration 405, loss = 0.00235900\n",
      "Iteration 406, loss = 0.00233025\n",
      "Iteration 407, loss = 0.00232924\n",
      "Iteration 408, loss = 0.00227833\n",
      "Iteration 409, loss = 0.00227284\n",
      "Iteration 410, loss = 0.00225202\n",
      "Iteration 411, loss = 0.00223675\n",
      "Iteration 412, loss = 0.00221406\n",
      "Iteration 413, loss = 0.00222062\n",
      "Iteration 414, loss = 0.00219239\n",
      "Iteration 415, loss = 0.00217001\n",
      "Iteration 416, loss = 0.00218407\n",
      "Iteration 417, loss = 0.00214044\n",
      "Iteration 418, loss = 0.00212678\n",
      "Iteration 419, loss = 0.00211644\n",
      "Iteration 420, loss = 0.00210886\n",
      "Iteration 421, loss = 0.00209746\n",
      "Iteration 422, loss = 0.00207445\n",
      "Iteration 423, loss = 0.00206064\n",
      "Iteration 424, loss = 0.00205462\n",
      "Iteration 425, loss = 0.00204211\n",
      "Iteration 426, loss = 0.00203442\n",
      "Iteration 427, loss = 0.00200890\n",
      "Iteration 428, loss = 0.00200703\n",
      "Iteration 429, loss = 0.00200320\n",
      "Iteration 430, loss = 0.00197916\n",
      "Iteration 431, loss = 0.00196345\n",
      "Iteration 432, loss = 0.00195345\n",
      "Iteration 433, loss = 0.00194624\n",
      "Iteration 434, loss = 0.00193166\n",
      "Iteration 435, loss = 0.00191217\n",
      "Iteration 436, loss = 0.00191048\n",
      "Iteration 437, loss = 0.00189955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 438, loss = 0.00188296\n",
      "Iteration 439, loss = 0.00187744\n",
      "Iteration 440, loss = 0.00190452\n",
      "Iteration 441, loss = 0.00185598\n",
      "Iteration 442, loss = 0.00184621\n",
      "Iteration 443, loss = 0.00183747\n",
      "Iteration 444, loss = 0.00182054\n",
      "Iteration 445, loss = 0.00182023\n",
      "Iteration 446, loss = 0.00181564\n",
      "Iteration 447, loss = 0.00179350\n",
      "Iteration 448, loss = 0.00178327\n",
      "Iteration 449, loss = 0.00177328\n",
      "Iteration 450, loss = 0.00175661\n",
      "Iteration 451, loss = 0.00174809\n",
      "Iteration 452, loss = 0.00173825\n",
      "Iteration 453, loss = 0.00172838\n",
      "Iteration 454, loss = 0.00172211\n",
      "Iteration 455, loss = 0.00171698\n",
      "Iteration 456, loss = 0.00170560\n",
      "Iteration 457, loss = 0.00169903\n",
      "Iteration 458, loss = 0.00169645\n",
      "Iteration 459, loss = 0.00169763\n",
      "Iteration 460, loss = 0.00168757\n",
      "Iteration 461, loss = 0.00166124\n",
      "Iteration 462, loss = 0.00166544\n",
      "Iteration 463, loss = 0.00164708\n",
      "Iteration 464, loss = 0.00163334\n",
      "Iteration 465, loss = 0.00162777\n",
      "Iteration 466, loss = 0.00162330\n",
      "Iteration 467, loss = 0.00161225\n",
      "Iteration 468, loss = 0.00160063\n",
      "Iteration 469, loss = 0.00160172\n",
      "Iteration 470, loss = 0.00158697\n",
      "Iteration 471, loss = 0.00158085\n",
      "Iteration 472, loss = 0.00159077\n",
      "Iteration 473, loss = 0.00157497\n",
      "Iteration 474, loss = 0.00155796\n",
      "Iteration 475, loss = 0.00155747\n",
      "Iteration 476, loss = 0.00154659\n",
      "Iteration 477, loss = 0.00152996\n",
      "Iteration 478, loss = 0.00152587\n",
      "Iteration 479, loss = 0.00152223\n",
      "Iteration 480, loss = 0.00153825\n",
      "Iteration 481, loss = 0.00150952\n",
      "Iteration 482, loss = 0.00150320\n",
      "Iteration 483, loss = 0.00148869\n",
      "Iteration 484, loss = 0.00148678\n",
      "Iteration 485, loss = 0.00147800\n",
      "Iteration 486, loss = 0.00146954\n",
      "Iteration 487, loss = 0.00146364\n",
      "Iteration 488, loss = 0.00145706\n",
      "Iteration 489, loss = 0.00145727\n",
      "Iteration 490, loss = 0.00144213\n",
      "Iteration 491, loss = 0.00144366\n",
      "Iteration 492, loss = 0.00143456\n",
      "Iteration 493, loss = 0.00142595\n",
      "Iteration 494, loss = 0.00142148\n",
      "Iteration 495, loss = 0.00141924\n",
      "Iteration 496, loss = 0.00140823\n",
      "Iteration 497, loss = 0.00139834\n",
      "Iteration 498, loss = 0.00139207\n",
      "Iteration 499, loss = 0.00139618\n",
      "Iteration 500, loss = 0.00139706\n",
      "Iteration 501, loss = 0.00138421\n",
      "Iteration 502, loss = 0.00137466\n",
      "Iteration 503, loss = 0.00136837\n",
      "Iteration 504, loss = 0.00137073\n",
      "Iteration 505, loss = 0.00135499\n",
      "Iteration 506, loss = 0.00135273\n",
      "Iteration 507, loss = 0.00133937\n",
      "Iteration 508, loss = 0.00133370\n",
      "Iteration 509, loss = 0.00133351\n",
      "Iteration 510, loss = 0.00132387\n",
      "Iteration 511, loss = 0.00132510\n",
      "Iteration 512, loss = 0.00131975\n",
      "Iteration 513, loss = 0.00131175\n",
      "Iteration 514, loss = 0.00130149\n",
      "Iteration 515, loss = 0.00129814\n",
      "Iteration 516, loss = 0.00129342\n",
      "Iteration 517, loss = 0.00128825\n",
      "Iteration 518, loss = 0.00128937\n",
      "Iteration 519, loss = 0.00129036\n",
      "Iteration 520, loss = 0.00127021\n",
      "Iteration 521, loss = 0.00126942\n",
      "Iteration 522, loss = 0.00126480\n",
      "Iteration 523, loss = 0.00125911\n",
      "Iteration 524, loss = 0.00125714\n",
      "Iteration 525, loss = 0.00125024\n",
      "Iteration 526, loss = 0.00124233\n",
      "Iteration 527, loss = 0.00124312\n",
      "Iteration 528, loss = 0.00123383\n",
      "Iteration 529, loss = 0.00123283\n",
      "Iteration 530, loss = 0.00122800\n",
      "Iteration 531, loss = 0.00123750\n",
      "Iteration 532, loss = 0.00124180\n",
      "Iteration 533, loss = 0.00121470\n",
      "Iteration 534, loss = 0.00121900\n",
      "Iteration 535, loss = 0.00120491\n",
      "Iteration 536, loss = 0.00119887\n",
      "Iteration 537, loss = 0.00119534\n",
      "Iteration 538, loss = 0.00119305\n",
      "Iteration 539, loss = 0.00119291\n",
      "Iteration 540, loss = 0.00118162\n",
      "Iteration 541, loss = 0.00118259\n",
      "Iteration 542, loss = 0.00118328\n",
      "Iteration 543, loss = 0.00117792\n",
      "Iteration 544, loss = 0.00117134\n",
      "Iteration 545, loss = 0.00116889\n",
      "Iteration 546, loss = 0.00116852\n",
      "Iteration 547, loss = 0.00115822\n",
      "Iteration 548, loss = 0.00115498\n",
      "Iteration 549, loss = 0.00114848\n",
      "Iteration 550, loss = 0.00114635\n",
      "Iteration 551, loss = 0.00114304\n",
      "Iteration 552, loss = 0.00113835\n",
      "Iteration 553, loss = 0.00113593\n",
      "Iteration 554, loss = 0.00113494\n",
      "Iteration 555, loss = 0.00113550\n",
      "Iteration 556, loss = 0.00113486\n",
      "Iteration 557, loss = 0.00112303\n",
      "Iteration 558, loss = 0.00111788\n",
      "Iteration 559, loss = 0.00112444\n",
      "Iteration 560, loss = 0.00110748\n",
      "Iteration 561, loss = 0.00110768\n",
      "Iteration 562, loss = 0.00110264\n",
      "Iteration 563, loss = 0.00110077\n",
      "Iteration 564, loss = 0.00109960\n",
      "Iteration 565, loss = 0.00109338\n",
      "Iteration 566, loss = 0.00108927\n",
      "Iteration 567, loss = 0.00108472\n",
      "Iteration 568, loss = 0.00109020\n",
      "Iteration 569, loss = 0.00109486\n",
      "Iteration 570, loss = 0.00107861\n",
      "Iteration 571, loss = 0.00107376\n",
      "Iteration 572, loss = 0.00107108\n",
      "Iteration 573, loss = 0.00106469\n",
      "Iteration 574, loss = 0.00106792\n",
      "Iteration 575, loss = 0.00105996\n",
      "Iteration 576, loss = 0.00105764\n",
      "Iteration 577, loss = 0.00105372\n",
      "Iteration 578, loss = 0.00105344\n",
      "Iteration 579, loss = 0.00105004\n",
      "Iteration 580, loss = 0.00104605\n",
      "Iteration 581, loss = 0.00104222\n",
      "Iteration 582, loss = 0.00104254\n",
      "Iteration 583, loss = 0.00104156\n",
      "Iteration 584, loss = 0.00103646\n",
      "Iteration 585, loss = 0.00103557\n",
      "Iteration 586, loss = 0.00102971\n",
      "Iteration 587, loss = 0.00102561\n",
      "Iteration 588, loss = 0.00103116\n",
      "Iteration 589, loss = 0.00102111\n",
      "Iteration 590, loss = 0.00101676\n",
      "Iteration 591, loss = 0.00101913\n",
      "Iteration 592, loss = 0.00101297\n",
      "Iteration 593, loss = 0.00101206\n",
      "Iteration 594, loss = 0.00100883\n",
      "Iteration 595, loss = 0.00101090\n",
      "Iteration 596, loss = 0.00100414\n",
      "Iteration 597, loss = 0.00100018\n",
      "Iteration 598, loss = 0.00100066\n",
      "Iteration 599, loss = 0.00099876\n",
      "Iteration 600, loss = 0.00099233\n",
      "Iteration 601, loss = 0.00099023\n",
      "Iteration 602, loss = 0.00099423\n",
      "Iteration 603, loss = 0.00098999\n",
      "Iteration 604, loss = 0.00098334\n",
      "Iteration 605, loss = 0.00098120\n",
      "Iteration 606, loss = 0.00097954\n",
      "Iteration 607, loss = 0.00097622\n",
      "Iteration 608, loss = 0.00097483\n",
      "Iteration 609, loss = 0.00097263\n",
      "Iteration 610, loss = 0.00096962\n",
      "Iteration 611, loss = 0.00096818\n",
      "Iteration 612, loss = 0.00096552\n",
      "Iteration 613, loss = 0.00096426\n",
      "Iteration 614, loss = 0.00096331\n",
      "Iteration 615, loss = 0.00096571\n",
      "Iteration 616, loss = 0.00095676\n",
      "Iteration 617, loss = 0.00095667\n",
      "Iteration 618, loss = 0.00095325\n",
      "Iteration 619, loss = 0.00095176\n",
      "Iteration 620, loss = 0.00094859\n",
      "Iteration 621, loss = 0.00094705\n",
      "Iteration 622, loss = 0.00094648\n",
      "Iteration 623, loss = 0.00094240\n",
      "Iteration 624, loss = 0.00093866\n",
      "Iteration 625, loss = 0.00093745\n",
      "Iteration 626, loss = 0.00093894\n",
      "Iteration 627, loss = 0.00093481\n",
      "Iteration 628, loss = 0.00093293\n",
      "Iteration 629, loss = 0.00093247\n",
      "Iteration 630, loss = 0.00092912\n",
      "Iteration 631, loss = 0.00092593\n",
      "Iteration 632, loss = 0.00092290\n",
      "Iteration 633, loss = 0.00092649\n",
      "Iteration 634, loss = 0.00092243\n",
      "Iteration 635, loss = 0.00092230\n",
      "Iteration 636, loss = 0.00091646\n",
      "Iteration 637, loss = 0.00091498\n",
      "Iteration 638, loss = 0.00091474\n",
      "Iteration 639, loss = 0.00091326\n",
      "Iteration 640, loss = 0.00091182\n",
      "Iteration 641, loss = 0.00090833\n",
      "Iteration 642, loss = 0.00090701\n",
      "Iteration 643, loss = 0.00090398\n",
      "Iteration 644, loss = 0.00090107\n",
      "Iteration 645, loss = 0.00090030\n",
      "Iteration 646, loss = 0.00089897\n",
      "Iteration 647, loss = 0.00089696\n",
      "Iteration 648, loss = 0.00089839\n",
      "Iteration 649, loss = 0.00089538\n",
      "Iteration 650, loss = 0.00089358\n",
      "Iteration 651, loss = 0.00088894\n",
      "Iteration 652, loss = 0.00089092\n",
      "Iteration 653, loss = 0.00088580\n",
      "Iteration 654, loss = 0.00088511\n",
      "Iteration 655, loss = 0.00088407\n",
      "Iteration 656, loss = 0.00088246\n",
      "Iteration 657, loss = 0.00088032\n",
      "Iteration 658, loss = 0.00087877\n",
      "Iteration 659, loss = 0.00087672\n",
      "Iteration 660, loss = 0.00087628\n",
      "Iteration 661, loss = 0.00087359\n",
      "Iteration 662, loss = 0.00087222\n",
      "Iteration 663, loss = 0.00087117\n",
      "Iteration 664, loss = 0.00087029\n",
      "Iteration 665, loss = 0.00086750\n",
      "Iteration 666, loss = 0.00086613\n",
      "Iteration 667, loss = 0.00086458\n",
      "Iteration 668, loss = 0.00086356\n",
      "Iteration 669, loss = 0.00086374\n",
      "Iteration 670, loss = 0.00086022\n",
      "Iteration 671, loss = 0.00085939\n",
      "Iteration 672, loss = 0.00085691\n",
      "Iteration 673, loss = 0.00085815\n",
      "Iteration 674, loss = 0.00085725\n",
      "Iteration 675, loss = 0.00085419\n",
      "Iteration 676, loss = 0.00085347\n",
      "Iteration 677, loss = 0.00085199\n",
      "Iteration 678, loss = 0.00084992\n",
      "Iteration 679, loss = 0.00084917\n",
      "Iteration 680, loss = 0.00084839\n",
      "Iteration 681, loss = 0.00084523\n",
      "Iteration 682, loss = 0.00084616\n",
      "Iteration 683, loss = 0.00084449\n",
      "Iteration 684, loss = 0.00084038\n",
      "Iteration 685, loss = 0.00083925\n",
      "Iteration 686, loss = 0.00084059\n",
      "Iteration 687, loss = 0.00083883\n",
      "Iteration 688, loss = 0.00083855\n",
      "Iteration 689, loss = 0.00083482\n",
      "Iteration 690, loss = 0.00083281\n",
      "Iteration 691, loss = 0.00083189\n",
      "Iteration 692, loss = 0.00083293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 693, loss = 0.00083054\n",
      "Iteration 694, loss = 0.00082946\n",
      "Iteration 695, loss = 0.00082824\n",
      "Iteration 696, loss = 0.00082622\n",
      "Iteration 697, loss = 0.00082481\n",
      "Iteration 698, loss = 0.00082399\n",
      "Iteration 699, loss = 0.00082125\n",
      "Iteration 700, loss = 0.00082039\n",
      "Iteration 701, loss = 0.00082059\n",
      "Iteration 702, loss = 0.00081792\n",
      "Iteration 703, loss = 0.00081814\n",
      "Iteration 704, loss = 0.00081650\n",
      "Iteration 705, loss = 0.00081619\n",
      "Iteration 706, loss = 0.00081362\n",
      "Iteration 707, loss = 0.00081275\n",
      "Iteration 708, loss = 0.00081234\n",
      "Iteration 709, loss = 0.00081094\n",
      "Iteration 710, loss = 0.00081223\n",
      "Iteration 711, loss = 0.00081460\n",
      "Iteration 712, loss = 0.00081649\n",
      "Iteration 713, loss = 0.00080676\n",
      "Iteration 714, loss = 0.00080509\n",
      "Iteration 715, loss = 0.00080340\n",
      "Iteration 716, loss = 0.00080143\n",
      "Iteration 717, loss = 0.00080144\n",
      "Iteration 718, loss = 0.00080094\n",
      "Iteration 719, loss = 0.00079863\n",
      "Iteration 720, loss = 0.00079869\n",
      "Iteration 721, loss = 0.00079533\n",
      "Iteration 722, loss = 0.00079579\n",
      "Iteration 723, loss = 0.00079610\n",
      "Iteration 724, loss = 0.00079286\n",
      "Iteration 725, loss = 0.00079162\n",
      "Iteration 726, loss = 0.00079054\n",
      "Iteration 727, loss = 0.00079086\n",
      "Iteration 728, loss = 0.00078890\n",
      "Iteration 729, loss = 0.00078719\n",
      "Iteration 730, loss = 0.00078656\n",
      "Iteration 731, loss = 0.00078620\n",
      "Iteration 732, loss = 0.00078597\n",
      "Iteration 733, loss = 0.00078324\n",
      "Iteration 734, loss = 0.00078297\n",
      "Iteration 735, loss = 0.00078234\n",
      "Iteration 736, loss = 0.00078233\n",
      "Iteration 737, loss = 0.00078048\n",
      "Iteration 738, loss = 0.00078131\n",
      "Iteration 739, loss = 0.00077733\n",
      "Iteration 740, loss = 0.00077778\n",
      "Iteration 741, loss = 0.00077608\n",
      "Iteration 742, loss = 0.00077617\n",
      "Iteration 743, loss = 0.00077279\n",
      "Iteration 744, loss = 0.00077642\n",
      "Iteration 745, loss = 0.00077152\n",
      "Iteration 746, loss = 0.00077168\n",
      "Iteration 747, loss = 0.00077031\n",
      "Iteration 748, loss = 0.00076835\n",
      "Iteration 749, loss = 0.00076774\n",
      "Iteration 750, loss = 0.00076681\n",
      "Iteration 751, loss = 0.00076644\n",
      "Iteration 752, loss = 0.00076606\n",
      "Iteration 753, loss = 0.00076423\n",
      "Iteration 754, loss = 0.00076288\n",
      "Iteration 755, loss = 0.00076186\n",
      "Iteration 756, loss = 0.00076137\n",
      "Iteration 757, loss = 0.00076079\n",
      "Iteration 758, loss = 0.00076162\n",
      "Iteration 759, loss = 0.00075904\n",
      "Iteration 760, loss = 0.00075714\n",
      "Iteration 761, loss = 0.00075672\n",
      "Iteration 762, loss = 0.00075591\n",
      "Iteration 763, loss = 0.00075486\n",
      "Iteration 764, loss = 0.00075329\n",
      "Iteration 765, loss = 0.00075261\n",
      "Iteration 766, loss = 0.00075280\n",
      "Iteration 767, loss = 0.00075124\n",
      "Iteration 768, loss = 0.00074999\n",
      "Iteration 769, loss = 0.00074958\n",
      "Iteration 770, loss = 0.00074922\n",
      "Iteration 771, loss = 0.00074780\n",
      "Iteration 772, loss = 0.00074759\n",
      "Iteration 773, loss = 0.00074663\n",
      "Iteration 774, loss = 0.00074520\n",
      "Iteration 775, loss = 0.00074380\n",
      "Iteration 776, loss = 0.00074313\n",
      "Iteration 777, loss = 0.00074212\n",
      "Iteration 778, loss = 0.00074185\n",
      "Iteration 779, loss = 0.00074045\n",
      "Iteration 780, loss = 0.00073954\n",
      "Iteration 781, loss = 0.00074116\n",
      "Iteration 782, loss = 0.00073889\n",
      "Iteration 783, loss = 0.00073741\n",
      "Iteration 784, loss = 0.00073667\n",
      "Iteration 785, loss = 0.00073589\n",
      "Iteration 786, loss = 0.00073415\n",
      "Iteration 787, loss = 0.00073318\n",
      "Iteration 788, loss = 0.00073299\n",
      "Iteration 789, loss = 0.00073243\n",
      "Iteration 790, loss = 0.00073125\n",
      "Iteration 791, loss = 0.00073047\n",
      "Iteration 792, loss = 0.00073097\n",
      "Iteration 793, loss = 0.00073000\n",
      "Iteration 794, loss = 0.00072947\n",
      "Iteration 795, loss = 0.00072846\n",
      "Iteration 796, loss = 0.00072563\n",
      "Iteration 797, loss = 0.00072551\n",
      "Iteration 798, loss = 0.00072744\n",
      "Iteration 799, loss = 0.00072374\n",
      "Iteration 800, loss = 0.00072341\n",
      "Iteration 801, loss = 0.00072176\n",
      "Iteration 802, loss = 0.00072215\n",
      "Iteration 803, loss = 0.00072065\n",
      "Iteration 804, loss = 0.00072157\n",
      "Iteration 805, loss = 0.00071907\n",
      "Iteration 806, loss = 0.00071941\n",
      "Iteration 807, loss = 0.00071740\n",
      "Iteration 808, loss = 0.00071716\n",
      "Iteration 809, loss = 0.00071542\n",
      "Iteration 810, loss = 0.00071521\n",
      "Iteration 811, loss = 0.00071486\n",
      "Iteration 812, loss = 0.00071403\n",
      "Iteration 813, loss = 0.00071295\n",
      "Iteration 814, loss = 0.00071179\n",
      "Iteration 815, loss = 0.00071093\n",
      "Iteration 816, loss = 0.00071092\n",
      "Iteration 817, loss = 0.00070992\n",
      "Iteration 818, loss = 0.00070899\n",
      "Iteration 819, loss = 0.00070835\n",
      "Iteration 820, loss = 0.00070874\n",
      "Iteration 821, loss = 0.00070831\n",
      "Iteration 822, loss = 0.00070662\n",
      "Iteration 823, loss = 0.00070509\n",
      "Iteration 824, loss = 0.00070480\n",
      "Iteration 825, loss = 0.00070616\n",
      "Iteration 826, loss = 0.00070276\n",
      "Iteration 827, loss = 0.00070430\n",
      "Iteration 828, loss = 0.00070304\n",
      "Iteration 829, loss = 0.00070122\n",
      "Iteration 830, loss = 0.00070304\n",
      "Iteration 831, loss = 0.00069996\n",
      "Iteration 832, loss = 0.00069950\n",
      "Iteration 833, loss = 0.00069660\n",
      "Iteration 834, loss = 0.00069859\n",
      "Iteration 835, loss = 0.00069690\n",
      "Iteration 836, loss = 0.00069588\n",
      "Iteration 837, loss = 0.00069549\n",
      "Iteration 838, loss = 0.00069374\n",
      "Iteration 839, loss = 0.00069337\n",
      "Iteration 840, loss = 0.00069162\n",
      "Iteration 841, loss = 0.00069140\n",
      "Iteration 842, loss = 0.00069043\n",
      "Iteration 843, loss = 0.00068953\n",
      "Iteration 844, loss = 0.00068976\n",
      "Iteration 845, loss = 0.00068933\n",
      "Iteration 846, loss = 0.00068784\n",
      "Iteration 847, loss = 0.00068748\n",
      "Iteration 848, loss = 0.00068573\n",
      "Iteration 849, loss = 0.00068535\n",
      "Iteration 850, loss = 0.00068417\n",
      "Iteration 851, loss = 0.00068525\n",
      "Iteration 852, loss = 0.00068368\n",
      "Iteration 853, loss = 0.00068271\n",
      "Iteration 854, loss = 0.00068212\n",
      "Iteration 855, loss = 0.00068165\n",
      "Iteration 856, loss = 0.00068177\n",
      "Iteration 857, loss = 0.00067903\n",
      "Iteration 858, loss = 0.00067846\n",
      "Iteration 859, loss = 0.00067793\n",
      "Iteration 860, loss = 0.00067729\n",
      "Iteration 861, loss = 0.00067712\n",
      "Iteration 862, loss = 0.00067684\n",
      "Iteration 863, loss = 0.00067446\n",
      "Iteration 864, loss = 0.00067562\n",
      "Iteration 865, loss = 0.00067397\n",
      "Iteration 866, loss = 0.00067299\n",
      "Iteration 867, loss = 0.00067202\n",
      "Iteration 868, loss = 0.00067071\n",
      "Iteration 869, loss = 0.00067042\n",
      "Iteration 870, loss = 0.00067146\n",
      "Iteration 871, loss = 0.00066990\n",
      "Iteration 872, loss = 0.00067085\n",
      "Iteration 873, loss = 0.00066802\n",
      "Iteration 874, loss = 0.00066673\n",
      "Iteration 875, loss = 0.00066880\n",
      "Iteration 876, loss = 0.00066844\n",
      "Iteration 877, loss = 0.00066645\n",
      "Iteration 878, loss = 0.00066603\n",
      "Iteration 879, loss = 0.00066401\n",
      "Iteration 880, loss = 0.00066292\n",
      "Iteration 881, loss = 0.00066215\n",
      "Iteration 882, loss = 0.00066176\n",
      "Iteration 883, loss = 0.00066133\n",
      "Iteration 884, loss = 0.00065993\n",
      "Iteration 885, loss = 0.00065985\n",
      "Iteration 886, loss = 0.00065840\n",
      "Iteration 887, loss = 0.00065998\n",
      "Iteration 888, loss = 0.00065982\n",
      "Iteration 889, loss = 0.00065878\n",
      "Iteration 890, loss = 0.00065795\n",
      "Iteration 891, loss = 0.00065832\n",
      "Iteration 892, loss = 0.00065637\n",
      "Iteration 893, loss = 0.00065378\n",
      "Iteration 894, loss = 0.00065228\n",
      "Iteration 895, loss = 0.00065183\n",
      "Iteration 896, loss = 0.00065253\n",
      "Iteration 897, loss = 0.00065112\n",
      "Iteration 898, loss = 0.00065078\n",
      "Iteration 899, loss = 0.00064968\n",
      "Iteration 900, loss = 0.00065056\n",
      "Iteration 901, loss = 0.00065015\n",
      "Iteration 902, loss = 0.00064907\n",
      "Iteration 903, loss = 0.00064648\n",
      "Iteration 904, loss = 0.00064547\n",
      "Iteration 905, loss = 0.00064512\n",
      "Iteration 906, loss = 0.00064370\n",
      "Iteration 907, loss = 0.00064317\n",
      "Iteration 908, loss = 0.00064264\n",
      "Iteration 909, loss = 0.00064198\n",
      "Iteration 910, loss = 0.00064278\n",
      "Iteration 911, loss = 0.00064178\n",
      "Iteration 912, loss = 0.00064026\n",
      "Iteration 913, loss = 0.00063919\n",
      "Iteration 914, loss = 0.00063876\n",
      "Iteration 915, loss = 0.00063851\n",
      "Iteration 916, loss = 0.00063792\n",
      "Iteration 917, loss = 0.00063978\n",
      "Iteration 918, loss = 0.00063761\n",
      "Iteration 919, loss = 0.00063619\n",
      "Iteration 920, loss = 0.00063660\n",
      "Iteration 921, loss = 0.00063416\n",
      "Iteration 922, loss = 0.00063268\n",
      "Iteration 923, loss = 0.00063203\n",
      "Iteration 924, loss = 0.00063204\n",
      "Iteration 925, loss = 0.00063229\n",
      "Iteration 926, loss = 0.00063239\n",
      "Iteration 927, loss = 0.00062942\n",
      "Iteration 928, loss = 0.00062908\n",
      "Iteration 929, loss = 0.00062911\n",
      "Iteration 930, loss = 0.00062780\n",
      "Iteration 931, loss = 0.00062687\n",
      "Iteration 932, loss = 0.00062624\n",
      "Iteration 933, loss = 0.00062762\n",
      "Iteration 934, loss = 0.00062545\n",
      "Iteration 935, loss = 0.00062414\n",
      "Iteration 936, loss = 0.00062495\n",
      "Iteration 937, loss = 0.00062399\n",
      "Iteration 938, loss = 0.00062453\n",
      "Iteration 939, loss = 0.00062335\n",
      "Iteration 940, loss = 0.00062396\n",
      "Iteration 941, loss = 0.00062442\n",
      "Iteration 942, loss = 0.00062267\n",
      "Iteration 943, loss = 0.00061948\n",
      "Iteration 944, loss = 0.00061790\n",
      "Iteration 945, loss = 0.00061708\n",
      "Iteration 946, loss = 0.00061658\n",
      "Iteration 947, loss = 0.00061587\n",
      "Iteration 948, loss = 0.00061696\n",
      "Iteration 949, loss = 0.00061715\n",
      "Iteration 950, loss = 0.00061512\n",
      "Iteration 951, loss = 0.00061394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 952, loss = 0.00061349\n",
      "Iteration 953, loss = 0.00061423\n",
      "Iteration 954, loss = 0.00061349\n",
      "Iteration 955, loss = 0.00061513\n",
      "Iteration 956, loss = 0.00061089\n",
      "Iteration 957, loss = 0.00060898\n",
      "Iteration 958, loss = 0.00060876\n",
      "Iteration 959, loss = 0.00060790\n",
      "Iteration 960, loss = 0.00060709\n",
      "Iteration 961, loss = 0.00060867\n",
      "Iteration 962, loss = 0.00060784\n",
      "Iteration 963, loss = 0.00060598\n",
      "Iteration 964, loss = 0.00060847\n",
      "Iteration 965, loss = 0.00060768\n",
      "Iteration 966, loss = 0.00060583\n",
      "Iteration 967, loss = 0.00060362\n",
      "Iteration 968, loss = 0.00060257\n",
      "Iteration 969, loss = 0.00060141\n",
      "Iteration 970, loss = 0.00060113\n",
      "Iteration 971, loss = 0.00060305\n",
      "Iteration 972, loss = 0.00059978\n",
      "Iteration 973, loss = 0.00059856\n",
      "Iteration 974, loss = 0.00059849\n",
      "Iteration 975, loss = 0.00059716\n",
      "Iteration 976, loss = 0.00059725\n",
      "Iteration 977, loss = 0.00059610\n",
      "Iteration 978, loss = 0.00059534\n",
      "Iteration 979, loss = 0.00059484\n",
      "Iteration 980, loss = 0.00059417\n",
      "Iteration 981, loss = 0.00059421\n",
      "Iteration 982, loss = 0.00059401\n",
      "Iteration 983, loss = 0.00059253\n",
      "Iteration 984, loss = 0.00059173\n",
      "Iteration 985, loss = 0.00059102\n",
      "Iteration 986, loss = 0.00058987\n",
      "Iteration 987, loss = 0.00058992\n",
      "Iteration 988, loss = 0.00058944\n",
      "Iteration 989, loss = 0.00059001\n",
      "Iteration 990, loss = 0.00058830\n",
      "Iteration 991, loss = 0.00059059\n",
      "Iteration 992, loss = 0.00058923\n",
      "Iteration 993, loss = 0.00058835\n",
      "Iteration 994, loss = 0.00058492\n",
      "Iteration 995, loss = 0.00058586\n",
      "Iteration 996, loss = 0.00058506\n",
      "Iteration 997, loss = 0.00058268\n",
      "Iteration 998, loss = 0.00058210\n",
      "Iteration 999, loss = 0.00058127\n",
      "Iteration 1000, loss = 0.00058046\n",
      "Iteration 1001, loss = 0.00058240\n",
      "Iteration 1002, loss = 0.00058068\n",
      "Iteration 1003, loss = 0.00058014\n",
      "Iteration 1004, loss = 0.00057922\n",
      "Iteration 1005, loss = 0.00057870\n",
      "Iteration 1006, loss = 0.00057931\n",
      "Iteration 1007, loss = 0.00057937\n",
      "Iteration 1008, loss = 0.00057866\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Accuracy: 0.93 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "# Save all output to file.\n",
    "# sys.stdout = open(\"parameter_logging.txt\",\"w\")\n",
    "\n",
    "# Initialize random number generator for reproducibility.\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Load in dataset.\n",
    "\n",
    "data = spio.loadmat(\"features_10s_2019-01-30.mat\");\n",
    "features = data['features'];\n",
    "labels = data['labels_features'];\n",
    "animal_id_features = data['animal_id_features'].transpose();\n",
    "animal_names = data['animal_names'].transpose();\n",
    "feat_names = data['feat_names'];\n",
    "col_names = pd.DataFrame(feat_names)\n",
    "# Label each feature column with its description.\n",
    "def find_between(s):\n",
    "    start = '\\'';\n",
    "    end = '\\'';\n",
    "    return((s.split(start))[1].split(end)[0])\n",
    "cols = [];\n",
    "c_names = col_names.values.ravel();\n",
    "\n",
    "for x in range(len(c_names)):\n",
    "    name = str (c_names[x]);\n",
    "    cols.append(find_between(name))\n",
    "\n",
    "# Create a DataFrame of features with columns named & rows labeled.\n",
    "feat_data = pd.DataFrame(data=features,columns=cols)\n",
    "feat_data.insert(0,'AnimalId',animal_id_features)\n",
    "feat_data.insert(0,'Labels',labels.transpose())\n",
    "\n",
    "# Randomly select an animal to leave out.\n",
    "index = random.randint(1,12);\n",
    "print(\"Animal chosen: %s\" % animal_names[index - 1][0])\n",
    "\n",
    "# Select the features corresponding to one animal.\n",
    "def get_single_animal_features(df, index) :\n",
    "    return df.loc[df['AnimalId'] == index]\n",
    "\n",
    "# Delete the rows corresponding to the animal left out.\n",
    "def get_loo_features(df, index):\n",
    "\tdf[df.AnimalId != index]\n",
    "\treturn df\n",
    "\n",
    "# Get features of 11/12 animals.\n",
    "single_animal_features = get_single_animal_features(feat_data, index);\n",
    "loo_features = get_loo_features(feat_data, index);\n",
    "\n",
    "# Get only labels corresponding to first animal's features.\n",
    "y = single_animal_features['Labels']\n",
    "X = single_animal_features.drop(columns={'Labels','AnimalId'})\n",
    "\n",
    "\"\"\"Split data into training and testing for cross-validation.\"\"\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2);\n",
    "\n",
    "\"\"\"Standardize the data since the MLP is sensitive to feature scaling.\"\"\"\n",
    "scaler = StandardScaler()\n",
    "# Fit only to the training data.\n",
    "scaler.fit(X_train)\n",
    "# Apply the transformations to the data.\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize the model with constant iteration max = 500.\n",
    "mlp = MLPClassifier(\n",
    "    max_iter=1500,\n",
    "    verbose=51,\n",
    "    tol=0.000001, \n",
    "    learning_rate='constant',\n",
    "    alpha=0.001,\n",
    "    solver='adam',\n",
    "    batch_size=512,\n",
    "    activation='tanh')\n",
    "\n",
    "# Run model with 4-fold cross validation. Report mean accuracy.\n",
    "scores = cross_val_score(mlp, X_train, y_train, cv=4)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.60970068\n",
      "Iteration 2, loss = 0.40075440\n",
      "Iteration 3, loss = 0.30629955\n",
      "Iteration 4, loss = 0.25696663\n",
      "Iteration 5, loss = 0.22981177\n",
      "Iteration 6, loss = 0.21103657\n",
      "Iteration 7, loss = 0.19792734\n",
      "Iteration 8, loss = 0.18802860\n",
      "Iteration 9, loss = 0.18084905\n",
      "Iteration 10, loss = 0.17395432\n",
      "Iteration 11, loss = 0.16933220\n",
      "Iteration 12, loss = 0.16433909\n",
      "Iteration 13, loss = 0.16051445\n",
      "Iteration 14, loss = 0.15758529\n",
      "Iteration 15, loss = 0.15413595\n",
      "Iteration 16, loss = 0.15167680\n",
      "Iteration 17, loss = 0.14951905\n",
      "Iteration 18, loss = 0.14677875\n",
      "Iteration 19, loss = 0.14511151\n",
      "Iteration 20, loss = 0.14287995\n",
      "Iteration 21, loss = 0.14133134\n",
      "Iteration 22, loss = 0.13886371\n",
      "Iteration 23, loss = 0.13733923\n",
      "Iteration 24, loss = 0.13564988\n",
      "Iteration 25, loss = 0.13416984\n",
      "Iteration 26, loss = 0.13431630\n",
      "Iteration 27, loss = 0.13364676\n",
      "Iteration 28, loss = 0.13152634\n",
      "Iteration 29, loss = 0.12977529\n",
      "Iteration 30, loss = 0.12887994\n",
      "Iteration 31, loss = 0.12696034\n",
      "Iteration 32, loss = 0.12588126\n",
      "Iteration 33, loss = 0.12517162\n",
      "Iteration 34, loss = 0.12314083\n",
      "Iteration 35, loss = 0.12196924\n",
      "Iteration 36, loss = 0.12095750\n",
      "Iteration 37, loss = 0.11989563\n",
      "Iteration 38, loss = 0.11837516\n",
      "Iteration 39, loss = 0.11711119\n",
      "Iteration 40, loss = 0.11570661\n",
      "Iteration 41, loss = 0.11573651\n",
      "Iteration 42, loss = 0.11342981\n",
      "Iteration 43, loss = 0.11291116\n",
      "Iteration 44, loss = 0.11133248\n",
      "Iteration 45, loss = 0.11017263\n",
      "Iteration 46, loss = 0.10893940\n",
      "Iteration 47, loss = 0.10877430\n",
      "Iteration 48, loss = 0.10720037\n",
      "Iteration 49, loss = 0.10600916\n",
      "Iteration 50, loss = 0.10600464\n",
      "Iteration 51, loss = 0.10417333\n",
      "Iteration 52, loss = 0.10270474\n",
      "Iteration 53, loss = 0.10177246\n",
      "Iteration 54, loss = 0.10070962\n",
      "Iteration 55, loss = 0.10085153\n",
      "Iteration 56, loss = 0.10155976\n",
      "Iteration 57, loss = 0.09837823\n",
      "Iteration 58, loss = 0.09639132\n",
      "Iteration 59, loss = 0.09635593\n",
      "Iteration 60, loss = 0.09785952\n",
      "Iteration 61, loss = 0.09433877\n",
      "Iteration 62, loss = 0.09436247\n",
      "Iteration 63, loss = 0.09363161\n",
      "Iteration 64, loss = 0.09438673\n",
      "Iteration 65, loss = 0.09121841\n",
      "Iteration 66, loss = 0.09121878\n",
      "Iteration 67, loss = 0.08873338\n",
      "Iteration 68, loss = 0.08813050\n",
      "Iteration 69, loss = 0.08725888\n",
      "Iteration 70, loss = 0.08650983\n",
      "Iteration 71, loss = 0.08510368\n",
      "Iteration 72, loss = 0.08607701\n",
      "Iteration 73, loss = 0.08330268\n",
      "Iteration 74, loss = 0.08266782\n",
      "Iteration 75, loss = 0.08163835\n",
      "Iteration 76, loss = 0.08072667\n",
      "Iteration 77, loss = 0.07984494\n",
      "Iteration 78, loss = 0.07940923\n",
      "Iteration 79, loss = 0.07917446\n",
      "Iteration 80, loss = 0.07800686\n",
      "Iteration 81, loss = 0.07756825\n",
      "Iteration 82, loss = 0.07654275\n",
      "Iteration 83, loss = 0.07473144\n",
      "Iteration 84, loss = 0.07422507\n",
      "Iteration 85, loss = 0.07380688\n",
      "Iteration 86, loss = 0.07331820\n",
      "Iteration 87, loss = 0.07244354\n",
      "Iteration 88, loss = 0.07062196\n",
      "Iteration 89, loss = 0.07060543\n",
      "Iteration 90, loss = 0.06994864\n",
      "Iteration 91, loss = 0.06904871\n",
      "Iteration 92, loss = 0.06843300\n",
      "Iteration 93, loss = 0.06697047\n",
      "Iteration 94, loss = 0.06685355\n",
      "Iteration 95, loss = 0.06577150\n",
      "Iteration 96, loss = 0.06502170\n",
      "Iteration 97, loss = 0.06355201\n",
      "Iteration 98, loss = 0.06320350\n",
      "Iteration 99, loss = 0.06256211\n",
      "Iteration 100, loss = 0.06220369\n",
      "Iteration 101, loss = 0.06216356\n",
      "Iteration 102, loss = 0.06133659\n",
      "Iteration 103, loss = 0.05979886\n",
      "Iteration 104, loss = 0.05914317\n",
      "Iteration 105, loss = 0.05902577\n",
      "Iteration 106, loss = 0.05791261\n",
      "Iteration 107, loss = 0.05791451\n",
      "Iteration 108, loss = 0.05752707\n",
      "Iteration 109, loss = 0.05549871\n",
      "Iteration 110, loss = 0.05489330\n",
      "Iteration 111, loss = 0.05497792\n",
      "Iteration 112, loss = 0.05404620\n",
      "Iteration 113, loss = 0.05284531\n",
      "Iteration 114, loss = 0.05329060\n",
      "Iteration 115, loss = 0.05318357\n",
      "Iteration 116, loss = 0.05128113\n",
      "Iteration 117, loss = 0.05163691\n",
      "Iteration 118, loss = 0.05096415\n",
      "Iteration 119, loss = 0.04960400\n",
      "Iteration 120, loss = 0.05040275\n",
      "Iteration 121, loss = 0.04821343\n",
      "Iteration 122, loss = 0.04672321\n",
      "Iteration 123, loss = 0.04655610\n",
      "Iteration 124, loss = 0.04572773\n",
      "Iteration 125, loss = 0.04599214\n",
      "Iteration 126, loss = 0.04504002\n",
      "Iteration 127, loss = 0.04556366\n",
      "Iteration 128, loss = 0.04459501\n",
      "Iteration 129, loss = 0.04506832\n",
      "Iteration 130, loss = 0.04262795\n",
      "Iteration 131, loss = 0.04239898\n",
      "Iteration 132, loss = 0.04141328\n",
      "Iteration 133, loss = 0.04051964\n",
      "Iteration 134, loss = 0.04000189\n",
      "Iteration 135, loss = 0.03961833\n",
      "Iteration 136, loss = 0.03936391\n",
      "Iteration 137, loss = 0.03889818\n",
      "Iteration 138, loss = 0.03883405\n",
      "Iteration 139, loss = 0.03859498\n",
      "Iteration 140, loss = 0.03848601\n",
      "Iteration 141, loss = 0.03845113\n",
      "Iteration 142, loss = 0.03600140\n",
      "Iteration 143, loss = 0.03567182\n",
      "Iteration 144, loss = 0.03523651\n",
      "Iteration 145, loss = 0.03485880\n",
      "Iteration 146, loss = 0.03446507\n",
      "Iteration 147, loss = 0.03415478\n",
      "Iteration 148, loss = 0.03327031\n",
      "Iteration 149, loss = 0.03309026\n",
      "Iteration 150, loss = 0.03267819\n",
      "Iteration 151, loss = 0.03303009\n",
      "Iteration 152, loss = 0.03170863\n",
      "Iteration 153, loss = 0.03078040\n",
      "Iteration 154, loss = 0.03047157\n",
      "Iteration 155, loss = 0.03024929\n",
      "Iteration 156, loss = 0.02964211\n",
      "Iteration 157, loss = 0.02994025\n",
      "Iteration 158, loss = 0.02930879\n",
      "Iteration 159, loss = 0.02869102\n",
      "Iteration 160, loss = 0.02795322\n",
      "Iteration 161, loss = 0.02747459\n",
      "Iteration 162, loss = 0.02735034\n",
      "Iteration 163, loss = 0.02742879\n",
      "Iteration 164, loss = 0.02676434\n",
      "Iteration 165, loss = 0.02701344\n",
      "Iteration 166, loss = 0.02690965\n",
      "Iteration 167, loss = 0.02551843\n",
      "Iteration 168, loss = 0.02568025\n",
      "Iteration 169, loss = 0.02441792\n",
      "Iteration 170, loss = 0.02418794\n",
      "Iteration 171, loss = 0.02392766\n",
      "Iteration 172, loss = 0.02330460\n",
      "Iteration 173, loss = 0.02361762\n",
      "Iteration 174, loss = 0.02406670\n",
      "Iteration 175, loss = 0.02337049\n",
      "Iteration 176, loss = 0.02245549\n",
      "Iteration 177, loss = 0.02268557\n",
      "Iteration 178, loss = 0.02247480\n",
      "Iteration 179, loss = 0.02151453\n",
      "Iteration 180, loss = 0.02111751\n",
      "Iteration 181, loss = 0.02087358\n",
      "Iteration 182, loss = 0.02027449\n",
      "Iteration 183, loss = 0.02002065\n",
      "Iteration 184, loss = 0.01974717\n",
      "Iteration 185, loss = 0.01958395\n",
      "Iteration 186, loss = 0.01962121\n",
      "Iteration 187, loss = 0.01926693\n",
      "Iteration 188, loss = 0.01869427\n",
      "Iteration 189, loss = 0.01830961\n",
      "Iteration 190, loss = 0.01814594\n",
      "Iteration 191, loss = 0.01798112\n",
      "Iteration 192, loss = 0.01851358\n",
      "Iteration 193, loss = 0.01747316\n",
      "Iteration 194, loss = 0.01714896\n",
      "Iteration 195, loss = 0.01759890\n",
      "Iteration 196, loss = 0.01809152\n",
      "Iteration 197, loss = 0.01702901\n",
      "Iteration 198, loss = 0.01613185\n",
      "Iteration 199, loss = 0.01587014\n",
      "Iteration 200, loss = 0.01588354\n",
      "Iteration 201, loss = 0.01538112\n",
      "Iteration 202, loss = 0.01521447\n",
      "Iteration 203, loss = 0.01497141\n",
      "Iteration 204, loss = 0.01468507\n",
      "Iteration 205, loss = 0.01473845\n",
      "Iteration 206, loss = 0.01489673\n",
      "Iteration 207, loss = 0.01425341\n",
      "Iteration 208, loss = 0.01405116\n",
      "Iteration 209, loss = 0.01372468\n",
      "Iteration 210, loss = 0.01370727\n",
      "Iteration 211, loss = 0.01339231\n",
      "Iteration 212, loss = 0.01325064\n",
      "Iteration 213, loss = 0.01339754\n",
      "Iteration 214, loss = 0.01306136\n",
      "Iteration 215, loss = 0.01260206\n",
      "Iteration 216, loss = 0.01266261\n",
      "Iteration 217, loss = 0.01244779\n",
      "Iteration 218, loss = 0.01213681\n",
      "Iteration 219, loss = 0.01191002\n",
      "Iteration 220, loss = 0.01178406\n",
      "Iteration 221, loss = 0.01169567\n",
      "Iteration 222, loss = 0.01138114\n",
      "Iteration 223, loss = 0.01125890\n",
      "Iteration 224, loss = 0.01125978\n",
      "Iteration 225, loss = 0.01107084\n",
      "Iteration 226, loss = 0.01070136\n",
      "Iteration 227, loss = 0.01061592\n",
      "Iteration 228, loss = 0.01042132\n",
      "Iteration 229, loss = 0.01033752\n",
      "Iteration 230, loss = 0.01016411\n",
      "Iteration 231, loss = 0.01012639\n",
      "Iteration 232, loss = 0.00998567\n",
      "Iteration 233, loss = 0.00965495\n",
      "Iteration 234, loss = 0.00973158\n",
      "Iteration 235, loss = 0.00953639\n",
      "Iteration 236, loss = 0.00935518\n",
      "Iteration 237, loss = 0.00913304\n",
      "Iteration 238, loss = 0.00908406\n",
      "Iteration 239, loss = 0.00905791\n",
      "Iteration 240, loss = 0.00915687\n",
      "Iteration 241, loss = 0.00912839\n",
      "Iteration 242, loss = 0.00868774\n",
      "Iteration 243, loss = 0.00854783\n",
      "Iteration 244, loss = 0.00836738\n",
      "Iteration 245, loss = 0.00848315\n",
      "Iteration 246, loss = 0.00816588\n",
      "Iteration 247, loss = 0.00815792\n",
      "Iteration 248, loss = 0.00825718\n",
      "Iteration 249, loss = 0.00797642\n",
      "Iteration 250, loss = 0.00811055\n",
      "Iteration 251, loss = 0.00780917\n",
      "Iteration 252, loss = 0.00766428\n",
      "Iteration 253, loss = 0.00736395\n",
      "Iteration 254, loss = 0.00729789\n",
      "Iteration 255, loss = 0.00733218\n",
      "Iteration 256, loss = 0.00721740\n",
      "Iteration 257, loss = 0.00717348\n",
      "Iteration 258, loss = 0.00701055\n",
      "Iteration 259, loss = 0.00683776\n",
      "Iteration 260, loss = 0.00675929\n",
      "Iteration 261, loss = 0.00668845\n",
      "Iteration 262, loss = 0.00660413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 263, loss = 0.00674378\n",
      "Iteration 264, loss = 0.00686363\n",
      "Iteration 265, loss = 0.00653948\n",
      "Iteration 266, loss = 0.00653266\n",
      "Iteration 267, loss = 0.00637579\n",
      "Iteration 268, loss = 0.00617483\n",
      "Iteration 269, loss = 0.00611711\n",
      "Iteration 270, loss = 0.00601512\n",
      "Iteration 271, loss = 0.00596811\n",
      "Iteration 272, loss = 0.00593024\n",
      "Iteration 273, loss = 0.00590901\n",
      "Iteration 274, loss = 0.00572302\n",
      "Iteration 275, loss = 0.00565450\n",
      "Iteration 276, loss = 0.00563294\n",
      "Iteration 277, loss = 0.00554109\n",
      "Iteration 278, loss = 0.00545215\n",
      "Iteration 279, loss = 0.00532741\n",
      "Iteration 280, loss = 0.00540097\n",
      "Iteration 281, loss = 0.00522546\n",
      "Iteration 282, loss = 0.00528594\n",
      "Iteration 283, loss = 0.00512713\n",
      "Iteration 284, loss = 0.00514537\n",
      "Iteration 285, loss = 0.00502976\n",
      "Iteration 286, loss = 0.00499660\n",
      "Iteration 287, loss = 0.00506949\n",
      "Iteration 288, loss = 0.00490177\n",
      "Iteration 289, loss = 0.00487970\n",
      "Iteration 290, loss = 0.00475553\n",
      "Iteration 291, loss = 0.00468719\n",
      "Iteration 292, loss = 0.00463128\n",
      "Iteration 293, loss = 0.00462403\n",
      "Iteration 294, loss = 0.00456085\n",
      "Iteration 295, loss = 0.00453775\n",
      "Iteration 296, loss = 0.00447732\n",
      "Iteration 297, loss = 0.00444144\n",
      "Iteration 298, loss = 0.00434077\n",
      "Iteration 299, loss = 0.00434339\n",
      "Iteration 300, loss = 0.00426608\n",
      "Iteration 301, loss = 0.00417960\n",
      "Iteration 302, loss = 0.00415479\n",
      "Iteration 303, loss = 0.00410603\n",
      "Iteration 304, loss = 0.00410139\n",
      "Iteration 305, loss = 0.00405932\n",
      "Iteration 306, loss = 0.00401461\n",
      "Iteration 307, loss = 0.00393451\n",
      "Iteration 308, loss = 0.00386535\n",
      "Iteration 309, loss = 0.00388481\n",
      "Iteration 310, loss = 0.00395244\n",
      "Iteration 311, loss = 0.00386876\n",
      "Iteration 312, loss = 0.00383259\n",
      "Iteration 313, loss = 0.00370702\n",
      "Iteration 314, loss = 0.00365317\n",
      "Iteration 315, loss = 0.00363724\n",
      "Iteration 316, loss = 0.00360542\n",
      "Iteration 317, loss = 0.00354490\n",
      "Iteration 318, loss = 0.00354567\n",
      "Iteration 319, loss = 0.00351626\n",
      "Iteration 320, loss = 0.00354817\n",
      "Iteration 321, loss = 0.00351771\n",
      "Iteration 322, loss = 0.00344196\n",
      "Iteration 323, loss = 0.00339387\n",
      "Iteration 324, loss = 0.00336102\n",
      "Iteration 325, loss = 0.00328172\n",
      "Iteration 326, loss = 0.00322359\n",
      "Iteration 327, loss = 0.00323181\n",
      "Iteration 328, loss = 0.00319704\n",
      "Iteration 329, loss = 0.00317114\n",
      "Iteration 330, loss = 0.00315668\n",
      "Iteration 331, loss = 0.00308949\n",
      "Iteration 332, loss = 0.00305167\n",
      "Iteration 333, loss = 0.00307766\n",
      "Iteration 334, loss = 0.00307987\n",
      "Iteration 335, loss = 0.00299263\n",
      "Iteration 336, loss = 0.00297302\n",
      "Iteration 337, loss = 0.00291360\n",
      "Iteration 338, loss = 0.00289340\n",
      "Iteration 339, loss = 0.00289954\n",
      "Iteration 340, loss = 0.00286050\n",
      "Iteration 341, loss = 0.00285902\n",
      "Iteration 342, loss = 0.00283007\n",
      "Iteration 343, loss = 0.00281073\n",
      "Iteration 344, loss = 0.00276845\n",
      "Iteration 345, loss = 0.00272257\n",
      "Iteration 346, loss = 0.00270063\n",
      "Iteration 347, loss = 0.00267140\n",
      "Iteration 348, loss = 0.00265257\n",
      "Iteration 349, loss = 0.00265007\n",
      "Iteration 350, loss = 0.00263321\n",
      "Iteration 351, loss = 0.00262015\n",
      "Iteration 352, loss = 0.00261950\n",
      "Iteration 353, loss = 0.00258294\n",
      "Iteration 354, loss = 0.00253787\n",
      "Iteration 355, loss = 0.00253594\n",
      "Iteration 356, loss = 0.00250381\n",
      "Iteration 357, loss = 0.00246396\n",
      "Iteration 358, loss = 0.00245048\n",
      "Iteration 359, loss = 0.00244483\n",
      "Iteration 360, loss = 0.00243208\n",
      "Iteration 361, loss = 0.00238350\n",
      "Iteration 362, loss = 0.00236632\n",
      "Iteration 363, loss = 0.00236531\n",
      "Iteration 364, loss = 0.00235581\n",
      "Iteration 365, loss = 0.00235719\n",
      "Iteration 366, loss = 0.00230018\n",
      "Iteration 367, loss = 0.00228239\n",
      "Iteration 368, loss = 0.00226625\n",
      "Iteration 369, loss = 0.00224806\n",
      "Iteration 370, loss = 0.00222972\n",
      "Iteration 371, loss = 0.00222764\n",
      "Iteration 372, loss = 0.00219699\n",
      "Iteration 373, loss = 0.00219152\n",
      "Iteration 374, loss = 0.00216123\n",
      "Iteration 375, loss = 0.00215880\n",
      "Iteration 376, loss = 0.00213541\n",
      "Iteration 377, loss = 0.00211127\n",
      "Iteration 378, loss = 0.00209030\n",
      "Iteration 379, loss = 0.00207853\n",
      "Iteration 380, loss = 0.00206740\n",
      "Iteration 381, loss = 0.00205289\n",
      "Iteration 382, loss = 0.00203483\n",
      "Iteration 383, loss = 0.00202237\n",
      "Iteration 384, loss = 0.00201447\n",
      "Iteration 385, loss = 0.00199965\n",
      "Iteration 386, loss = 0.00200525\n",
      "Iteration 387, loss = 0.00198005\n",
      "Iteration 388, loss = 0.00194302\n",
      "Iteration 389, loss = 0.00195189\n",
      "Iteration 390, loss = 0.00194311\n",
      "Iteration 391, loss = 0.00191897\n",
      "Iteration 392, loss = 0.00190165\n",
      "Iteration 393, loss = 0.00189197\n",
      "Iteration 394, loss = 0.00189380\n",
      "Iteration 395, loss = 0.00190905\n",
      "Iteration 396, loss = 0.00185667\n",
      "Iteration 397, loss = 0.00184975\n",
      "Iteration 398, loss = 0.00182682\n",
      "Iteration 399, loss = 0.00181104\n",
      "Iteration 400, loss = 0.00179758\n",
      "Iteration 401, loss = 0.00178830\n",
      "Iteration 402, loss = 0.00177445\n",
      "Iteration 403, loss = 0.00176572\n",
      "Iteration 404, loss = 0.00180608\n",
      "Iteration 405, loss = 0.00176256\n",
      "Iteration 406, loss = 0.00176976\n",
      "Iteration 407, loss = 0.00172949\n",
      "Iteration 408, loss = 0.00174154\n",
      "Iteration 409, loss = 0.00174371\n",
      "Iteration 410, loss = 0.00169492\n",
      "Iteration 411, loss = 0.00168823\n",
      "Iteration 412, loss = 0.00168679\n",
      "Iteration 413, loss = 0.00166463\n",
      "Iteration 414, loss = 0.00165126\n",
      "Iteration 415, loss = 0.00165423\n",
      "Iteration 416, loss = 0.00163452\n",
      "Iteration 417, loss = 0.00162552\n",
      "Iteration 418, loss = 0.00160903\n",
      "Iteration 419, loss = 0.00159738\n",
      "Iteration 420, loss = 0.00159868\n",
      "Iteration 421, loss = 0.00159299\n",
      "Iteration 422, loss = 0.00157820\n",
      "Iteration 423, loss = 0.00156814\n",
      "Iteration 424, loss = 0.00156678\n",
      "Iteration 425, loss = 0.00155065\n",
      "Iteration 426, loss = 0.00153877\n",
      "Iteration 427, loss = 0.00153607\n",
      "Iteration 428, loss = 0.00152804\n",
      "Iteration 429, loss = 0.00151787\n",
      "Iteration 430, loss = 0.00150938\n",
      "Iteration 431, loss = 0.00151246\n",
      "Iteration 432, loss = 0.00149447\n",
      "Iteration 433, loss = 0.00148755\n",
      "Iteration 434, loss = 0.00147366\n",
      "Iteration 435, loss = 0.00147300\n",
      "Iteration 436, loss = 0.00146091\n",
      "Iteration 437, loss = 0.00145087\n",
      "Iteration 438, loss = 0.00144485\n",
      "Iteration 439, loss = 0.00144655\n",
      "Iteration 440, loss = 0.00143214\n",
      "Iteration 441, loss = 0.00142522\n",
      "Iteration 442, loss = 0.00141848\n",
      "Iteration 443, loss = 0.00140984\n",
      "Iteration 444, loss = 0.00140437\n",
      "Iteration 445, loss = 0.00139530\n",
      "Iteration 446, loss = 0.00139153\n",
      "Iteration 447, loss = 0.00138692\n",
      "Iteration 448, loss = 0.00138155\n",
      "Iteration 449, loss = 0.00137387\n",
      "Iteration 450, loss = 0.00138305\n",
      "Iteration 451, loss = 0.00135841\n",
      "Iteration 452, loss = 0.00135383\n",
      "Iteration 453, loss = 0.00135691\n",
      "Iteration 454, loss = 0.00135476\n",
      "Iteration 455, loss = 0.00133459\n",
      "Iteration 456, loss = 0.00132644\n",
      "Iteration 457, loss = 0.00133386\n",
      "Iteration 458, loss = 0.00132436\n",
      "Iteration 459, loss = 0.00131198\n",
      "Iteration 460, loss = 0.00130228\n",
      "Iteration 461, loss = 0.00130670\n",
      "Iteration 462, loss = 0.00129701\n",
      "Iteration 463, loss = 0.00128436\n",
      "Iteration 464, loss = 0.00128429\n",
      "Iteration 465, loss = 0.00127895\n",
      "Iteration 466, loss = 0.00128932\n",
      "Iteration 467, loss = 0.00126997\n",
      "Iteration 468, loss = 0.00126261\n",
      "Iteration 469, loss = 0.00125842\n",
      "Iteration 470, loss = 0.00124634\n",
      "Iteration 471, loss = 0.00124557\n",
      "Iteration 472, loss = 0.00123502\n",
      "Iteration 473, loss = 0.00122981\n",
      "Iteration 474, loss = 0.00122547\n",
      "Iteration 475, loss = 0.00122736\n",
      "Iteration 476, loss = 0.00121666\n",
      "Iteration 477, loss = 0.00121318\n",
      "Iteration 478, loss = 0.00120510\n",
      "Iteration 479, loss = 0.00120125\n",
      "Iteration 480, loss = 0.00119902\n",
      "Iteration 481, loss = 0.00119604\n",
      "Iteration 482, loss = 0.00118794\n",
      "Iteration 483, loss = 0.00118200\n",
      "Iteration 484, loss = 0.00118158\n",
      "Iteration 485, loss = 0.00118320\n",
      "Iteration 486, loss = 0.00117885\n",
      "Iteration 487, loss = 0.00116675\n",
      "Iteration 488, loss = 0.00116154\n",
      "Iteration 489, loss = 0.00115525\n",
      "Iteration 490, loss = 0.00115778\n",
      "Iteration 491, loss = 0.00115100\n",
      "Iteration 492, loss = 0.00114281\n",
      "Iteration 493, loss = 0.00114252\n",
      "Iteration 494, loss = 0.00113643\n",
      "Iteration 495, loss = 0.00113490\n",
      "Iteration 496, loss = 0.00113209\n",
      "Iteration 497, loss = 0.00112812\n",
      "Iteration 498, loss = 0.00112192\n",
      "Iteration 499, loss = 0.00112866\n",
      "Iteration 500, loss = 0.00111814\n",
      "Iteration 501, loss = 0.00110994\n",
      "Iteration 502, loss = 0.00110902\n",
      "Iteration 503, loss = 0.00110274\n",
      "Iteration 504, loss = 0.00109913\n",
      "Iteration 505, loss = 0.00109584\n",
      "Iteration 506, loss = 0.00109321\n",
      "Iteration 507, loss = 0.00109153\n",
      "Iteration 508, loss = 0.00108984\n",
      "Iteration 509, loss = 0.00108834\n",
      "Iteration 510, loss = 0.00108292\n",
      "Iteration 511, loss = 0.00107842\n",
      "Iteration 512, loss = 0.00107784\n",
      "Iteration 513, loss = 0.00106997\n",
      "Iteration 514, loss = 0.00106719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 515, loss = 0.00106479\n",
      "Iteration 516, loss = 0.00105809\n",
      "Iteration 517, loss = 0.00105423\n",
      "Iteration 518, loss = 0.00105527\n",
      "Iteration 519, loss = 0.00105170\n",
      "Iteration 520, loss = 0.00105021\n",
      "Iteration 521, loss = 0.00104723\n",
      "Iteration 522, loss = 0.00103851\n",
      "Iteration 523, loss = 0.00103657\n",
      "Iteration 524, loss = 0.00103483\n",
      "Iteration 525, loss = 0.00103372\n",
      "Iteration 526, loss = 0.00102907\n",
      "Iteration 527, loss = 0.00103053\n",
      "Iteration 528, loss = 0.00102223\n",
      "Iteration 529, loss = 0.00101843\n",
      "Iteration 530, loss = 0.00101513\n",
      "Iteration 531, loss = 0.00102994\n",
      "Iteration 532, loss = 0.00101718\n",
      "Iteration 533, loss = 0.00100922\n",
      "Iteration 534, loss = 0.00101316\n",
      "Iteration 535, loss = 0.00100869\n",
      "Iteration 536, loss = 0.00100428\n",
      "Iteration 537, loss = 0.00100136\n",
      "Iteration 538, loss = 0.00099626\n",
      "Iteration 539, loss = 0.00099530\n",
      "Iteration 540, loss = 0.00099050\n",
      "Iteration 541, loss = 0.00099328\n",
      "Iteration 542, loss = 0.00098554\n",
      "Iteration 543, loss = 0.00098141\n",
      "Iteration 544, loss = 0.00097884\n",
      "Iteration 545, loss = 0.00097559\n",
      "Iteration 546, loss = 0.00097625\n",
      "Iteration 547, loss = 0.00097370\n",
      "Iteration 548, loss = 0.00097806\n",
      "Iteration 549, loss = 0.00096876\n",
      "Iteration 550, loss = 0.00096388\n",
      "Iteration 551, loss = 0.00096685\n",
      "Iteration 552, loss = 0.00096256\n",
      "Iteration 553, loss = 0.00095913\n",
      "Iteration 554, loss = 0.00095518\n",
      "Iteration 555, loss = 0.00095644\n",
      "Iteration 556, loss = 0.00095124\n",
      "Iteration 557, loss = 0.00095082\n",
      "Iteration 558, loss = 0.00094757\n",
      "Iteration 559, loss = 0.00094834\n",
      "Iteration 560, loss = 0.00094310\n",
      "Iteration 561, loss = 0.00094051\n",
      "Iteration 562, loss = 0.00094254\n",
      "Iteration 563, loss = 0.00093819\n",
      "Iteration 564, loss = 0.00093951\n",
      "Iteration 565, loss = 0.00093939\n",
      "Iteration 566, loss = 0.00093062\n",
      "Iteration 567, loss = 0.00092813\n",
      "Iteration 568, loss = 0.00092704\n",
      "Iteration 569, loss = 0.00093144\n",
      "Iteration 570, loss = 0.00092469\n",
      "Iteration 571, loss = 0.00091944\n",
      "Iteration 572, loss = 0.00091901\n",
      "Iteration 573, loss = 0.00091786\n",
      "Iteration 574, loss = 0.00091526\n",
      "Iteration 575, loss = 0.00091626\n",
      "Iteration 576, loss = 0.00091465\n",
      "Iteration 577, loss = 0.00090911\n",
      "Iteration 578, loss = 0.00090758\n",
      "Iteration 579, loss = 0.00090531\n",
      "Iteration 580, loss = 0.00090572\n",
      "Iteration 581, loss = 0.00090111\n",
      "Iteration 582, loss = 0.00090412\n",
      "Iteration 583, loss = 0.00090003\n",
      "Iteration 584, loss = 0.00089916\n",
      "Iteration 585, loss = 0.00089370\n",
      "Iteration 586, loss = 0.00089294\n",
      "Iteration 587, loss = 0.00089058\n",
      "Iteration 588, loss = 0.00089106\n",
      "Iteration 589, loss = 0.00089335\n",
      "Iteration 590, loss = 0.00088918\n",
      "Iteration 591, loss = 0.00088493\n",
      "Iteration 592, loss = 0.00088212\n",
      "Iteration 593, loss = 0.00088433\n",
      "Iteration 594, loss = 0.00088406\n",
      "Iteration 595, loss = 0.00088182\n",
      "Iteration 596, loss = 0.00087808\n",
      "Iteration 597, loss = 0.00087383\n",
      "Iteration 598, loss = 0.00087230\n",
      "Iteration 599, loss = 0.00087206\n",
      "Iteration 600, loss = 0.00087054\n",
      "Iteration 601, loss = 0.00087066\n",
      "Iteration 602, loss = 0.00086794\n",
      "Iteration 603, loss = 0.00086879\n",
      "Iteration 604, loss = 0.00086371\n",
      "Iteration 605, loss = 0.00086352\n",
      "Iteration 606, loss = 0.00086123\n",
      "Iteration 607, loss = 0.00085841\n",
      "Iteration 608, loss = 0.00085742\n",
      "Iteration 609, loss = 0.00085745\n",
      "Iteration 610, loss = 0.00085489\n",
      "Iteration 611, loss = 0.00085576\n",
      "Iteration 612, loss = 0.00085368\n",
      "Iteration 613, loss = 0.00085058\n",
      "Iteration 614, loss = 0.00085042\n",
      "Iteration 615, loss = 0.00085256\n",
      "Iteration 616, loss = 0.00084697\n",
      "Iteration 617, loss = 0.00084535\n",
      "Iteration 618, loss = 0.00084440\n",
      "Iteration 619, loss = 0.00084404\n",
      "Iteration 620, loss = 0.00084237\n",
      "Iteration 621, loss = 0.00083907\n",
      "Iteration 622, loss = 0.00084590\n",
      "Iteration 623, loss = 0.00083834\n",
      "Iteration 624, loss = 0.00083555\n",
      "Iteration 625, loss = 0.00083536\n",
      "Iteration 626, loss = 0.00083431\n",
      "Iteration 627, loss = 0.00083150\n",
      "Iteration 628, loss = 0.00083053\n",
      "Iteration 629, loss = 0.00083004\n",
      "Iteration 630, loss = 0.00083252\n",
      "Iteration 631, loss = 0.00082644\n",
      "Iteration 632, loss = 0.00082526\n",
      "Iteration 633, loss = 0.00082305\n",
      "Iteration 634, loss = 0.00082230\n",
      "Iteration 635, loss = 0.00082051\n",
      "Iteration 636, loss = 0.00081903\n",
      "Iteration 637, loss = 0.00081796\n",
      "Iteration 638, loss = 0.00081742\n",
      "Iteration 639, loss = 0.00081609\n",
      "Iteration 640, loss = 0.00081421\n",
      "Iteration 641, loss = 0.00081555\n",
      "Iteration 642, loss = 0.00081261\n",
      "Iteration 643, loss = 0.00081161\n",
      "Iteration 644, loss = 0.00080954\n",
      "Iteration 645, loss = 0.00081074\n",
      "Iteration 646, loss = 0.00080693\n",
      "Iteration 647, loss = 0.00080880\n",
      "Iteration 648, loss = 0.00080646\n",
      "Iteration 649, loss = 0.00080506\n",
      "Iteration 650, loss = 0.00080305\n",
      "Iteration 651, loss = 0.00080288\n",
      "Iteration 652, loss = 0.00080098\n",
      "Iteration 653, loss = 0.00079877\n",
      "Iteration 654, loss = 0.00079752\n",
      "Iteration 655, loss = 0.00079741\n",
      "Iteration 656, loss = 0.00079520\n",
      "Iteration 657, loss = 0.00079439\n",
      "Iteration 658, loss = 0.00079403\n",
      "Iteration 659, loss = 0.00079115\n",
      "Iteration 660, loss = 0.00079085\n",
      "Iteration 661, loss = 0.00078932\n",
      "Iteration 662, loss = 0.00078841\n",
      "Iteration 663, loss = 0.00078834\n",
      "Iteration 664, loss = 0.00078735\n",
      "Iteration 665, loss = 0.00078660\n",
      "Iteration 666, loss = 0.00078373\n",
      "Iteration 667, loss = 0.00078308\n",
      "Iteration 668, loss = 0.00078136\n",
      "Iteration 669, loss = 0.00078180\n",
      "Iteration 670, loss = 0.00077924\n",
      "Iteration 671, loss = 0.00078056\n",
      "Iteration 672, loss = 0.00078201\n",
      "Iteration 673, loss = 0.00077843\n",
      "Iteration 674, loss = 0.00077784\n",
      "Iteration 675, loss = 0.00077624\n",
      "Iteration 676, loss = 0.00077694\n",
      "Iteration 677, loss = 0.00077163\n",
      "Iteration 678, loss = 0.00077477\n",
      "Iteration 679, loss = 0.00077144\n",
      "Iteration 680, loss = 0.00076965\n",
      "Iteration 681, loss = 0.00076672\n",
      "Iteration 682, loss = 0.00076709\n",
      "Iteration 683, loss = 0.00076696\n",
      "Iteration 684, loss = 0.00076549\n",
      "Iteration 685, loss = 0.00076641\n",
      "Iteration 686, loss = 0.00076207\n",
      "Iteration 687, loss = 0.00076243\n",
      "Iteration 688, loss = 0.00076447\n",
      "Iteration 689, loss = 0.00076067\n",
      "Iteration 690, loss = 0.00075909\n",
      "Iteration 691, loss = 0.00075920\n",
      "Iteration 692, loss = 0.00075663\n",
      "Iteration 693, loss = 0.00075527\n",
      "Iteration 694, loss = 0.00075421\n",
      "Iteration 695, loss = 0.00075345\n",
      "Iteration 696, loss = 0.00075294\n",
      "Iteration 697, loss = 0.00075050\n",
      "Iteration 698, loss = 0.00075028\n",
      "Iteration 699, loss = 0.00075007\n",
      "Iteration 700, loss = 0.00074829\n",
      "Iteration 701, loss = 0.00074859\n",
      "Iteration 702, loss = 0.00074649\n",
      "Iteration 703, loss = 0.00074682\n",
      "Iteration 704, loss = 0.00074602\n",
      "Iteration 705, loss = 0.00074490\n",
      "Iteration 706, loss = 0.00074368\n",
      "Iteration 707, loss = 0.00074305\n",
      "Iteration 708, loss = 0.00074257\n",
      "Iteration 709, loss = 0.00074268\n",
      "Iteration 710, loss = 0.00073870\n",
      "Iteration 711, loss = 0.00073969\n",
      "Iteration 712, loss = 0.00074279\n",
      "Iteration 713, loss = 0.00073899\n",
      "Iteration 714, loss = 0.00073744\n",
      "Iteration 715, loss = 0.00073476\n",
      "Iteration 716, loss = 0.00073296\n",
      "Iteration 717, loss = 0.00073176\n",
      "Iteration 718, loss = 0.00073273\n",
      "Iteration 719, loss = 0.00073201\n",
      "Iteration 720, loss = 0.00073052\n",
      "Iteration 721, loss = 0.00072946\n",
      "Iteration 722, loss = 0.00072762\n",
      "Iteration 723, loss = 0.00072634\n",
      "Iteration 724, loss = 0.00072440\n",
      "Iteration 725, loss = 0.00072456\n",
      "Iteration 726, loss = 0.00072368\n",
      "Iteration 727, loss = 0.00072383\n",
      "Iteration 728, loss = 0.00072223\n",
      "Iteration 729, loss = 0.00072104\n",
      "Iteration 730, loss = 0.00071901\n",
      "Iteration 731, loss = 0.00071879\n",
      "Iteration 732, loss = 0.00071708\n",
      "Iteration 733, loss = 0.00071664\n",
      "Iteration 734, loss = 0.00071733\n",
      "Iteration 735, loss = 0.00071621\n",
      "Iteration 736, loss = 0.00071371\n",
      "Iteration 737, loss = 0.00071334\n",
      "Iteration 738, loss = 0.00071510\n",
      "Iteration 739, loss = 0.00071109\n",
      "Iteration 740, loss = 0.00071042\n",
      "Iteration 741, loss = 0.00070908\n",
      "Iteration 742, loss = 0.00070872\n",
      "Iteration 743, loss = 0.00070633\n",
      "Iteration 744, loss = 0.00070565\n",
      "Iteration 745, loss = 0.00070590\n",
      "Iteration 746, loss = 0.00070424\n",
      "Iteration 747, loss = 0.00070372\n",
      "Iteration 748, loss = 0.00070330\n",
      "Iteration 749, loss = 0.00070272\n",
      "Iteration 750, loss = 0.00070169\n",
      "Iteration 751, loss = 0.00069952\n",
      "Iteration 752, loss = 0.00069937\n",
      "Iteration 753, loss = 0.00069840\n",
      "Iteration 754, loss = 0.00069769\n",
      "Iteration 755, loss = 0.00069787\n",
      "Iteration 756, loss = 0.00069826\n",
      "Iteration 757, loss = 0.00070066\n",
      "Iteration 758, loss = 0.00070227\n",
      "Iteration 759, loss = 0.00069569\n",
      "Iteration 760, loss = 0.00069334\n",
      "Iteration 761, loss = 0.00069065\n",
      "Iteration 762, loss = 0.00069090\n",
      "Iteration 763, loss = 0.00069135\n",
      "Iteration 764, loss = 0.00068996\n",
      "Iteration 765, loss = 0.00068867\n",
      "Iteration 766, loss = 0.00068617\n",
      "Iteration 767, loss = 0.00068608\n",
      "Iteration 768, loss = 0.00068743\n",
      "Iteration 769, loss = 0.00068378\n",
      "Iteration 770, loss = 0.00068369\n",
      "Iteration 771, loss = 0.00068230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 772, loss = 0.00068110\n",
      "Iteration 773, loss = 0.00068176\n",
      "Iteration 774, loss = 0.00068043\n",
      "Iteration 775, loss = 0.00067784\n",
      "Iteration 776, loss = 0.00067811\n",
      "Iteration 777, loss = 0.00067651\n",
      "Iteration 778, loss = 0.00067458\n",
      "Iteration 779, loss = 0.00067461\n",
      "Iteration 780, loss = 0.00067517\n",
      "Iteration 781, loss = 0.00067326\n",
      "Iteration 782, loss = 0.00067143\n",
      "Iteration 783, loss = 0.00067046\n",
      "Iteration 784, loss = 0.00066997\n",
      "Iteration 785, loss = 0.00067045\n",
      "Iteration 786, loss = 0.00066833\n",
      "Iteration 787, loss = 0.00066697\n",
      "Iteration 788, loss = 0.00066676\n",
      "Iteration 789, loss = 0.00066600\n",
      "Iteration 790, loss = 0.00066526\n",
      "Iteration 791, loss = 0.00066760\n",
      "Iteration 792, loss = 0.00066580\n",
      "Iteration 793, loss = 0.00066189\n",
      "Iteration 794, loss = 0.00066155\n",
      "Iteration 795, loss = 0.00066111\n",
      "Iteration 796, loss = 0.00065942\n",
      "Iteration 797, loss = 0.00065938\n",
      "Iteration 798, loss = 0.00065800\n",
      "Iteration 799, loss = 0.00065709\n",
      "Iteration 800, loss = 0.00065577\n",
      "Iteration 801, loss = 0.00065687\n",
      "Iteration 802, loss = 0.00065578\n",
      "Iteration 803, loss = 0.00065420\n",
      "Iteration 804, loss = 0.00065236\n",
      "Iteration 805, loss = 0.00065307\n",
      "Iteration 806, loss = 0.00065106\n",
      "Iteration 807, loss = 0.00065134\n",
      "Iteration 808, loss = 0.00065400\n",
      "Iteration 809, loss = 0.00065587\n",
      "Iteration 810, loss = 0.00064683\n",
      "Iteration 811, loss = 0.00064655\n",
      "Iteration 812, loss = 0.00064644\n",
      "Iteration 813, loss = 0.00064550\n",
      "Iteration 814, loss = 0.00064476\n",
      "Iteration 815, loss = 0.00064416\n",
      "Iteration 816, loss = 0.00064286\n",
      "Iteration 817, loss = 0.00064151\n",
      "Iteration 818, loss = 0.00064233\n",
      "Iteration 819, loss = 0.00064010\n",
      "Iteration 820, loss = 0.00063865\n",
      "Iteration 821, loss = 0.00063754\n",
      "Iteration 822, loss = 0.00063711\n",
      "Iteration 823, loss = 0.00063750\n",
      "Iteration 824, loss = 0.00063626\n",
      "Iteration 825, loss = 0.00063686\n",
      "Iteration 826, loss = 0.00063600\n",
      "Iteration 827, loss = 0.00063636\n",
      "Iteration 828, loss = 0.00063381\n",
      "Iteration 829, loss = 0.00063164\n",
      "Iteration 830, loss = 0.00063178\n",
      "Iteration 831, loss = 0.00062958\n",
      "Iteration 832, loss = 0.00063050\n",
      "Iteration 833, loss = 0.00062826\n",
      "Iteration 834, loss = 0.00062766\n",
      "Iteration 835, loss = 0.00063174\n",
      "Iteration 836, loss = 0.00063515\n",
      "Iteration 837, loss = 0.00063367\n",
      "Iteration 838, loss = 0.00063119\n",
      "Iteration 839, loss = 0.00062959\n",
      "Iteration 840, loss = 0.00062722\n",
      "Iteration 841, loss = 0.00062427\n",
      "Iteration 842, loss = 0.00062221\n",
      "Iteration 843, loss = 0.00062352\n",
      "Iteration 844, loss = 0.00062539\n",
      "Iteration 845, loss = 0.00061946\n",
      "Iteration 846, loss = 0.00061745\n",
      "Iteration 847, loss = 0.00061634\n",
      "Iteration 848, loss = 0.00061723\n",
      "Iteration 849, loss = 0.00061695\n",
      "Iteration 850, loss = 0.00061495\n",
      "Iteration 851, loss = 0.00061487\n",
      "Iteration 852, loss = 0.00061693\n",
      "Iteration 853, loss = 0.00061363\n",
      "Iteration 854, loss = 0.00061261\n",
      "Iteration 855, loss = 0.00061083\n",
      "Iteration 856, loss = 0.00061074\n",
      "Iteration 857, loss = 0.00061024\n",
      "Iteration 858, loss = 0.00060943\n",
      "Iteration 859, loss = 0.00060786\n",
      "Iteration 860, loss = 0.00060674\n",
      "Iteration 861, loss = 0.00060638\n",
      "Iteration 862, loss = 0.00060518\n",
      "Iteration 863, loss = 0.00060732\n",
      "Iteration 864, loss = 0.00060578\n",
      "Iteration 865, loss = 0.00060384\n",
      "Iteration 866, loss = 0.00060174\n",
      "Iteration 867, loss = 0.00060124\n",
      "Iteration 868, loss = 0.00060263\n",
      "Iteration 869, loss = 0.00060050\n",
      "Iteration 870, loss = 0.00060037\n",
      "Iteration 871, loss = 0.00059887\n",
      "Iteration 872, loss = 0.00059623\n",
      "Iteration 873, loss = 0.00059753\n",
      "Iteration 874, loss = 0.00059560\n",
      "Iteration 875, loss = 0.00059725\n",
      "Iteration 876, loss = 0.00059665\n",
      "Iteration 877, loss = 0.00059417\n",
      "Iteration 878, loss = 0.00059439\n",
      "Iteration 879, loss = 0.00059326\n",
      "Iteration 880, loss = 0.00059103\n",
      "Iteration 881, loss = 0.00059159\n",
      "Iteration 882, loss = 0.00059442\n",
      "Iteration 883, loss = 0.00059167\n",
      "Iteration 884, loss = 0.00058994\n",
      "Iteration 885, loss = 0.00059081\n",
      "Iteration 886, loss = 0.00059565\n",
      "Iteration 887, loss = 0.00058882\n",
      "Iteration 888, loss = 0.00058590\n",
      "Iteration 889, loss = 0.00059093\n",
      "Iteration 890, loss = 0.00059418\n",
      "Iteration 891, loss = 0.00059051\n",
      "Iteration 892, loss = 0.00058684\n",
      "Iteration 893, loss = 0.00058336\n",
      "Iteration 894, loss = 0.00058173\n",
      "Iteration 895, loss = 0.00058056\n",
      "Iteration 896, loss = 0.00058038\n",
      "Iteration 897, loss = 0.00058027\n",
      "Iteration 898, loss = 0.00058128\n",
      "Iteration 899, loss = 0.00057921\n",
      "Iteration 900, loss = 0.00057797\n",
      "Iteration 901, loss = 0.00057713\n",
      "Iteration 902, loss = 0.00057692\n",
      "Iteration 903, loss = 0.00057616\n",
      "Iteration 904, loss = 0.00057781\n",
      "Iteration 905, loss = 0.00057674\n",
      "Iteration 906, loss = 0.00057427\n",
      "Iteration 907, loss = 0.00057273\n",
      "Iteration 908, loss = 0.00057201\n",
      "Iteration 909, loss = 0.00057387\n",
      "Iteration 910, loss = 0.00057123\n",
      "Iteration 911, loss = 0.00057165\n",
      "Iteration 912, loss = 0.00057211\n",
      "Iteration 913, loss = 0.00057208\n",
      "Iteration 914, loss = 0.00056751\n",
      "Iteration 915, loss = 0.00057122\n",
      "Iteration 916, loss = 0.00056938\n",
      "Iteration 917, loss = 0.00056620\n",
      "Iteration 918, loss = 0.00056582\n",
      "Iteration 919, loss = 0.00056422\n",
      "Iteration 920, loss = 0.00056767\n",
      "Iteration 921, loss = 0.00056366\n",
      "Iteration 922, loss = 0.00056306\n",
      "Iteration 923, loss = 0.00056235\n",
      "Iteration 924, loss = 0.00056193\n",
      "Iteration 925, loss = 0.00056387\n",
      "Iteration 926, loss = 0.00056588\n",
      "Iteration 927, loss = 0.00056272\n",
      "Iteration 928, loss = 0.00055930\n",
      "Iteration 929, loss = 0.00055835\n",
      "Iteration 930, loss = 0.00055779\n",
      "Iteration 931, loss = 0.00055872\n",
      "Iteration 932, loss = 0.00055856\n",
      "Iteration 933, loss = 0.00055762\n",
      "Iteration 934, loss = 0.00056083\n",
      "Iteration 935, loss = 0.00056342\n",
      "Iteration 936, loss = 0.00055955\n",
      "Iteration 937, loss = 0.00055641\n",
      "Iteration 938, loss = 0.00055489\n",
      "Iteration 939, loss = 0.00055425\n",
      "Iteration 940, loss = 0.00055641\n",
      "Iteration 941, loss = 0.00055184\n",
      "Iteration 942, loss = 0.00055356\n",
      "Iteration 943, loss = 0.00055088\n",
      "Iteration 944, loss = 0.00054992\n",
      "Iteration 945, loss = 0.00055046\n",
      "Iteration 946, loss = 0.00055071\n",
      "Iteration 947, loss = 0.00055008\n",
      "Iteration 948, loss = 0.00055003\n",
      "Iteration 949, loss = 0.00054825\n",
      "Iteration 950, loss = 0.00055023\n",
      "Iteration 951, loss = 0.00055103\n",
      "Iteration 952, loss = 0.00054716\n",
      "Iteration 953, loss = 0.00054961\n",
      "Iteration 954, loss = 0.00054596\n",
      "Iteration 955, loss = 0.00054740\n",
      "Iteration 956, loss = 0.00055117\n",
      "Iteration 957, loss = 0.00054560\n",
      "Iteration 958, loss = 0.00054538\n",
      "Iteration 959, loss = 0.00054823\n",
      "Iteration 960, loss = 0.00054648\n",
      "Iteration 961, loss = 0.00054270\n",
      "Iteration 962, loss = 0.00054353\n",
      "Iteration 963, loss = 0.00055125\n",
      "Iteration 964, loss = 0.00054776\n",
      "Iteration 965, loss = 0.00054228\n",
      "Iteration 966, loss = 0.00054570\n",
      "Iteration 967, loss = 0.00054192\n",
      "Iteration 968, loss = 0.00053708\n",
      "Iteration 969, loss = 0.00053735\n",
      "Iteration 970, loss = 0.00053490\n",
      "Iteration 971, loss = 0.00053622\n",
      "Iteration 972, loss = 0.00053846\n",
      "Iteration 973, loss = 0.00053523\n",
      "Iteration 974, loss = 0.00053494\n",
      "Iteration 975, loss = 0.00053507\n",
      "Iteration 976, loss = 0.00053585\n",
      "Iteration 977, loss = 0.00053216\n",
      "Iteration 978, loss = 0.00053395\n",
      "Iteration 979, loss = 0.00053196\n",
      "Iteration 980, loss = 0.00053103\n",
      "Iteration 981, loss = 0.00053053\n",
      "Iteration 982, loss = 0.00053028\n",
      "Iteration 983, loss = 0.00053276\n",
      "Iteration 984, loss = 0.00053410\n",
      "Iteration 985, loss = 0.00052925\n",
      "Iteration 986, loss = 0.00052841\n",
      "Iteration 987, loss = 0.00052794\n",
      "Iteration 988, loss = 0.00052699\n",
      "Iteration 989, loss = 0.00053003\n",
      "Iteration 990, loss = 0.00054514\n",
      "Iteration 991, loss = 0.00053380\n",
      "Iteration 992, loss = 0.00052979\n",
      "Iteration 993, loss = 0.00052947\n",
      "Iteration 994, loss = 0.00052502\n",
      "Iteration 995, loss = 0.00052742\n",
      "Iteration 996, loss = 0.00052890\n",
      "Iteration 997, loss = 0.00052941\n",
      "Iteration 998, loss = 0.00052645\n",
      "Iteration 999, loss = 0.00052645\n",
      "Iteration 1000, loss = 0.00052372\n",
      "Iteration 1001, loss = 0.00052295\n",
      "Iteration 1002, loss = 0.00052163\n",
      "Iteration 1003, loss = 0.00052372\n",
      "Iteration 1004, loss = 0.00052273\n",
      "Iteration 1005, loss = 0.00052053\n",
      "Iteration 1006, loss = 0.00052156\n",
      "Iteration 1007, loss = 0.00052062\n",
      "Iteration 1008, loss = 0.00052368\n",
      "Iteration 1009, loss = 0.00052320\n",
      "Iteration 1010, loss = 0.00051964\n",
      "Iteration 1011, loss = 0.00052386\n",
      "Iteration 1012, loss = 0.00052169\n",
      "Iteration 1013, loss = 0.00052011\n",
      "Iteration 1014, loss = 0.00051800\n",
      "Iteration 1015, loss = 0.00051676\n",
      "Iteration 1016, loss = 0.00051708\n",
      "Iteration 1017, loss = 0.00051954\n",
      "Iteration 1018, loss = 0.00052114\n",
      "Iteration 1019, loss = 0.00051684\n",
      "Iteration 1020, loss = 0.00051726\n",
      "Iteration 1021, loss = 0.00051517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1022, loss = 0.00053397\n",
      "Iteration 1023, loss = 0.00052368\n",
      "Iteration 1024, loss = 0.00051975\n",
      "Iteration 1025, loss = 0.00052109\n",
      "Iteration 1026, loss = 0.00051387\n",
      "Iteration 1027, loss = 0.00051325\n",
      "Iteration 1028, loss = 0.00051277\n",
      "Iteration 1029, loss = 0.00051075\n",
      "Iteration 1030, loss = 0.00051411\n",
      "Iteration 1031, loss = 0.00051380\n",
      "Iteration 1032, loss = 0.00051479\n",
      "Iteration 1033, loss = 0.00051497\n",
      "Iteration 1034, loss = 0.00051266\n",
      "Iteration 1035, loss = 0.00051263\n",
      "Iteration 1036, loss = 0.00051472\n",
      "Iteration 1037, loss = 0.00051263\n",
      "Iteration 1038, loss = 0.00051054\n",
      "Iteration 1039, loss = 0.00051464\n",
      "Iteration 1040, loss = 0.00051810\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot ROC, AUC.\n",
    "classes=[\"Normal\",\"Pre-Ictal\",\"Seizure\"]\n",
    "visualizer = ROCAUC(mlp, classes=classes)\n",
    "\n",
    "visualizer.fit(X_train, y_train)  # Fit the training data to the visualizer\n",
    "visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "g = visualizer.poof()             # Draw/show/poof the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.36638348\n",
      "Iteration 2, loss = 0.25607374\n",
      "Iteration 3, loss = 0.21600529\n",
      "Iteration 4, loss = 0.19661974\n",
      "Iteration 5, loss = 0.18676429\n",
      "Iteration 6, loss = 0.17917422\n",
      "Iteration 7, loss = 0.17341084\n",
      "Iteration 8, loss = 0.16856754\n",
      "Iteration 9, loss = 0.16470497\n",
      "Iteration 10, loss = 0.16113570\n",
      "Iteration 11, loss = 0.15809339\n",
      "Iteration 12, loss = 0.15555725\n",
      "Iteration 13, loss = 0.15314988\n",
      "Iteration 14, loss = 0.15145479\n",
      "Iteration 15, loss = 0.14965862\n",
      "Iteration 16, loss = 0.14806356\n",
      "Iteration 17, loss = 0.14599863\n",
      "Iteration 18, loss = 0.14476214\n",
      "Iteration 19, loss = 0.14360478\n",
      "Iteration 20, loss = 0.14194466\n",
      "Iteration 21, loss = 0.14072223\n",
      "Iteration 22, loss = 0.13997716\n",
      "Iteration 23, loss = 0.13887746\n",
      "Iteration 24, loss = 0.13762222\n",
      "Iteration 25, loss = 0.13688575\n",
      "Iteration 26, loss = 0.13529189\n",
      "Iteration 27, loss = 0.13396246\n",
      "Iteration 28, loss = 0.13353976\n",
      "Iteration 29, loss = 0.13275356\n",
      "Iteration 30, loss = 0.13076496\n",
      "Iteration 31, loss = 0.12980392\n",
      "Iteration 32, loss = 0.12858601\n",
      "Iteration 33, loss = 0.12860786\n",
      "Iteration 34, loss = 0.13002592\n",
      "Iteration 35, loss = 0.12654675\n",
      "Iteration 36, loss = 0.12621261\n",
      "Iteration 37, loss = 0.12468945\n",
      "Iteration 38, loss = 0.12276312\n",
      "Iteration 39, loss = 0.12280701\n",
      "Iteration 40, loss = 0.12176858\n",
      "Iteration 41, loss = 0.12090566\n",
      "Iteration 42, loss = 0.11937118\n",
      "Iteration 43, loss = 0.11848504\n",
      "Iteration 44, loss = 0.11808509\n",
      "Iteration 45, loss = 0.11644492\n",
      "Iteration 46, loss = 0.11625104\n",
      "Iteration 47, loss = 0.11505246\n",
      "Iteration 48, loss = 0.11345752\n",
      "Iteration 49, loss = 0.11200751\n",
      "Iteration 50, loss = 0.11260452\n",
      "Iteration 51, loss = 0.11245668\n",
      "Iteration 52, loss = 0.11149542\n",
      "Iteration 53, loss = 0.11112822\n",
      "Iteration 54, loss = 0.10769915\n",
      "Iteration 55, loss = 0.10673551\n",
      "Iteration 56, loss = 0.10641139\n",
      "Iteration 57, loss = 0.10542293\n",
      "Iteration 58, loss = 0.10346176\n",
      "Iteration 59, loss = 0.10328231\n",
      "Iteration 60, loss = 0.10242481\n",
      "Iteration 61, loss = 0.10089513\n",
      "Iteration 62, loss = 0.09987679\n",
      "Iteration 63, loss = 0.09927990\n",
      "Iteration 64, loss = 0.09822534\n",
      "Iteration 65, loss = 0.09789865\n",
      "Iteration 66, loss = 0.09693381\n",
      "Iteration 67, loss = 0.09561564\n",
      "Iteration 68, loss = 0.09436163\n",
      "Iteration 69, loss = 0.09350855\n",
      "Iteration 70, loss = 0.09264776\n",
      "Iteration 71, loss = 0.09194485\n",
      "Iteration 72, loss = 0.09158402\n",
      "Iteration 73, loss = 0.08977430\n",
      "Iteration 74, loss = 0.09045195\n",
      "Iteration 75, loss = 0.08841128\n",
      "Iteration 76, loss = 0.08728550\n",
      "Iteration 77, loss = 0.08633947\n",
      "Iteration 78, loss = 0.08604888\n",
      "Iteration 79, loss = 0.08590343\n",
      "Iteration 80, loss = 0.08544526\n",
      "Iteration 81, loss = 0.08409661\n",
      "Iteration 82, loss = 0.08313336\n",
      "Iteration 83, loss = 0.08344720\n",
      "Iteration 84, loss = 0.08086358\n",
      "Iteration 85, loss = 0.07983336\n",
      "Iteration 86, loss = 0.07905176\n",
      "Iteration 87, loss = 0.07814645\n",
      "Iteration 88, loss = 0.07728290\n",
      "Iteration 89, loss = 0.07775431\n",
      "Iteration 90, loss = 0.07626541\n",
      "Iteration 91, loss = 0.07570882\n",
      "Iteration 92, loss = 0.07468800\n",
      "Iteration 93, loss = 0.07398660\n",
      "Iteration 94, loss = 0.07297298\n",
      "Iteration 95, loss = 0.07240406\n",
      "Iteration 96, loss = 0.07147583\n",
      "Iteration 97, loss = 0.07034968\n",
      "Iteration 98, loss = 0.07002436\n",
      "Iteration 99, loss = 0.07061031\n",
      "Iteration 100, loss = 0.07011102\n",
      "Iteration 101, loss = 0.06817444\n",
      "Iteration 102, loss = 0.06677450\n",
      "Iteration 103, loss = 0.06677675\n",
      "Iteration 104, loss = 0.06581405\n",
      "Iteration 105, loss = 0.06523244\n",
      "Iteration 106, loss = 0.06373459\n",
      "Iteration 107, loss = 0.06490925\n",
      "Iteration 108, loss = 0.06278234\n",
      "Iteration 109, loss = 0.06193284\n",
      "Iteration 110, loss = 0.06150498\n",
      "Iteration 111, loss = 0.06123963\n",
      "Iteration 112, loss = 0.06026698\n",
      "Iteration 113, loss = 0.05907963\n",
      "Iteration 114, loss = 0.05886654\n",
      "Iteration 115, loss = 0.05912128\n",
      "Iteration 116, loss = 0.05776762\n",
      "Iteration 117, loss = 0.05698126\n",
      "Iteration 118, loss = 0.05579222\n",
      "Iteration 119, loss = 0.05575763\n",
      "Iteration 120, loss = 0.05495158\n",
      "Iteration 121, loss = 0.05446841\n",
      "Iteration 122, loss = 0.05366743\n",
      "Iteration 123, loss = 0.05316090\n",
      "Iteration 124, loss = 0.05263055\n",
      "Iteration 125, loss = 0.05251644\n",
      "Iteration 126, loss = 0.05132705\n",
      "Iteration 127, loss = 0.05099687\n",
      "Iteration 128, loss = 0.05027449\n",
      "Iteration 129, loss = 0.05025671\n",
      "Iteration 130, loss = 0.04869747\n",
      "Iteration 131, loss = 0.04890358\n",
      "Iteration 132, loss = 0.04927997\n",
      "Iteration 133, loss = 0.04741681\n",
      "Iteration 134, loss = 0.04736905\n",
      "Iteration 135, loss = 0.04720380\n",
      "Iteration 136, loss = 0.04567745\n",
      "Iteration 137, loss = 0.04480703\n",
      "Iteration 138, loss = 0.04488129\n",
      "Iteration 139, loss = 0.04462675\n",
      "Iteration 140, loss = 0.04430479\n",
      "Iteration 141, loss = 0.04300281\n",
      "Iteration 142, loss = 0.04319905\n",
      "Iteration 143, loss = 0.04234195\n",
      "Iteration 144, loss = 0.04102243\n",
      "Iteration 145, loss = 0.04031973\n",
      "Iteration 146, loss = 0.03987993\n",
      "Iteration 147, loss = 0.03982746\n",
      "Iteration 148, loss = 0.03932685\n",
      "Iteration 149, loss = 0.03841915\n",
      "Iteration 150, loss = 0.03847052\n",
      "Iteration 151, loss = 0.03754661\n",
      "Iteration 152, loss = 0.03708569\n",
      "Iteration 153, loss = 0.03674681\n",
      "Iteration 154, loss = 0.03676668\n",
      "Iteration 155, loss = 0.03596803\n",
      "Iteration 156, loss = 0.03565781\n",
      "Iteration 157, loss = 0.03549362\n",
      "Iteration 158, loss = 0.03449737\n",
      "Iteration 159, loss = 0.03407596\n",
      "Iteration 160, loss = 0.03354533\n",
      "Iteration 161, loss = 0.03464444\n",
      "Iteration 162, loss = 0.03340355\n",
      "Iteration 163, loss = 0.03247358\n",
      "Iteration 164, loss = 0.03214892\n",
      "Iteration 165, loss = 0.03181407\n",
      "Iteration 166, loss = 0.03094199\n",
      "Iteration 167, loss = 0.03071684\n",
      "Iteration 168, loss = 0.03045289\n",
      "Iteration 169, loss = 0.02987009\n",
      "Iteration 170, loss = 0.02968792\n",
      "Iteration 171, loss = 0.02940472\n",
      "Iteration 172, loss = 0.02895699\n",
      "Iteration 173, loss = 0.02831583\n",
      "Iteration 174, loss = 0.02820867\n",
      "Iteration 175, loss = 0.02812229\n",
      "Iteration 176, loss = 0.02744544\n",
      "Iteration 177, loss = 0.02820273\n",
      "Iteration 178, loss = 0.02752725\n",
      "Iteration 179, loss = 0.02669624\n",
      "Iteration 180, loss = 0.02594912\n",
      "Iteration 181, loss = 0.02541633\n",
      "Iteration 182, loss = 0.02549234\n",
      "Iteration 183, loss = 0.02526135\n",
      "Iteration 184, loss = 0.02502930\n",
      "Iteration 185, loss = 0.02456449\n",
      "Iteration 186, loss = 0.02405564\n",
      "Iteration 187, loss = 0.02384074\n",
      "Iteration 188, loss = 0.02320891\n",
      "Iteration 189, loss = 0.02341279\n",
      "Iteration 190, loss = 0.02339158\n",
      "Iteration 191, loss = 0.02278010\n",
      "Iteration 192, loss = 0.02193315\n",
      "Iteration 193, loss = 0.02176505\n",
      "Iteration 194, loss = 0.02130437\n",
      "Iteration 195, loss = 0.02120474\n",
      "Iteration 196, loss = 0.02116224\n",
      "Iteration 197, loss = 0.02107451\n",
      "Iteration 198, loss = 0.02059370\n",
      "Iteration 199, loss = 0.02008888\n",
      "Iteration 200, loss = 0.01994740\n",
      "Iteration 201, loss = 0.01966486\n",
      "Iteration 202, loss = 0.01933057\n",
      "Iteration 203, loss = 0.01915372\n",
      "Iteration 204, loss = 0.01885015\n",
      "Iteration 205, loss = 0.01846458\n",
      "Iteration 206, loss = 0.01822949\n",
      "Iteration 207, loss = 0.01814646\n",
      "Iteration 208, loss = 0.01780775\n",
      "Iteration 209, loss = 0.01770930\n",
      "Iteration 210, loss = 0.01752168\n",
      "Iteration 211, loss = 0.01728456\n",
      "Iteration 212, loss = 0.01734450\n",
      "Iteration 213, loss = 0.01682072\n",
      "Iteration 214, loss = 0.01656401\n",
      "Iteration 215, loss = 0.01696001\n",
      "Iteration 216, loss = 0.01690412\n",
      "Iteration 217, loss = 0.01591968\n",
      "Iteration 218, loss = 0.01565977\n",
      "Iteration 219, loss = 0.01543328\n",
      "Iteration 220, loss = 0.01531005\n",
      "Iteration 221, loss = 0.01545508\n",
      "Iteration 222, loss = 0.01497334\n",
      "Iteration 223, loss = 0.01481329\n",
      "Iteration 224, loss = 0.01449499\n",
      "Iteration 225, loss = 0.01422543\n",
      "Iteration 226, loss = 0.01404705\n",
      "Iteration 227, loss = 0.01395832\n",
      "Iteration 228, loss = 0.01370878\n",
      "Iteration 229, loss = 0.01344818\n",
      "Iteration 230, loss = 0.01327807\n",
      "Iteration 231, loss = 0.01333412\n",
      "Iteration 232, loss = 0.01313697\n",
      "Iteration 233, loss = 0.01318333\n",
      "Iteration 234, loss = 0.01282383\n",
      "Iteration 235, loss = 0.01240778\n",
      "Iteration 236, loss = 0.01227168\n",
      "Iteration 237, loss = 0.01230512\n",
      "Iteration 238, loss = 0.01194616\n",
      "Iteration 239, loss = 0.01189884\n",
      "Iteration 240, loss = 0.01164530\n",
      "Iteration 241, loss = 0.01156030\n",
      "Iteration 242, loss = 0.01157246\n",
      "Iteration 243, loss = 0.01123135\n",
      "Iteration 244, loss = 0.01118161\n",
      "Iteration 245, loss = 0.01093887\n",
      "Iteration 246, loss = 0.01077994\n",
      "Iteration 247, loss = 0.01062060\n",
      "Iteration 248, loss = 0.01055513\n",
      "Iteration 249, loss = 0.01036694\n",
      "Iteration 250, loss = 0.01051268\n",
      "Iteration 251, loss = 0.01019528\n",
      "Iteration 252, loss = 0.01006372\n",
      "Iteration 253, loss = 0.00979956\n",
      "Iteration 254, loss = 0.00997653\n",
      "Iteration 255, loss = 0.00981988\n",
      "Iteration 256, loss = 0.01008393\n",
      "Iteration 257, loss = 0.00943861\n",
      "Iteration 258, loss = 0.00927435\n",
      "Iteration 259, loss = 0.00918047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 260, loss = 0.00902615\n",
      "Iteration 261, loss = 0.00907121\n",
      "Iteration 262, loss = 0.00936268\n",
      "Iteration 263, loss = 0.00884862\n",
      "Iteration 264, loss = 0.00867319\n",
      "Iteration 265, loss = 0.00852770\n",
      "Iteration 266, loss = 0.00849651\n",
      "Iteration 267, loss = 0.00836835\n",
      "Iteration 268, loss = 0.00837537\n",
      "Iteration 269, loss = 0.00824232\n",
      "Iteration 270, loss = 0.00804143\n",
      "Iteration 271, loss = 0.00806270\n",
      "Iteration 272, loss = 0.00811088\n",
      "Iteration 273, loss = 0.00792832\n",
      "Iteration 274, loss = 0.00765861\n",
      "Iteration 275, loss = 0.00763129\n",
      "Iteration 276, loss = 0.00754786\n",
      "Iteration 277, loss = 0.00759429\n",
      "Iteration 278, loss = 0.00739580\n",
      "Iteration 279, loss = 0.00724897\n",
      "Iteration 280, loss = 0.00715250\n",
      "Iteration 281, loss = 0.00707605\n",
      "Iteration 282, loss = 0.00700985\n",
      "Iteration 283, loss = 0.00696093\n",
      "Iteration 284, loss = 0.00684406\n",
      "Iteration 285, loss = 0.00683584\n",
      "Iteration 286, loss = 0.00683085\n",
      "Iteration 287, loss = 0.00667594\n",
      "Iteration 288, loss = 0.00656715\n",
      "Iteration 289, loss = 0.00647932\n",
      "Iteration 290, loss = 0.00644589\n",
      "Iteration 291, loss = 0.00638817\n",
      "Iteration 292, loss = 0.00625506\n",
      "Iteration 293, loss = 0.00616878\n",
      "Iteration 294, loss = 0.00615143\n",
      "Iteration 295, loss = 0.00608768\n",
      "Iteration 296, loss = 0.00594579\n",
      "Iteration 297, loss = 0.00593690\n",
      "Iteration 298, loss = 0.00589652\n",
      "Iteration 299, loss = 0.00586016\n",
      "Iteration 300, loss = 0.00577239\n",
      "Iteration 301, loss = 0.00569981\n",
      "Iteration 302, loss = 0.00568343\n",
      "Iteration 303, loss = 0.00559327\n",
      "Iteration 304, loss = 0.00548463\n",
      "Iteration 305, loss = 0.00550772\n",
      "Iteration 306, loss = 0.00547161\n",
      "Iteration 307, loss = 0.00536333\n",
      "Iteration 308, loss = 0.00530941\n",
      "Iteration 309, loss = 0.00533653\n",
      "Iteration 310, loss = 0.00528631\n",
      "Iteration 311, loss = 0.00513238\n",
      "Iteration 312, loss = 0.00510202\n",
      "Iteration 313, loss = 0.00503819\n",
      "Iteration 314, loss = 0.00504783\n",
      "Iteration 315, loss = 0.00495989\n",
      "Iteration 316, loss = 0.00488432\n",
      "Iteration 317, loss = 0.00485678\n",
      "Iteration 318, loss = 0.00482721\n",
      "Iteration 319, loss = 0.00475950\n",
      "Iteration 320, loss = 0.00471816\n",
      "Iteration 321, loss = 0.00468494\n",
      "Iteration 322, loss = 0.00460754\n",
      "Iteration 323, loss = 0.00460111\n",
      "Iteration 324, loss = 0.00451259\n",
      "Iteration 325, loss = 0.00444892\n",
      "Iteration 326, loss = 0.00443788\n",
      "Iteration 327, loss = 0.00437515\n",
      "Iteration 328, loss = 0.00431552\n",
      "Iteration 329, loss = 0.00431345\n",
      "Iteration 330, loss = 0.00428522\n",
      "Iteration 331, loss = 0.00428520\n",
      "Iteration 332, loss = 0.00420445\n",
      "Iteration 333, loss = 0.00411801\n",
      "Iteration 334, loss = 0.00412689\n",
      "Iteration 335, loss = 0.00405081\n",
      "Iteration 336, loss = 0.00402521\n",
      "Iteration 337, loss = 0.00396779\n",
      "Iteration 338, loss = 0.00392497\n",
      "Iteration 339, loss = 0.00390599\n",
      "Iteration 340, loss = 0.00387647\n",
      "Iteration 341, loss = 0.00382875\n",
      "Iteration 342, loss = 0.00382638\n",
      "Iteration 343, loss = 0.00377495\n",
      "Iteration 344, loss = 0.00377109\n",
      "Iteration 345, loss = 0.00372306\n",
      "Iteration 346, loss = 0.00369582\n",
      "Iteration 347, loss = 0.00365736\n",
      "Iteration 348, loss = 0.00370918\n",
      "Iteration 349, loss = 0.00354862\n",
      "Iteration 350, loss = 0.00360560\n",
      "Iteration 351, loss = 0.00358258\n",
      "Iteration 352, loss = 0.00349415\n",
      "Iteration 353, loss = 0.00346552\n",
      "Iteration 354, loss = 0.00343340\n",
      "Iteration 355, loss = 0.00339338\n",
      "Iteration 356, loss = 0.00337399\n",
      "Iteration 357, loss = 0.00338058\n",
      "Iteration 358, loss = 0.00332023\n",
      "Iteration 359, loss = 0.00329096\n",
      "Iteration 360, loss = 0.00324737\n",
      "Iteration 361, loss = 0.00321884\n",
      "Iteration 362, loss = 0.00321561\n",
      "Iteration 363, loss = 0.00316631\n",
      "Iteration 364, loss = 0.00315594\n",
      "Iteration 365, loss = 0.00312216\n",
      "Iteration 366, loss = 0.00312766\n",
      "Iteration 367, loss = 0.00310359\n",
      "Iteration 368, loss = 0.00303690\n",
      "Iteration 369, loss = 0.00303776\n",
      "Iteration 370, loss = 0.00302039\n",
      "Iteration 371, loss = 0.00300138\n",
      "Iteration 372, loss = 0.00296140\n",
      "Iteration 373, loss = 0.00295397\n",
      "Iteration 374, loss = 0.00291388\n",
      "Iteration 375, loss = 0.00289944\n",
      "Iteration 376, loss = 0.00287097\n",
      "Iteration 377, loss = 0.00285649\n",
      "Iteration 378, loss = 0.00284877\n",
      "Iteration 379, loss = 0.00281117\n",
      "Iteration 380, loss = 0.00278693\n",
      "Iteration 381, loss = 0.00276810\n",
      "Iteration 382, loss = 0.00275254\n",
      "Iteration 383, loss = 0.00271897\n",
      "Iteration 384, loss = 0.00270947\n",
      "Iteration 385, loss = 0.00269720\n",
      "Iteration 386, loss = 0.00267168\n",
      "Iteration 387, loss = 0.00266640\n",
      "Iteration 388, loss = 0.00262784\n",
      "Iteration 389, loss = 0.00260641\n",
      "Iteration 390, loss = 0.00259061\n",
      "Iteration 391, loss = 0.00257676\n",
      "Iteration 392, loss = 0.00256388\n",
      "Iteration 393, loss = 0.00253177\n",
      "Iteration 394, loss = 0.00251957\n",
      "Iteration 395, loss = 0.00250270\n",
      "Iteration 396, loss = 0.00249405\n",
      "Iteration 397, loss = 0.00248442\n",
      "Iteration 398, loss = 0.00244622\n",
      "Iteration 399, loss = 0.00243941\n",
      "Iteration 400, loss = 0.00241942\n",
      "Iteration 401, loss = 0.00240825\n",
      "Iteration 402, loss = 0.00239954\n",
      "Iteration 403, loss = 0.00237726\n",
      "Iteration 404, loss = 0.00235554\n",
      "Iteration 405, loss = 0.00235263\n",
      "Iteration 406, loss = 0.00236097\n",
      "Iteration 407, loss = 0.00231013\n",
      "Iteration 408, loss = 0.00228783\n",
      "Iteration 409, loss = 0.00227614\n",
      "Iteration 410, loss = 0.00226025\n",
      "Iteration 411, loss = 0.00225853\n",
      "Iteration 412, loss = 0.00223618\n",
      "Iteration 413, loss = 0.00221816\n",
      "Iteration 414, loss = 0.00221179\n",
      "Iteration 415, loss = 0.00218895\n",
      "Iteration 416, loss = 0.00218231\n",
      "Iteration 417, loss = 0.00216399\n",
      "Iteration 418, loss = 0.00215102\n",
      "Iteration 419, loss = 0.00213980\n",
      "Iteration 420, loss = 0.00214246\n",
      "Iteration 421, loss = 0.00211889\n",
      "Iteration 422, loss = 0.00210139\n",
      "Iteration 423, loss = 0.00209577\n",
      "Iteration 424, loss = 0.00207899\n",
      "Iteration 425, loss = 0.00206621\n",
      "Iteration 426, loss = 0.00206701\n",
      "Iteration 427, loss = 0.00205788\n",
      "Iteration 428, loss = 0.00204939\n",
      "Iteration 429, loss = 0.00202152\n",
      "Iteration 430, loss = 0.00201437\n",
      "Iteration 431, loss = 0.00200354\n",
      "Iteration 432, loss = 0.00198437\n",
      "Iteration 433, loss = 0.00197410\n",
      "Iteration 434, loss = 0.00196011\n",
      "Iteration 435, loss = 0.00197645\n",
      "Iteration 436, loss = 0.00195001\n",
      "Iteration 437, loss = 0.00192681\n",
      "Iteration 438, loss = 0.00193064\n",
      "Iteration 439, loss = 0.00191835\n",
      "Iteration 440, loss = 0.00195318\n",
      "Iteration 441, loss = 0.00189740\n",
      "Iteration 442, loss = 0.00187941\n",
      "Iteration 443, loss = 0.00187350\n",
      "Iteration 444, loss = 0.00185635\n",
      "Iteration 445, loss = 0.00185820\n",
      "Iteration 446, loss = 0.00185727\n",
      "Iteration 447, loss = 0.00185537\n",
      "Iteration 448, loss = 0.00184724\n",
      "Iteration 449, loss = 0.00181202\n",
      "Iteration 450, loss = 0.00179887\n",
      "Iteration 451, loss = 0.00180384\n",
      "Iteration 452, loss = 0.00178824\n",
      "Iteration 453, loss = 0.00177543\n",
      "Iteration 454, loss = 0.00177160\n",
      "Iteration 455, loss = 0.00176497\n",
      "Iteration 456, loss = 0.00175208\n",
      "Iteration 457, loss = 0.00174639\n",
      "Iteration 458, loss = 0.00173817\n",
      "Iteration 459, loss = 0.00174945\n",
      "Iteration 460, loss = 0.00172225\n",
      "Iteration 461, loss = 0.00172301\n",
      "Iteration 462, loss = 0.00169970\n",
      "Iteration 463, loss = 0.00171056\n",
      "Iteration 464, loss = 0.00168701\n",
      "Iteration 465, loss = 0.00167980\n",
      "Iteration 466, loss = 0.00166938\n",
      "Iteration 467, loss = 0.00166724\n",
      "Iteration 468, loss = 0.00165393\n",
      "Iteration 469, loss = 0.00164959\n",
      "Iteration 470, loss = 0.00164328\n",
      "Iteration 471, loss = 0.00163351\n",
      "Iteration 472, loss = 0.00162608\n",
      "Iteration 473, loss = 0.00161766\n",
      "Iteration 474, loss = 0.00162455\n",
      "Iteration 475, loss = 0.00160698\n",
      "Iteration 476, loss = 0.00159442\n",
      "Iteration 477, loss = 0.00159198\n",
      "Iteration 478, loss = 0.00158601\n",
      "Iteration 479, loss = 0.00157810\n",
      "Iteration 480, loss = 0.00157020\n",
      "Iteration 481, loss = 0.00157099\n",
      "Iteration 482, loss = 0.00155894\n",
      "Iteration 483, loss = 0.00154907\n",
      "Iteration 484, loss = 0.00155013\n",
      "Iteration 485, loss = 0.00154361\n",
      "Iteration 486, loss = 0.00153325\n",
      "Iteration 487, loss = 0.00152619\n",
      "Iteration 488, loss = 0.00152397\n",
      "Iteration 489, loss = 0.00152473\n",
      "Iteration 490, loss = 0.00152202\n",
      "Iteration 491, loss = 0.00150376\n",
      "Iteration 492, loss = 0.00150367\n",
      "Iteration 493, loss = 0.00149541\n",
      "Iteration 494, loss = 0.00148699\n",
      "Iteration 495, loss = 0.00148283\n",
      "Iteration 496, loss = 0.00148123\n",
      "Iteration 497, loss = 0.00147684\n",
      "Iteration 498, loss = 0.00147274\n",
      "Iteration 499, loss = 0.00147227\n",
      "Iteration 500, loss = 0.00145818\n",
      "Iteration 501, loss = 0.00146455\n",
      "Iteration 502, loss = 0.00144606\n",
      "Iteration 503, loss = 0.00143666\n",
      "Iteration 504, loss = 0.00143304\n",
      "Iteration 505, loss = 0.00142781\n",
      "Iteration 506, loss = 0.00142650\n",
      "Iteration 507, loss = 0.00142015\n",
      "Iteration 508, loss = 0.00141188\n",
      "Iteration 509, loss = 0.00140717\n",
      "Iteration 510, loss = 0.00140281\n",
      "Iteration 511, loss = 0.00140054\n",
      "Iteration 512, loss = 0.00139423\n",
      "Iteration 513, loss = 0.00138816\n",
      "Iteration 514, loss = 0.00138574\n",
      "Iteration 515, loss = 0.00137928\n",
      "Iteration 516, loss = 0.00137467\n",
      "Iteration 517, loss = 0.00137176\n",
      "Iteration 518, loss = 0.00136334\n",
      "Iteration 519, loss = 0.00136149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 520, loss = 0.00135517\n",
      "Iteration 521, loss = 0.00135386\n",
      "Iteration 522, loss = 0.00134786\n",
      "Iteration 523, loss = 0.00134744\n",
      "Iteration 524, loss = 0.00133924\n",
      "Iteration 525, loss = 0.00134312\n",
      "Iteration 526, loss = 0.00134215\n",
      "Iteration 527, loss = 0.00133620\n",
      "Iteration 528, loss = 0.00132378\n",
      "Iteration 529, loss = 0.00131974\n",
      "Iteration 530, loss = 0.00131616\n",
      "Iteration 531, loss = 0.00131348\n",
      "Iteration 532, loss = 0.00131091\n",
      "Iteration 533, loss = 0.00130425\n",
      "Iteration 534, loss = 0.00130254\n",
      "Iteration 535, loss = 0.00129684\n",
      "Iteration 536, loss = 0.00129351\n",
      "Iteration 537, loss = 0.00129736\n",
      "Iteration 538, loss = 0.00128455\n",
      "Iteration 539, loss = 0.00128476\n",
      "Iteration 540, loss = 0.00127972\n",
      "Iteration 541, loss = 0.00127463\n",
      "Iteration 542, loss = 0.00127131\n",
      "Iteration 543, loss = 0.00126885\n",
      "Iteration 544, loss = 0.00126365\n",
      "Iteration 545, loss = 0.00126449\n",
      "Iteration 546, loss = 0.00126629\n",
      "Iteration 547, loss = 0.00125746\n",
      "Iteration 548, loss = 0.00125630\n",
      "Iteration 549, loss = 0.00124945\n",
      "Iteration 550, loss = 0.00124687\n",
      "Iteration 551, loss = 0.00123941\n",
      "Iteration 552, loss = 0.00124331\n",
      "Iteration 553, loss = 0.00124473\n",
      "Iteration 554, loss = 0.00124597\n",
      "Iteration 555, loss = 0.00123255\n",
      "Iteration 556, loss = 0.00123319\n",
      "Iteration 557, loss = 0.00123220\n",
      "Iteration 558, loss = 0.00122096\n",
      "Iteration 559, loss = 0.00121939\n",
      "Iteration 560, loss = 0.00121403\n",
      "Iteration 561, loss = 0.00120878\n",
      "Iteration 562, loss = 0.00120559\n",
      "Iteration 563, loss = 0.00120477\n",
      "Iteration 564, loss = 0.00120049\n",
      "Iteration 565, loss = 0.00119847\n",
      "Iteration 566, loss = 0.00119826\n",
      "Iteration 567, loss = 0.00119461\n",
      "Iteration 568, loss = 0.00118807\n",
      "Iteration 569, loss = 0.00118649\n",
      "Iteration 570, loss = 0.00118716\n",
      "Iteration 571, loss = 0.00118256\n",
      "Iteration 572, loss = 0.00118057\n",
      "Iteration 573, loss = 0.00117605\n",
      "Iteration 574, loss = 0.00117445\n",
      "Iteration 575, loss = 0.00117269\n",
      "Iteration 576, loss = 0.00117491\n",
      "Iteration 577, loss = 0.00116763\n",
      "Iteration 578, loss = 0.00116721\n",
      "Iteration 579, loss = 0.00116631\n",
      "Iteration 580, loss = 0.00116069\n",
      "Iteration 581, loss = 0.00115578\n",
      "Iteration 582, loss = 0.00115215\n",
      "Iteration 583, loss = 0.00115435\n",
      "Iteration 584, loss = 0.00114736\n",
      "Iteration 585, loss = 0.00115149\n",
      "Iteration 586, loss = 0.00114364\n",
      "Iteration 587, loss = 0.00114520\n",
      "Iteration 588, loss = 0.00114557\n",
      "Iteration 589, loss = 0.00114217\n",
      "Iteration 590, loss = 0.00113799\n",
      "Iteration 591, loss = 0.00113427\n",
      "Iteration 592, loss = 0.00113127\n",
      "Iteration 593, loss = 0.00112629\n",
      "Iteration 594, loss = 0.00112488\n",
      "Iteration 595, loss = 0.00112865\n",
      "Iteration 596, loss = 0.00112329\n",
      "Iteration 597, loss = 0.00111638\n",
      "Iteration 598, loss = 0.00111464\n",
      "Iteration 599, loss = 0.00111309\n",
      "Iteration 600, loss = 0.00111245\n",
      "Iteration 601, loss = 0.00110853\n",
      "Iteration 602, loss = 0.00110803\n",
      "Iteration 603, loss = 0.00110592\n",
      "Iteration 604, loss = 0.00110260\n",
      "Iteration 605, loss = 0.00110152\n",
      "Iteration 606, loss = 0.00109700\n",
      "Iteration 607, loss = 0.00110004\n",
      "Iteration 608, loss = 0.00109792\n",
      "Iteration 609, loss = 0.00109265\n",
      "Iteration 610, loss = 0.00108955\n",
      "Iteration 611, loss = 0.00108916\n",
      "Iteration 612, loss = 0.00108678\n",
      "Iteration 613, loss = 0.00109560\n",
      "Iteration 614, loss = 0.00109143\n",
      "Iteration 615, loss = 0.00108039\n",
      "Iteration 616, loss = 0.00107984\n",
      "Iteration 617, loss = 0.00107459\n",
      "Iteration 618, loss = 0.00107229\n",
      "Iteration 619, loss = 0.00107037\n",
      "Iteration 620, loss = 0.00106865\n",
      "Iteration 621, loss = 0.00106947\n",
      "Iteration 622, loss = 0.00107186\n",
      "Iteration 623, loss = 0.00106462\n",
      "Iteration 624, loss = 0.00106108\n",
      "Iteration 625, loss = 0.00105977\n",
      "Iteration 626, loss = 0.00105711\n",
      "Iteration 627, loss = 0.00105727\n",
      "Iteration 628, loss = 0.00105536\n",
      "Iteration 629, loss = 0.00105147\n",
      "Iteration 630, loss = 0.00105073\n",
      "Iteration 631, loss = 0.00104938\n",
      "Iteration 632, loss = 0.00104939\n",
      "Iteration 633, loss = 0.00104660\n",
      "Iteration 634, loss = 0.00104354\n",
      "Iteration 635, loss = 0.00104110\n",
      "Iteration 636, loss = 0.00104096\n",
      "Iteration 637, loss = 0.00103909\n",
      "Iteration 638, loss = 0.00103655\n",
      "Iteration 639, loss = 0.00103567\n",
      "Iteration 640, loss = 0.00103447\n",
      "Iteration 641, loss = 0.00103246\n",
      "Iteration 642, loss = 0.00103246\n",
      "Iteration 643, loss = 0.00102941\n",
      "Iteration 644, loss = 0.00102879\n",
      "Iteration 645, loss = 0.00102670\n",
      "Iteration 646, loss = 0.00102375\n",
      "Iteration 647, loss = 0.00102141\n",
      "Iteration 648, loss = 0.00102262\n",
      "Iteration 649, loss = 0.00102196\n",
      "Iteration 650, loss = 0.00101866\n",
      "Iteration 651, loss = 0.00101749\n",
      "Iteration 652, loss = 0.00101435\n",
      "Iteration 653, loss = 0.00101717\n",
      "Iteration 654, loss = 0.00101180\n",
      "Iteration 655, loss = 0.00101157\n",
      "Iteration 656, loss = 0.00101033\n",
      "Iteration 657, loss = 0.00100703\n",
      "Iteration 658, loss = 0.00100574\n",
      "Iteration 659, loss = 0.00100223\n",
      "Iteration 660, loss = 0.00100331\n",
      "Iteration 661, loss = 0.00100424\n",
      "Iteration 662, loss = 0.00100052\n",
      "Iteration 663, loss = 0.00099721\n",
      "Iteration 664, loss = 0.00099465\n",
      "Iteration 665, loss = 0.00099417\n",
      "Iteration 666, loss = 0.00099247\n",
      "Iteration 667, loss = 0.00099145\n",
      "Iteration 668, loss = 0.00098837\n",
      "Iteration 669, loss = 0.00098936\n",
      "Iteration 670, loss = 0.00098745\n",
      "Iteration 671, loss = 0.00098503\n",
      "Iteration 672, loss = 0.00098168\n",
      "Iteration 673, loss = 0.00098249\n",
      "Iteration 674, loss = 0.00098031\n",
      "Iteration 675, loss = 0.00097825\n",
      "Iteration 676, loss = 0.00097640\n",
      "Iteration 677, loss = 0.00097589\n",
      "Iteration 678, loss = 0.00097501\n",
      "Iteration 679, loss = 0.00097460\n",
      "Iteration 680, loss = 0.00097128\n",
      "Iteration 681, loss = 0.00097036\n",
      "Iteration 682, loss = 0.00096863\n",
      "Iteration 683, loss = 0.00096626\n",
      "Iteration 684, loss = 0.00096506\n",
      "Iteration 685, loss = 0.00096411\n",
      "Iteration 686, loss = 0.00096330\n",
      "Iteration 687, loss = 0.00096523\n",
      "Iteration 688, loss = 0.00096468\n",
      "Iteration 689, loss = 0.00096387\n",
      "Iteration 690, loss = 0.00095938\n",
      "Iteration 691, loss = 0.00095600\n",
      "Iteration 692, loss = 0.00095509\n",
      "Iteration 693, loss = 0.00095427\n",
      "Iteration 694, loss = 0.00095114\n",
      "Iteration 695, loss = 0.00094999\n",
      "Iteration 696, loss = 0.00094791\n",
      "Iteration 697, loss = 0.00094656\n",
      "Iteration 698, loss = 0.00094649\n",
      "Iteration 699, loss = 0.00094560\n",
      "Iteration 700, loss = 0.00094291\n",
      "Iteration 701, loss = 0.00094281\n",
      "Iteration 702, loss = 0.00094230\n",
      "Iteration 703, loss = 0.00093902\n",
      "Iteration 704, loss = 0.00093821\n",
      "Iteration 705, loss = 0.00093811\n",
      "Iteration 706, loss = 0.00093653\n",
      "Iteration 707, loss = 0.00094006\n",
      "Iteration 708, loss = 0.00093788\n",
      "Iteration 709, loss = 0.00093408\n",
      "Iteration 710, loss = 0.00093023\n",
      "Iteration 711, loss = 0.00093079\n",
      "Iteration 712, loss = 0.00092753\n",
      "Iteration 713, loss = 0.00092612\n",
      "Iteration 714, loss = 0.00092540\n",
      "Iteration 715, loss = 0.00092718\n",
      "Iteration 716, loss = 0.00092523\n",
      "Iteration 717, loss = 0.00092390\n",
      "Iteration 718, loss = 0.00092193\n",
      "Iteration 719, loss = 0.00091871\n",
      "Iteration 720, loss = 0.00091726\n",
      "Iteration 721, loss = 0.00091603\n",
      "Iteration 722, loss = 0.00091333\n",
      "Iteration 723, loss = 0.00091433\n",
      "Iteration 724, loss = 0.00091415\n",
      "Iteration 725, loss = 0.00091211\n",
      "Iteration 726, loss = 0.00091012\n",
      "Iteration 727, loss = 0.00091028\n",
      "Iteration 728, loss = 0.00090863\n",
      "Iteration 729, loss = 0.00090554\n",
      "Iteration 730, loss = 0.00090555\n",
      "Iteration 731, loss = 0.00090342\n",
      "Iteration 732, loss = 0.00090321\n",
      "Iteration 733, loss = 0.00090088\n",
      "Iteration 734, loss = 0.00089920\n",
      "Iteration 735, loss = 0.00089788\n",
      "Iteration 736, loss = 0.00089972\n",
      "Iteration 737, loss = 0.00090180\n",
      "Iteration 738, loss = 0.00089436\n",
      "Iteration 739, loss = 0.00089274\n",
      "Iteration 740, loss = 0.00089086\n",
      "Iteration 741, loss = 0.00088827\n",
      "Iteration 742, loss = 0.00088885\n",
      "Iteration 743, loss = 0.00088921\n",
      "Iteration 744, loss = 0.00088730\n",
      "Iteration 745, loss = 0.00088399\n",
      "Iteration 746, loss = 0.00088411\n",
      "Iteration 747, loss = 0.00088411\n",
      "Iteration 748, loss = 0.00088437\n",
      "Iteration 749, loss = 0.00088101\n",
      "Iteration 750, loss = 0.00088140\n",
      "Iteration 751, loss = 0.00087887\n",
      "Iteration 752, loss = 0.00087753\n",
      "Iteration 753, loss = 0.00087555\n",
      "Iteration 754, loss = 0.00087436\n",
      "Iteration 755, loss = 0.00087508\n",
      "Iteration 756, loss = 0.00087231\n",
      "Iteration 757, loss = 0.00087051\n",
      "Iteration 758, loss = 0.00087199\n",
      "Iteration 759, loss = 0.00087080\n",
      "Iteration 760, loss = 0.00086879\n",
      "Iteration 761, loss = 0.00087039\n",
      "Iteration 762, loss = 0.00086717\n",
      "Iteration 763, loss = 0.00086388\n",
      "Iteration 764, loss = 0.00086429\n",
      "Iteration 765, loss = 0.00086421\n",
      "Iteration 766, loss = 0.00086035\n",
      "Iteration 767, loss = 0.00085850\n",
      "Iteration 768, loss = 0.00086138\n",
      "Iteration 769, loss = 0.00085625\n",
      "Iteration 770, loss = 0.00085485\n",
      "Iteration 771, loss = 0.00085319\n",
      "Iteration 772, loss = 0.00085243\n",
      "Iteration 773, loss = 0.00085194\n",
      "Iteration 774, loss = 0.00085199\n",
      "Iteration 775, loss = 0.00085231\n",
      "Iteration 776, loss = 0.00084881\n",
      "Iteration 777, loss = 0.00084840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 778, loss = 0.00084616\n",
      "Iteration 779, loss = 0.00084368\n",
      "Iteration 780, loss = 0.00084460\n",
      "Iteration 781, loss = 0.00084224\n",
      "Iteration 782, loss = 0.00084160\n",
      "Iteration 783, loss = 0.00084271\n",
      "Iteration 784, loss = 0.00084037\n",
      "Iteration 785, loss = 0.00083975\n",
      "Iteration 786, loss = 0.00083691\n",
      "Iteration 787, loss = 0.00083496\n",
      "Iteration 788, loss = 0.00083519\n",
      "Iteration 789, loss = 0.00083555\n",
      "Iteration 790, loss = 0.00083230\n",
      "Iteration 791, loss = 0.00083391\n",
      "Iteration 792, loss = 0.00082874\n",
      "Iteration 793, loss = 0.00082768\n",
      "Iteration 794, loss = 0.00082858\n",
      "Iteration 795, loss = 0.00082571\n",
      "Iteration 796, loss = 0.00082745\n",
      "Iteration 797, loss = 0.00082713\n",
      "Iteration 798, loss = 0.00082412\n",
      "Iteration 799, loss = 0.00082181\n",
      "Iteration 800, loss = 0.00082054\n",
      "Iteration 801, loss = 0.00081888\n",
      "Iteration 802, loss = 0.00082069\n",
      "Iteration 803, loss = 0.00081715\n",
      "Iteration 804, loss = 0.00081594\n",
      "Iteration 805, loss = 0.00081368\n",
      "Iteration 806, loss = 0.00081329\n",
      "Iteration 807, loss = 0.00081570\n",
      "Iteration 808, loss = 0.00081350\n",
      "Iteration 809, loss = 0.00081345\n",
      "Iteration 810, loss = 0.00081217\n",
      "Iteration 811, loss = 0.00080858\n",
      "Iteration 812, loss = 0.00080815\n",
      "Iteration 813, loss = 0.00080584\n",
      "Iteration 814, loss = 0.00081029\n",
      "Iteration 815, loss = 0.00081413\n",
      "Iteration 816, loss = 0.00080987\n",
      "Iteration 817, loss = 0.00080514\n",
      "Iteration 818, loss = 0.00080077\n",
      "Iteration 819, loss = 0.00080044\n",
      "Iteration 820, loss = 0.00079956\n",
      "Iteration 821, loss = 0.00079640\n",
      "Iteration 822, loss = 0.00079649\n",
      "Iteration 823, loss = 0.00079516\n",
      "Iteration 824, loss = 0.00079447\n",
      "Iteration 825, loss = 0.00079323\n",
      "Iteration 826, loss = 0.00079211\n",
      "Iteration 827, loss = 0.00079011\n",
      "Iteration 828, loss = 0.00079217\n",
      "Iteration 829, loss = 0.00080043\n",
      "Iteration 830, loss = 0.00079120\n",
      "Iteration 831, loss = 0.00078825\n",
      "Iteration 832, loss = 0.00078642\n",
      "Iteration 833, loss = 0.00078663\n",
      "Iteration 834, loss = 0.00078643\n",
      "Iteration 835, loss = 0.00078267\n",
      "Iteration 836, loss = 0.00078227\n",
      "Iteration 837, loss = 0.00078030\n",
      "Iteration 838, loss = 0.00078003\n",
      "Iteration 839, loss = 0.00077788\n",
      "Iteration 840, loss = 0.00077655\n",
      "Iteration 841, loss = 0.00077797\n",
      "Iteration 842, loss = 0.00077676\n",
      "Iteration 843, loss = 0.00078177\n",
      "Iteration 844, loss = 0.00077892\n",
      "Iteration 845, loss = 0.00077443\n",
      "Iteration 846, loss = 0.00077643\n",
      "Iteration 847, loss = 0.00077484\n",
      "Iteration 848, loss = 0.00076973\n",
      "Iteration 849, loss = 0.00076751\n",
      "Iteration 850, loss = 0.00076693\n",
      "Iteration 851, loss = 0.00076875\n",
      "Iteration 852, loss = 0.00076824\n",
      "Iteration 853, loss = 0.00077362\n",
      "Iteration 854, loss = 0.00076674\n",
      "Iteration 855, loss = 0.00076416\n",
      "Iteration 856, loss = 0.00076307\n",
      "Iteration 857, loss = 0.00076089\n",
      "Iteration 858, loss = 0.00075957\n",
      "Iteration 859, loss = 0.00075780\n",
      "Iteration 860, loss = 0.00075627\n",
      "Iteration 861, loss = 0.00076059\n",
      "Iteration 862, loss = 0.00076132\n",
      "Iteration 863, loss = 0.00075451\n",
      "Iteration 864, loss = 0.00075463\n",
      "Iteration 865, loss = 0.00075487\n",
      "Iteration 866, loss = 0.00075170\n",
      "Iteration 867, loss = 0.00075157\n",
      "Iteration 868, loss = 0.00074917\n",
      "Iteration 869, loss = 0.00074835\n",
      "Iteration 870, loss = 0.00075091\n",
      "Iteration 871, loss = 0.00074795\n",
      "Iteration 872, loss = 0.00074993\n",
      "Iteration 873, loss = 0.00074909\n",
      "Iteration 874, loss = 0.00075373\n",
      "Iteration 875, loss = 0.00074951\n",
      "Iteration 876, loss = 0.00074351\n",
      "Iteration 877, loss = 0.00074401\n",
      "Iteration 878, loss = 0.00074892\n",
      "Iteration 879, loss = 0.00074172\n",
      "Iteration 880, loss = 0.00073955\n",
      "Iteration 881, loss = 0.00073842\n",
      "Iteration 882, loss = 0.00073598\n",
      "Iteration 883, loss = 0.00073749\n",
      "Iteration 884, loss = 0.00073760\n",
      "Iteration 885, loss = 0.00073605\n",
      "Iteration 886, loss = 0.00074091\n",
      "Iteration 887, loss = 0.00074131\n",
      "Iteration 888, loss = 0.00073217\n",
      "Iteration 889, loss = 0.00073132\n",
      "Iteration 890, loss = 0.00072922\n",
      "Iteration 891, loss = 0.00072794\n",
      "Iteration 892, loss = 0.00073039\n",
      "Iteration 893, loss = 0.00073733\n",
      "Iteration 894, loss = 0.00072950\n",
      "Iteration 895, loss = 0.00072630\n",
      "Iteration 896, loss = 0.00072313\n",
      "Iteration 897, loss = 0.00072321\n",
      "Iteration 898, loss = 0.00072196\n",
      "Iteration 899, loss = 0.00072348\n",
      "Iteration 900, loss = 0.00072186\n",
      "Iteration 901, loss = 0.00071871\n",
      "Iteration 902, loss = 0.00071883\n",
      "Iteration 903, loss = 0.00071864\n",
      "Iteration 904, loss = 0.00071727\n",
      "Iteration 905, loss = 0.00071956\n",
      "Iteration 906, loss = 0.00072297\n",
      "Iteration 907, loss = 0.00071838\n",
      "Iteration 908, loss = 0.00071570\n",
      "Iteration 909, loss = 0.00071572\n",
      "Iteration 910, loss = 0.00071473\n",
      "Iteration 911, loss = 0.00071303\n",
      "Iteration 912, loss = 0.00071313\n",
      "Iteration 913, loss = 0.00071147\n",
      "Iteration 914, loss = 0.00071236\n",
      "Iteration 915, loss = 0.00070909\n",
      "Iteration 916, loss = 0.00070828\n",
      "Iteration 917, loss = 0.00070905\n",
      "Iteration 918, loss = 0.00071034\n",
      "Iteration 919, loss = 0.00071437\n",
      "Iteration 920, loss = 0.00071168\n",
      "Iteration 921, loss = 0.00071234\n",
      "Iteration 922, loss = 0.00070831\n",
      "Iteration 923, loss = 0.00070586\n",
      "Iteration 924, loss = 0.00070809\n",
      "Iteration 925, loss = 0.00070425\n",
      "Iteration 926, loss = 0.00070236\n",
      "Iteration 927, loss = 0.00070387\n",
      "Iteration 928, loss = 0.00070122\n",
      "Iteration 929, loss = 0.00070038\n",
      "Iteration 930, loss = 0.00070076\n",
      "Iteration 931, loss = 0.00071104\n",
      "Iteration 932, loss = 0.00069726\n",
      "Iteration 933, loss = 0.00069869\n",
      "Iteration 934, loss = 0.00069845\n",
      "Iteration 935, loss = 0.00069386\n",
      "Iteration 936, loss = 0.00069361\n",
      "Iteration 937, loss = 0.00069686\n",
      "Iteration 938, loss = 0.00069336\n",
      "Iteration 939, loss = 0.00069287\n",
      "Iteration 940, loss = 0.00069122\n",
      "Iteration 941, loss = 0.00068917\n",
      "Iteration 942, loss = 0.00068942\n",
      "Iteration 943, loss = 0.00069586\n",
      "Iteration 944, loss = 0.00069299\n",
      "Iteration 945, loss = 0.00068933\n",
      "Iteration 946, loss = 0.00068668\n",
      "Iteration 947, loss = 0.00069164\n",
      "Iteration 948, loss = 0.00069609\n",
      "Iteration 949, loss = 0.00068670\n",
      "Iteration 950, loss = 0.00068477\n",
      "Iteration 951, loss = 0.00068293\n",
      "Iteration 952, loss = 0.00068713\n",
      "Iteration 953, loss = 0.00068320\n",
      "Iteration 954, loss = 0.00068225\n",
      "Iteration 955, loss = 0.00068046\n",
      "Iteration 956, loss = 0.00068148\n",
      "Iteration 957, loss = 0.00068195\n",
      "Iteration 958, loss = 0.00068060\n",
      "Iteration 959, loss = 0.00067911\n",
      "Iteration 960, loss = 0.00067952\n",
      "Iteration 961, loss = 0.00067781\n",
      "Iteration 962, loss = 0.00067785\n",
      "Iteration 963, loss = 0.00068244\n",
      "Iteration 964, loss = 0.00068091\n",
      "Iteration 965, loss = 0.00067644\n",
      "Iteration 966, loss = 0.00067369\n",
      "Iteration 967, loss = 0.00067646\n",
      "Iteration 968, loss = 0.00067586\n",
      "Iteration 969, loss = 0.00067555\n",
      "Iteration 970, loss = 0.00067576\n",
      "Iteration 971, loss = 0.00067725\n",
      "Iteration 972, loss = 0.00067426\n",
      "Iteration 973, loss = 0.00067689\n",
      "Iteration 974, loss = 0.00067534\n",
      "Iteration 975, loss = 0.00067244\n",
      "Iteration 976, loss = 0.00067836\n",
      "Iteration 977, loss = 0.00069192\n",
      "Iteration 978, loss = 0.00070082\n",
      "Iteration 979, loss = 0.00070159\n",
      "Iteration 980, loss = 0.00068318\n",
      "Iteration 981, loss = 0.00067140\n",
      "Iteration 982, loss = 0.00066713\n",
      "Iteration 983, loss = 0.00066467\n",
      "Iteration 984, loss = 0.00066390\n",
      "Iteration 985, loss = 0.00066449\n",
      "Iteration 986, loss = 0.00066141\n",
      "Iteration 987, loss = 0.00066305\n",
      "Iteration 988, loss = 0.00066569\n",
      "Iteration 989, loss = 0.00066364\n",
      "Iteration 990, loss = 0.00066696\n",
      "Iteration 991, loss = 0.00066629\n",
      "Iteration 992, loss = 0.00066573\n",
      "Iteration 993, loss = 0.00066346\n",
      "Iteration 994, loss = 0.00066140\n",
      "Iteration 995, loss = 0.00066291\n",
      "Iteration 996, loss = 0.00066427\n",
      "Iteration 997, loss = 0.00065910\n",
      "Iteration 998, loss = 0.00066197\n",
      "Iteration 999, loss = 0.00065621\n",
      "Iteration 1000, loss = 0.00065788\n",
      "Iteration 1001, loss = 0.00065609\n",
      "Iteration 1002, loss = 0.00065587\n",
      "Iteration 1003, loss = 0.00065429\n",
      "Iteration 1004, loss = 0.00065604\n",
      "Iteration 1005, loss = 0.00065274\n",
      "Iteration 1006, loss = 0.00065225\n",
      "Iteration 1007, loss = 0.00065281\n",
      "Iteration 1008, loss = 0.00065205\n",
      "Iteration 1009, loss = 0.00065328\n",
      "Iteration 1010, loss = 0.00065596\n",
      "Iteration 1011, loss = 0.00065561\n",
      "Iteration 1012, loss = 0.00065434\n",
      "Iteration 1013, loss = 0.00065322\n",
      "Iteration 1014, loss = 0.00065481\n",
      "Iteration 1015, loss = 0.00065337\n",
      "Iteration 1016, loss = 0.00065700\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.29297777\n",
      "Iteration 2, loss = 0.23509245\n",
      "Iteration 3, loss = 0.21059692\n",
      "Iteration 4, loss = 0.19662423\n",
      "Iteration 5, loss = 0.18585898\n",
      "Iteration 6, loss = 0.17870526\n",
      "Iteration 7, loss = 0.17276696\n",
      "Iteration 8, loss = 0.16860946\n",
      "Iteration 9, loss = 0.16415016\n",
      "Iteration 10, loss = 0.16075900\n",
      "Iteration 11, loss = 0.15746905\n",
      "Iteration 12, loss = 0.15556442\n",
      "Iteration 13, loss = 0.15330177\n",
      "Iteration 14, loss = 0.15118819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15, loss = 0.14917321\n",
      "Iteration 16, loss = 0.14748215\n",
      "Iteration 17, loss = 0.14615543\n",
      "Iteration 18, loss = 0.14518947\n",
      "Iteration 19, loss = 0.14263787\n",
      "Iteration 20, loss = 0.14169727\n",
      "Iteration 21, loss = 0.13984941\n",
      "Iteration 22, loss = 0.13876908\n",
      "Iteration 23, loss = 0.13687758\n",
      "Iteration 24, loss = 0.13551036\n",
      "Iteration 25, loss = 0.13422434\n",
      "Iteration 26, loss = 0.13398576\n",
      "Iteration 27, loss = 0.13077629\n",
      "Iteration 28, loss = 0.13077887\n",
      "Iteration 29, loss = 0.12868185\n",
      "Iteration 30, loss = 0.12758821\n",
      "Iteration 31, loss = 0.12599436\n",
      "Iteration 32, loss = 0.12507060\n",
      "Iteration 33, loss = 0.12397806\n",
      "Iteration 34, loss = 0.12208397\n",
      "Iteration 35, loss = 0.12147248\n",
      "Iteration 36, loss = 0.12021349\n",
      "Iteration 37, loss = 0.11944404\n",
      "Iteration 38, loss = 0.11839392\n",
      "Iteration 39, loss = 0.11722339\n",
      "Iteration 40, loss = 0.11548516\n",
      "Iteration 41, loss = 0.11442792\n",
      "Iteration 42, loss = 0.11331870\n",
      "Iteration 43, loss = 0.11373549\n",
      "Iteration 44, loss = 0.11334843\n",
      "Iteration 45, loss = 0.11075633\n",
      "Iteration 46, loss = 0.10882453\n",
      "Iteration 47, loss = 0.10793422\n",
      "Iteration 48, loss = 0.10656661\n",
      "Iteration 49, loss = 0.10574685\n",
      "Iteration 50, loss = 0.10493913\n",
      "Iteration 51, loss = 0.10374546\n",
      "Iteration 52, loss = 0.10295290\n",
      "Iteration 53, loss = 0.10137564\n",
      "Iteration 54, loss = 0.10048816\n",
      "Iteration 55, loss = 0.10037143\n",
      "Iteration 56, loss = 0.09887546\n",
      "Iteration 57, loss = 0.09926317\n",
      "Iteration 58, loss = 0.09749259\n",
      "Iteration 59, loss = 0.09659075\n",
      "Iteration 60, loss = 0.09704403\n",
      "Iteration 61, loss = 0.09420837\n",
      "Iteration 62, loss = 0.09500321\n",
      "Iteration 63, loss = 0.09253566\n",
      "Iteration 64, loss = 0.09100159\n",
      "Iteration 65, loss = 0.08991946\n",
      "Iteration 66, loss = 0.08974383\n",
      "Iteration 67, loss = 0.08909711\n",
      "Iteration 68, loss = 0.08930243\n",
      "Iteration 69, loss = 0.08678192\n",
      "Iteration 70, loss = 0.08512087\n",
      "Iteration 71, loss = 0.08488385\n",
      "Iteration 72, loss = 0.08386620\n",
      "Iteration 73, loss = 0.08387630\n",
      "Iteration 74, loss = 0.08278269\n",
      "Iteration 75, loss = 0.08131585\n",
      "Iteration 76, loss = 0.08032300\n",
      "Iteration 77, loss = 0.07983361\n",
      "Iteration 78, loss = 0.07909654\n",
      "Iteration 79, loss = 0.07867315\n",
      "Iteration 80, loss = 0.07785081\n",
      "Iteration 81, loss = 0.07646293\n",
      "Iteration 82, loss = 0.07537725\n",
      "Iteration 83, loss = 0.07472267\n",
      "Iteration 84, loss = 0.07433882\n",
      "Iteration 85, loss = 0.07347015\n",
      "Iteration 86, loss = 0.07385940\n",
      "Iteration 87, loss = 0.07162831\n",
      "Iteration 88, loss = 0.07107496\n",
      "Iteration 89, loss = 0.07021558\n",
      "Iteration 90, loss = 0.06962198\n",
      "Iteration 91, loss = 0.06931626\n",
      "Iteration 92, loss = 0.06901501\n",
      "Iteration 93, loss = 0.06813537\n",
      "Iteration 94, loss = 0.06713630\n",
      "Iteration 95, loss = 0.06581664\n",
      "Iteration 96, loss = 0.06548802\n",
      "Iteration 97, loss = 0.06472154\n",
      "Iteration 98, loss = 0.06482539\n",
      "Iteration 99, loss = 0.06448550\n",
      "Iteration 100, loss = 0.06261077\n",
      "Iteration 101, loss = 0.06117302\n",
      "Iteration 102, loss = 0.06147837\n",
      "Iteration 103, loss = 0.06037809\n",
      "Iteration 104, loss = 0.06003579\n",
      "Iteration 105, loss = 0.06033007\n",
      "Iteration 106, loss = 0.05826958\n",
      "Iteration 107, loss = 0.05793650\n",
      "Iteration 108, loss = 0.05738149\n",
      "Iteration 109, loss = 0.05683282\n",
      "Iteration 110, loss = 0.05550136\n",
      "Iteration 111, loss = 0.05617226\n",
      "Iteration 112, loss = 0.05521139\n",
      "Iteration 113, loss = 0.05443231\n",
      "Iteration 114, loss = 0.05354293\n",
      "Iteration 115, loss = 0.05291052\n",
      "Iteration 116, loss = 0.05244238\n",
      "Iteration 117, loss = 0.05218268\n",
      "Iteration 118, loss = 0.05161732\n",
      "Iteration 119, loss = 0.05102888\n",
      "Iteration 120, loss = 0.04974523\n",
      "Iteration 121, loss = 0.04903663\n",
      "Iteration 122, loss = 0.04864083\n",
      "Iteration 123, loss = 0.04765863\n",
      "Iteration 124, loss = 0.04786180\n",
      "Iteration 125, loss = 0.04659670\n",
      "Iteration 126, loss = 0.04651930\n",
      "Iteration 127, loss = 0.04567390\n",
      "Iteration 128, loss = 0.04491851\n",
      "Iteration 129, loss = 0.04454121\n",
      "Iteration 130, loss = 0.04456666\n",
      "Iteration 131, loss = 0.04473457\n",
      "Iteration 132, loss = 0.04302360\n",
      "Iteration 133, loss = 0.04259831\n",
      "Iteration 134, loss = 0.04245921\n",
      "Iteration 135, loss = 0.04188036\n",
      "Iteration 136, loss = 0.04136528\n",
      "Iteration 137, loss = 0.04050392\n",
      "Iteration 138, loss = 0.04036722\n",
      "Iteration 139, loss = 0.03919116\n",
      "Iteration 140, loss = 0.03890802\n",
      "Iteration 141, loss = 0.03923012\n",
      "Iteration 142, loss = 0.03916374\n",
      "Iteration 143, loss = 0.03717813\n",
      "Iteration 144, loss = 0.03676390\n",
      "Iteration 145, loss = 0.03690649\n",
      "Iteration 146, loss = 0.03604328\n",
      "Iteration 147, loss = 0.03613738\n",
      "Iteration 148, loss = 0.03515570\n",
      "Iteration 149, loss = 0.03593799\n",
      "Iteration 150, loss = 0.03565578\n",
      "Iteration 151, loss = 0.03417804\n",
      "Iteration 152, loss = 0.03381797\n",
      "Iteration 153, loss = 0.03303379\n",
      "Iteration 154, loss = 0.03259089\n",
      "Iteration 155, loss = 0.03226876\n",
      "Iteration 156, loss = 0.03295968\n",
      "Iteration 157, loss = 0.03239533\n",
      "Iteration 158, loss = 0.03131950\n",
      "Iteration 159, loss = 0.03071199\n",
      "Iteration 160, loss = 0.03073423\n",
      "Iteration 161, loss = 0.03060336\n",
      "Iteration 162, loss = 0.03124632\n",
      "Iteration 163, loss = 0.03048283\n",
      "Iteration 164, loss = 0.02925132\n",
      "Iteration 165, loss = 0.02858651\n",
      "Iteration 166, loss = 0.02835887\n",
      "Iteration 167, loss = 0.02725361\n",
      "Iteration 168, loss = 0.02721475\n",
      "Iteration 169, loss = 0.02704343\n",
      "Iteration 170, loss = 0.02654045\n",
      "Iteration 171, loss = 0.02692166\n",
      "Iteration 172, loss = 0.02556068\n",
      "Iteration 173, loss = 0.02523428\n",
      "Iteration 174, loss = 0.02523584\n",
      "Iteration 175, loss = 0.02458382\n",
      "Iteration 176, loss = 0.02445786\n",
      "Iteration 177, loss = 0.02412653\n",
      "Iteration 178, loss = 0.02418073\n",
      "Iteration 179, loss = 0.02359496\n",
      "Iteration 180, loss = 0.02306726\n",
      "Iteration 181, loss = 0.02291911\n",
      "Iteration 182, loss = 0.02337065\n",
      "Iteration 183, loss = 0.02333399\n",
      "Iteration 184, loss = 0.02286033\n",
      "Iteration 185, loss = 0.02171659\n",
      "Iteration 186, loss = 0.02145829\n",
      "Iteration 187, loss = 0.02119564\n",
      "Iteration 188, loss = 0.02075608\n",
      "Iteration 189, loss = 0.02074609\n",
      "Iteration 190, loss = 0.02019701\n",
      "Iteration 191, loss = 0.02048998\n",
      "Iteration 192, loss = 0.01974608\n",
      "Iteration 193, loss = 0.01980699\n",
      "Iteration 194, loss = 0.01928763\n",
      "Iteration 195, loss = 0.01941512\n",
      "Iteration 196, loss = 0.01899300\n",
      "Iteration 197, loss = 0.01862659\n",
      "Iteration 198, loss = 0.01817972\n",
      "Iteration 199, loss = 0.01835451\n",
      "Iteration 200, loss = 0.01785851\n",
      "Iteration 201, loss = 0.01763762\n",
      "Iteration 202, loss = 0.01717093\n",
      "Iteration 203, loss = 0.01711525\n",
      "Iteration 204, loss = 0.01690947\n",
      "Iteration 205, loss = 0.01650392\n",
      "Iteration 206, loss = 0.01665293\n",
      "Iteration 207, loss = 0.01628597\n",
      "Iteration 208, loss = 0.01588319\n",
      "Iteration 209, loss = 0.01571696\n",
      "Iteration 210, loss = 0.01545461\n",
      "Iteration 211, loss = 0.01520726\n",
      "Iteration 212, loss = 0.01505518\n",
      "Iteration 213, loss = 0.01527729\n",
      "Iteration 214, loss = 0.01503314\n",
      "Iteration 215, loss = 0.01463882\n",
      "Iteration 216, loss = 0.01481948\n",
      "Iteration 217, loss = 0.01420443\n",
      "Iteration 218, loss = 0.01392711\n",
      "Iteration 219, loss = 0.01384541\n",
      "Iteration 220, loss = 0.01383058\n",
      "Iteration 221, loss = 0.01342884\n",
      "Iteration 222, loss = 0.01330343\n",
      "Iteration 223, loss = 0.01311402\n",
      "Iteration 224, loss = 0.01307367\n",
      "Iteration 225, loss = 0.01283309\n",
      "Iteration 226, loss = 0.01271237\n",
      "Iteration 227, loss = 0.01245181\n",
      "Iteration 228, loss = 0.01240080\n",
      "Iteration 229, loss = 0.01237413\n",
      "Iteration 230, loss = 0.01194847\n",
      "Iteration 231, loss = 0.01201736\n",
      "Iteration 232, loss = 0.01171950\n",
      "Iteration 233, loss = 0.01150058\n",
      "Iteration 234, loss = 0.01139978\n",
      "Iteration 235, loss = 0.01129559\n",
      "Iteration 236, loss = 0.01111645\n",
      "Iteration 237, loss = 0.01101947\n",
      "Iteration 238, loss = 0.01078410\n",
      "Iteration 239, loss = 0.01064721\n",
      "Iteration 240, loss = 0.01057619\n",
      "Iteration 241, loss = 0.01047395\n",
      "Iteration 242, loss = 0.01052568\n",
      "Iteration 243, loss = 0.01017488\n",
      "Iteration 244, loss = 0.01014449\n",
      "Iteration 245, loss = 0.00996285\n",
      "Iteration 246, loss = 0.00988693\n",
      "Iteration 247, loss = 0.00966983\n",
      "Iteration 248, loss = 0.00949820\n",
      "Iteration 249, loss = 0.00941052\n",
      "Iteration 250, loss = 0.00932185\n",
      "Iteration 251, loss = 0.00945400\n",
      "Iteration 252, loss = 0.00950358\n",
      "Iteration 253, loss = 0.00904709\n",
      "Iteration 254, loss = 0.00916488\n",
      "Iteration 255, loss = 0.00922502\n",
      "Iteration 256, loss = 0.00881238\n",
      "Iteration 257, loss = 0.00857617\n",
      "Iteration 258, loss = 0.00857097\n",
      "Iteration 259, loss = 0.00848662\n",
      "Iteration 260, loss = 0.00852612\n",
      "Iteration 261, loss = 0.00848064\n",
      "Iteration 262, loss = 0.00811539\n",
      "Iteration 263, loss = 0.00799293\n",
      "Iteration 264, loss = 0.00798567\n",
      "Iteration 265, loss = 0.00783609\n",
      "Iteration 266, loss = 0.00772896\n",
      "Iteration 267, loss = 0.00768161\n",
      "Iteration 268, loss = 0.00763406\n",
      "Iteration 269, loss = 0.00755750\n",
      "Iteration 270, loss = 0.00738955\n",
      "Iteration 271, loss = 0.00728302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 272, loss = 0.00731920\n",
      "Iteration 273, loss = 0.00724004\n",
      "Iteration 274, loss = 0.00707182\n",
      "Iteration 275, loss = 0.00694180\n",
      "Iteration 276, loss = 0.00689216\n",
      "Iteration 277, loss = 0.00679389\n",
      "Iteration 278, loss = 0.00675870\n",
      "Iteration 279, loss = 0.00674627\n",
      "Iteration 280, loss = 0.00664679\n",
      "Iteration 281, loss = 0.00674982\n",
      "Iteration 282, loss = 0.00653881\n",
      "Iteration 283, loss = 0.00653018\n",
      "Iteration 284, loss = 0.00636736\n",
      "Iteration 285, loss = 0.00623216\n",
      "Iteration 286, loss = 0.00623075\n",
      "Iteration 287, loss = 0.00614420\n",
      "Iteration 288, loss = 0.00609679\n",
      "Iteration 289, loss = 0.00609548\n",
      "Iteration 290, loss = 0.00591487\n",
      "Iteration 291, loss = 0.00588222\n",
      "Iteration 292, loss = 0.00586436\n",
      "Iteration 293, loss = 0.00597485\n",
      "Iteration 294, loss = 0.00578485\n",
      "Iteration 295, loss = 0.00562609\n",
      "Iteration 296, loss = 0.00562624\n",
      "Iteration 297, loss = 0.00556446\n",
      "Iteration 298, loss = 0.00555680\n",
      "Iteration 299, loss = 0.00543004\n",
      "Iteration 300, loss = 0.00536370\n",
      "Iteration 301, loss = 0.00527992\n",
      "Iteration 302, loss = 0.00527751\n",
      "Iteration 303, loss = 0.00519669\n",
      "Iteration 304, loss = 0.00517751\n",
      "Iteration 305, loss = 0.00509034\n",
      "Iteration 306, loss = 0.00509740\n",
      "Iteration 307, loss = 0.00496686\n",
      "Iteration 308, loss = 0.00496679\n",
      "Iteration 309, loss = 0.00491410\n",
      "Iteration 310, loss = 0.00488565\n",
      "Iteration 311, loss = 0.00478813\n",
      "Iteration 312, loss = 0.00474275\n",
      "Iteration 313, loss = 0.00471137\n",
      "Iteration 314, loss = 0.00472978\n",
      "Iteration 315, loss = 0.00462179\n",
      "Iteration 316, loss = 0.00455791\n",
      "Iteration 317, loss = 0.00462836\n",
      "Iteration 318, loss = 0.00460576\n",
      "Iteration 319, loss = 0.00449345\n",
      "Iteration 320, loss = 0.00440704\n",
      "Iteration 321, loss = 0.00436624\n",
      "Iteration 322, loss = 0.00432683\n",
      "Iteration 323, loss = 0.00429620\n",
      "Iteration 324, loss = 0.00427854\n",
      "Iteration 325, loss = 0.00428040\n",
      "Iteration 326, loss = 0.00425763\n",
      "Iteration 327, loss = 0.00412543\n",
      "Iteration 328, loss = 0.00407595\n",
      "Iteration 329, loss = 0.00403018\n",
      "Iteration 330, loss = 0.00401458\n",
      "Iteration 331, loss = 0.00399487\n",
      "Iteration 332, loss = 0.00394913\n",
      "Iteration 333, loss = 0.00387937\n",
      "Iteration 334, loss = 0.00387096\n",
      "Iteration 335, loss = 0.00387128\n",
      "Iteration 336, loss = 0.00384223\n",
      "Iteration 337, loss = 0.00382048\n",
      "Iteration 338, loss = 0.00380569\n",
      "Iteration 339, loss = 0.00373339\n",
      "Iteration 340, loss = 0.00367546\n",
      "Iteration 341, loss = 0.00365896\n",
      "Iteration 342, loss = 0.00362525\n",
      "Iteration 343, loss = 0.00360975\n",
      "Iteration 344, loss = 0.00363214\n",
      "Iteration 345, loss = 0.00352265\n",
      "Iteration 346, loss = 0.00350202\n",
      "Iteration 347, loss = 0.00345753\n",
      "Iteration 348, loss = 0.00345831\n",
      "Iteration 349, loss = 0.00345312\n",
      "Iteration 350, loss = 0.00338596\n",
      "Iteration 351, loss = 0.00335840\n",
      "Iteration 352, loss = 0.00334877\n",
      "Iteration 353, loss = 0.00330727\n",
      "Iteration 354, loss = 0.00328950\n",
      "Iteration 355, loss = 0.00328971\n",
      "Iteration 356, loss = 0.00328083\n",
      "Iteration 357, loss = 0.00318908\n",
      "Iteration 358, loss = 0.00318239\n",
      "Iteration 359, loss = 0.00314117\n",
      "Iteration 360, loss = 0.00313132\n",
      "Iteration 361, loss = 0.00309145\n",
      "Iteration 362, loss = 0.00308789\n",
      "Iteration 363, loss = 0.00305458\n",
      "Iteration 364, loss = 0.00302996\n",
      "Iteration 365, loss = 0.00306243\n",
      "Iteration 366, loss = 0.00299642\n",
      "Iteration 367, loss = 0.00297167\n",
      "Iteration 368, loss = 0.00293339\n",
      "Iteration 369, loss = 0.00291171\n",
      "Iteration 370, loss = 0.00295161\n",
      "Iteration 371, loss = 0.00297443\n",
      "Iteration 372, loss = 0.00292754\n",
      "Iteration 373, loss = 0.00288196\n",
      "Iteration 374, loss = 0.00280391\n",
      "Iteration 375, loss = 0.00278719\n",
      "Iteration 376, loss = 0.00282164\n",
      "Iteration 377, loss = 0.00280990\n",
      "Iteration 378, loss = 0.00276277\n",
      "Iteration 379, loss = 0.00270612\n",
      "Iteration 380, loss = 0.00268502\n",
      "Iteration 381, loss = 0.00267411\n",
      "Iteration 382, loss = 0.00266252\n",
      "Iteration 383, loss = 0.00264246\n",
      "Iteration 384, loss = 0.00263551\n",
      "Iteration 385, loss = 0.00260146\n",
      "Iteration 386, loss = 0.00259192\n",
      "Iteration 387, loss = 0.00257751\n",
      "Iteration 388, loss = 0.00256342\n",
      "Iteration 389, loss = 0.00253219\n",
      "Iteration 390, loss = 0.00252475\n",
      "Iteration 391, loss = 0.00250364\n",
      "Iteration 392, loss = 0.00247262\n",
      "Iteration 393, loss = 0.00246499\n",
      "Iteration 394, loss = 0.00244651\n",
      "Iteration 395, loss = 0.00242648\n",
      "Iteration 396, loss = 0.00241151\n",
      "Iteration 397, loss = 0.00240273\n",
      "Iteration 398, loss = 0.00240241\n",
      "Iteration 399, loss = 0.00240161\n",
      "Iteration 400, loss = 0.00236432\n",
      "Iteration 401, loss = 0.00234889\n",
      "Iteration 402, loss = 0.00235140\n",
      "Iteration 403, loss = 0.00232334\n",
      "Iteration 404, loss = 0.00229534\n",
      "Iteration 405, loss = 0.00227778\n",
      "Iteration 406, loss = 0.00226165\n",
      "Iteration 407, loss = 0.00226132\n",
      "Iteration 408, loss = 0.00223791\n",
      "Iteration 409, loss = 0.00222237\n",
      "Iteration 410, loss = 0.00221658\n",
      "Iteration 411, loss = 0.00220769\n",
      "Iteration 412, loss = 0.00219768\n",
      "Iteration 413, loss = 0.00216990\n",
      "Iteration 414, loss = 0.00216409\n",
      "Iteration 415, loss = 0.00215343\n",
      "Iteration 416, loss = 0.00213688\n",
      "Iteration 417, loss = 0.00212339\n",
      "Iteration 418, loss = 0.00211690\n",
      "Iteration 419, loss = 0.00212781\n",
      "Iteration 420, loss = 0.00208930\n",
      "Iteration 421, loss = 0.00208976\n",
      "Iteration 422, loss = 0.00207184\n",
      "Iteration 423, loss = 0.00205674\n",
      "Iteration 424, loss = 0.00207252\n",
      "Iteration 425, loss = 0.00204652\n",
      "Iteration 426, loss = 0.00204072\n",
      "Iteration 427, loss = 0.00201487\n",
      "Iteration 428, loss = 0.00200829\n",
      "Iteration 429, loss = 0.00200362\n",
      "Iteration 430, loss = 0.00199772\n",
      "Iteration 431, loss = 0.00198801\n",
      "Iteration 432, loss = 0.00195654\n",
      "Iteration 433, loss = 0.00194605\n",
      "Iteration 434, loss = 0.00193698\n",
      "Iteration 435, loss = 0.00192699\n",
      "Iteration 436, loss = 0.00192491\n",
      "Iteration 437, loss = 0.00190490\n",
      "Iteration 438, loss = 0.00190020\n",
      "Iteration 439, loss = 0.00188312\n",
      "Iteration 440, loss = 0.00187895\n",
      "Iteration 441, loss = 0.00186829\n",
      "Iteration 442, loss = 0.00186097\n",
      "Iteration 443, loss = 0.00187617\n",
      "Iteration 444, loss = 0.00185726\n",
      "Iteration 445, loss = 0.00182850\n",
      "Iteration 446, loss = 0.00182931\n",
      "Iteration 447, loss = 0.00181973\n",
      "Iteration 448, loss = 0.00180849\n",
      "Iteration 449, loss = 0.00180081\n",
      "Iteration 450, loss = 0.00178863\n",
      "Iteration 451, loss = 0.00178250\n",
      "Iteration 452, loss = 0.00176996\n",
      "Iteration 453, loss = 0.00177136\n",
      "Iteration 454, loss = 0.00175457\n",
      "Iteration 455, loss = 0.00174546\n",
      "Iteration 456, loss = 0.00174941\n",
      "Iteration 457, loss = 0.00172946\n",
      "Iteration 458, loss = 0.00173095\n",
      "Iteration 459, loss = 0.00171805\n",
      "Iteration 460, loss = 0.00170947\n",
      "Iteration 461, loss = 0.00170152\n",
      "Iteration 462, loss = 0.00169535\n",
      "Iteration 463, loss = 0.00168855\n",
      "Iteration 464, loss = 0.00167960\n",
      "Iteration 465, loss = 0.00166962\n",
      "Iteration 466, loss = 0.00166385\n",
      "Iteration 467, loss = 0.00166976\n",
      "Iteration 468, loss = 0.00166294\n",
      "Iteration 469, loss = 0.00164685\n",
      "Iteration 470, loss = 0.00164573\n",
      "Iteration 471, loss = 0.00163819\n",
      "Iteration 472, loss = 0.00163365\n",
      "Iteration 473, loss = 0.00161751\n",
      "Iteration 474, loss = 0.00160788\n",
      "Iteration 475, loss = 0.00160284\n",
      "Iteration 476, loss = 0.00160078\n",
      "Iteration 477, loss = 0.00159006\n",
      "Iteration 478, loss = 0.00158439\n",
      "Iteration 479, loss = 0.00157944\n",
      "Iteration 480, loss = 0.00157232\n",
      "Iteration 481, loss = 0.00156393\n",
      "Iteration 482, loss = 0.00156286\n",
      "Iteration 483, loss = 0.00154903\n",
      "Iteration 484, loss = 0.00154519\n",
      "Iteration 485, loss = 0.00153959\n",
      "Iteration 486, loss = 0.00153354\n",
      "Iteration 487, loss = 0.00153149\n",
      "Iteration 488, loss = 0.00153134\n",
      "Iteration 489, loss = 0.00152247\n",
      "Iteration 490, loss = 0.00152403\n",
      "Iteration 491, loss = 0.00150758\n",
      "Iteration 492, loss = 0.00150175\n",
      "Iteration 493, loss = 0.00150090\n",
      "Iteration 494, loss = 0.00149115\n",
      "Iteration 495, loss = 0.00148220\n",
      "Iteration 496, loss = 0.00148738\n",
      "Iteration 497, loss = 0.00147470\n",
      "Iteration 498, loss = 0.00147576\n",
      "Iteration 499, loss = 0.00147099\n",
      "Iteration 500, loss = 0.00145973\n",
      "Iteration 501, loss = 0.00146056\n",
      "Iteration 502, loss = 0.00145140\n",
      "Iteration 503, loss = 0.00144399\n",
      "Iteration 504, loss = 0.00143747\n",
      "Iteration 505, loss = 0.00143806\n",
      "Iteration 506, loss = 0.00142929\n",
      "Iteration 507, loss = 0.00142921\n",
      "Iteration 508, loss = 0.00142797\n",
      "Iteration 509, loss = 0.00142618\n",
      "Iteration 510, loss = 0.00141406\n",
      "Iteration 511, loss = 0.00141078\n",
      "Iteration 512, loss = 0.00140497\n",
      "Iteration 513, loss = 0.00141001\n",
      "Iteration 514, loss = 0.00139389\n",
      "Iteration 515, loss = 0.00139715\n",
      "Iteration 516, loss = 0.00138978\n",
      "Iteration 517, loss = 0.00140069\n",
      "Iteration 518, loss = 0.00138295\n",
      "Iteration 519, loss = 0.00138187\n",
      "Iteration 520, loss = 0.00137026\n",
      "Iteration 521, loss = 0.00136542\n",
      "Iteration 522, loss = 0.00136099\n",
      "Iteration 523, loss = 0.00135889\n",
      "Iteration 524, loss = 0.00135373\n",
      "Iteration 525, loss = 0.00135063\n",
      "Iteration 526, loss = 0.00134980\n",
      "Iteration 527, loss = 0.00134426\n",
      "Iteration 528, loss = 0.00134441\n",
      "Iteration 529, loss = 0.00133206\n",
      "Iteration 530, loss = 0.00133226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 531, loss = 0.00132639\n",
      "Iteration 532, loss = 0.00132169\n",
      "Iteration 533, loss = 0.00131988\n",
      "Iteration 534, loss = 0.00132076\n",
      "Iteration 535, loss = 0.00131019\n",
      "Iteration 536, loss = 0.00130903\n",
      "Iteration 537, loss = 0.00130152\n",
      "Iteration 538, loss = 0.00129730\n",
      "Iteration 539, loss = 0.00129679\n",
      "Iteration 540, loss = 0.00129329\n",
      "Iteration 541, loss = 0.00129117\n",
      "Iteration 542, loss = 0.00128900\n",
      "Iteration 543, loss = 0.00127971\n",
      "Iteration 544, loss = 0.00128126\n",
      "Iteration 545, loss = 0.00128077\n",
      "Iteration 546, loss = 0.00128195\n",
      "Iteration 547, loss = 0.00127516\n",
      "Iteration 548, loss = 0.00126339\n",
      "Iteration 549, loss = 0.00126114\n",
      "Iteration 550, loss = 0.00125914\n",
      "Iteration 551, loss = 0.00125585\n",
      "Iteration 552, loss = 0.00125219\n",
      "Iteration 553, loss = 0.00125450\n",
      "Iteration 554, loss = 0.00124914\n",
      "Iteration 555, loss = 0.00124431\n",
      "Iteration 556, loss = 0.00124143\n",
      "Iteration 557, loss = 0.00124171\n",
      "Iteration 558, loss = 0.00123574\n",
      "Iteration 559, loss = 0.00123240\n",
      "Iteration 560, loss = 0.00122848\n",
      "Iteration 561, loss = 0.00122638\n",
      "Iteration 562, loss = 0.00122368\n",
      "Iteration 563, loss = 0.00122081\n",
      "Iteration 564, loss = 0.00121637\n",
      "Iteration 565, loss = 0.00121617\n",
      "Iteration 566, loss = 0.00121046\n",
      "Iteration 567, loss = 0.00121427\n",
      "Iteration 568, loss = 0.00120880\n",
      "Iteration 569, loss = 0.00120568\n",
      "Iteration 570, loss = 0.00120212\n",
      "Iteration 571, loss = 0.00120316\n",
      "Iteration 572, loss = 0.00120016\n",
      "Iteration 573, loss = 0.00119579\n",
      "Iteration 574, loss = 0.00119391\n",
      "Iteration 575, loss = 0.00119724\n",
      "Iteration 576, loss = 0.00118490\n",
      "Iteration 577, loss = 0.00118449\n",
      "Iteration 578, loss = 0.00118093\n",
      "Iteration 579, loss = 0.00118316\n",
      "Iteration 580, loss = 0.00118063\n",
      "Iteration 581, loss = 0.00117775\n",
      "Iteration 582, loss = 0.00118139\n",
      "Iteration 583, loss = 0.00117325\n",
      "Iteration 584, loss = 0.00116973\n",
      "Iteration 585, loss = 0.00116623\n",
      "Iteration 586, loss = 0.00116099\n",
      "Iteration 587, loss = 0.00115848\n",
      "Iteration 588, loss = 0.00115586\n",
      "Iteration 589, loss = 0.00115396\n",
      "Iteration 590, loss = 0.00115093\n",
      "Iteration 591, loss = 0.00114885\n",
      "Iteration 592, loss = 0.00114792\n",
      "Iteration 593, loss = 0.00114495\n",
      "Iteration 594, loss = 0.00114481\n",
      "Iteration 595, loss = 0.00114280\n",
      "Iteration 596, loss = 0.00113755\n",
      "Iteration 597, loss = 0.00113417\n",
      "Iteration 598, loss = 0.00113260\n",
      "Iteration 599, loss = 0.00112985\n",
      "Iteration 600, loss = 0.00113221\n",
      "Iteration 601, loss = 0.00112724\n",
      "Iteration 602, loss = 0.00112533\n",
      "Iteration 603, loss = 0.00112302\n",
      "Iteration 604, loss = 0.00111961\n",
      "Iteration 605, loss = 0.00111925\n",
      "Iteration 606, loss = 0.00111535\n",
      "Iteration 607, loss = 0.00111695\n",
      "Iteration 608, loss = 0.00111112\n",
      "Iteration 609, loss = 0.00110870\n",
      "Iteration 610, loss = 0.00110910\n",
      "Iteration 611, loss = 0.00111136\n",
      "Iteration 612, loss = 0.00110467\n",
      "Iteration 613, loss = 0.00110262\n",
      "Iteration 614, loss = 0.00109825\n",
      "Iteration 615, loss = 0.00109857\n",
      "Iteration 616, loss = 0.00109545\n",
      "Iteration 617, loss = 0.00109394\n",
      "Iteration 618, loss = 0.00109486\n",
      "Iteration 619, loss = 0.00109112\n",
      "Iteration 620, loss = 0.00108950\n",
      "Iteration 621, loss = 0.00108589\n",
      "Iteration 622, loss = 0.00108462\n",
      "Iteration 623, loss = 0.00108393\n",
      "Iteration 624, loss = 0.00107943\n",
      "Iteration 625, loss = 0.00108092\n",
      "Iteration 626, loss = 0.00107823\n",
      "Iteration 627, loss = 0.00107734\n",
      "Iteration 628, loss = 0.00107284\n",
      "Iteration 629, loss = 0.00107098\n",
      "Iteration 630, loss = 0.00107310\n",
      "Iteration 631, loss = 0.00106867\n",
      "Iteration 632, loss = 0.00106645\n",
      "Iteration 633, loss = 0.00106553\n",
      "Iteration 634, loss = 0.00106340\n",
      "Iteration 635, loss = 0.00105994\n",
      "Iteration 636, loss = 0.00106050\n",
      "Iteration 637, loss = 0.00105647\n",
      "Iteration 638, loss = 0.00105670\n",
      "Iteration 639, loss = 0.00105421\n",
      "Iteration 640, loss = 0.00105305\n",
      "Iteration 641, loss = 0.00105805\n",
      "Iteration 642, loss = 0.00104915\n",
      "Iteration 643, loss = 0.00104721\n",
      "Iteration 644, loss = 0.00104556\n",
      "Iteration 645, loss = 0.00104301\n",
      "Iteration 646, loss = 0.00104349\n",
      "Iteration 647, loss = 0.00103997\n",
      "Iteration 648, loss = 0.00103852\n",
      "Iteration 649, loss = 0.00103660\n",
      "Iteration 650, loss = 0.00103406\n",
      "Iteration 651, loss = 0.00103490\n",
      "Iteration 652, loss = 0.00103439\n",
      "Iteration 653, loss = 0.00103034\n",
      "Iteration 654, loss = 0.00103110\n",
      "Iteration 655, loss = 0.00102809\n",
      "Iteration 656, loss = 0.00102794\n",
      "Iteration 657, loss = 0.00102436\n",
      "Iteration 658, loss = 0.00102441\n",
      "Iteration 659, loss = 0.00102212\n",
      "Iteration 660, loss = 0.00102151\n",
      "Iteration 661, loss = 0.00101832\n",
      "Iteration 662, loss = 0.00101623\n",
      "Iteration 663, loss = 0.00101416\n",
      "Iteration 664, loss = 0.00101348\n",
      "Iteration 665, loss = 0.00101201\n",
      "Iteration 666, loss = 0.00101160\n",
      "Iteration 667, loss = 0.00100822\n",
      "Iteration 668, loss = 0.00100680\n",
      "Iteration 669, loss = 0.00100771\n",
      "Iteration 670, loss = 0.00100420\n",
      "Iteration 671, loss = 0.00100237\n",
      "Iteration 672, loss = 0.00100128\n",
      "Iteration 673, loss = 0.00100065\n",
      "Iteration 674, loss = 0.00099667\n",
      "Iteration 675, loss = 0.00099615\n",
      "Iteration 676, loss = 0.00099452\n",
      "Iteration 677, loss = 0.00099261\n",
      "Iteration 678, loss = 0.00099133\n",
      "Iteration 679, loss = 0.00099002\n",
      "Iteration 680, loss = 0.00098977\n",
      "Iteration 681, loss = 0.00099456\n",
      "Iteration 682, loss = 0.00098943\n",
      "Iteration 683, loss = 0.00098740\n",
      "Iteration 684, loss = 0.00098449\n",
      "Iteration 685, loss = 0.00098189\n",
      "Iteration 686, loss = 0.00098617\n",
      "Iteration 687, loss = 0.00098198\n",
      "Iteration 688, loss = 0.00098236\n",
      "Iteration 689, loss = 0.00097670\n",
      "Iteration 690, loss = 0.00097602\n",
      "Iteration 691, loss = 0.00097495\n",
      "Iteration 692, loss = 0.00097666\n",
      "Iteration 693, loss = 0.00097381\n",
      "Iteration 694, loss = 0.00096895\n",
      "Iteration 695, loss = 0.00096936\n",
      "Iteration 696, loss = 0.00096683\n",
      "Iteration 697, loss = 0.00096598\n",
      "Iteration 698, loss = 0.00096236\n",
      "Iteration 699, loss = 0.00096192\n",
      "Iteration 700, loss = 0.00095996\n",
      "Iteration 701, loss = 0.00095981\n",
      "Iteration 702, loss = 0.00095812\n",
      "Iteration 703, loss = 0.00095503\n",
      "Iteration 704, loss = 0.00095710\n",
      "Iteration 705, loss = 0.00095436\n",
      "Iteration 706, loss = 0.00095253\n",
      "Iteration 707, loss = 0.00094956\n",
      "Iteration 708, loss = 0.00094841\n",
      "Iteration 709, loss = 0.00094832\n",
      "Iteration 710, loss = 0.00094746\n",
      "Iteration 711, loss = 0.00094602\n",
      "Iteration 712, loss = 0.00094486\n",
      "Iteration 713, loss = 0.00094198\n",
      "Iteration 714, loss = 0.00094198\n",
      "Iteration 715, loss = 0.00093838\n",
      "Iteration 716, loss = 0.00093841\n",
      "Iteration 717, loss = 0.00093803\n",
      "Iteration 718, loss = 0.00093612\n",
      "Iteration 719, loss = 0.00093484\n",
      "Iteration 720, loss = 0.00093406\n",
      "Iteration 721, loss = 0.00093149\n",
      "Iteration 722, loss = 0.00092966\n",
      "Iteration 723, loss = 0.00092817\n",
      "Iteration 724, loss = 0.00092997\n",
      "Iteration 725, loss = 0.00092812\n",
      "Iteration 726, loss = 0.00092696\n",
      "Iteration 727, loss = 0.00092474\n",
      "Iteration 728, loss = 0.00092656\n",
      "Iteration 729, loss = 0.00092617\n",
      "Iteration 730, loss = 0.00092270\n",
      "Iteration 731, loss = 0.00092028\n",
      "Iteration 732, loss = 0.00091826\n",
      "Iteration 733, loss = 0.00092214\n",
      "Iteration 734, loss = 0.00091777\n",
      "Iteration 735, loss = 0.00091408\n",
      "Iteration 736, loss = 0.00091362\n",
      "Iteration 737, loss = 0.00091178\n",
      "Iteration 738, loss = 0.00090958\n",
      "Iteration 739, loss = 0.00090875\n",
      "Iteration 740, loss = 0.00091043\n",
      "Iteration 741, loss = 0.00090749\n",
      "Iteration 742, loss = 0.00090626\n",
      "Iteration 743, loss = 0.00090752\n",
      "Iteration 744, loss = 0.00090433\n",
      "Iteration 745, loss = 0.00090272\n",
      "Iteration 746, loss = 0.00089871\n",
      "Iteration 747, loss = 0.00089656\n",
      "Iteration 748, loss = 0.00089567\n",
      "Iteration 749, loss = 0.00089559\n",
      "Iteration 750, loss = 0.00089363\n",
      "Iteration 751, loss = 0.00089603\n",
      "Iteration 752, loss = 0.00089247\n",
      "Iteration 753, loss = 0.00088954\n",
      "Iteration 754, loss = 0.00089010\n",
      "Iteration 755, loss = 0.00089106\n",
      "Iteration 756, loss = 0.00088999\n",
      "Iteration 757, loss = 0.00088554\n",
      "Iteration 758, loss = 0.00088394\n",
      "Iteration 759, loss = 0.00088147\n",
      "Iteration 760, loss = 0.00088094\n",
      "Iteration 761, loss = 0.00087925\n",
      "Iteration 762, loss = 0.00087810\n",
      "Iteration 763, loss = 0.00088000\n",
      "Iteration 764, loss = 0.00088052\n",
      "Iteration 765, loss = 0.00087528\n",
      "Iteration 766, loss = 0.00087631\n",
      "Iteration 767, loss = 0.00087396\n",
      "Iteration 768, loss = 0.00087561\n",
      "Iteration 769, loss = 0.00087867\n",
      "Iteration 770, loss = 0.00087070\n",
      "Iteration 771, loss = 0.00087228\n",
      "Iteration 772, loss = 0.00086975\n",
      "Iteration 773, loss = 0.00086856\n",
      "Iteration 774, loss = 0.00086456\n",
      "Iteration 775, loss = 0.00086403\n",
      "Iteration 776, loss = 0.00086608\n",
      "Iteration 777, loss = 0.00086318\n",
      "Iteration 778, loss = 0.00086018\n",
      "Iteration 779, loss = 0.00085975\n",
      "Iteration 780, loss = 0.00086006\n",
      "Iteration 781, loss = 0.00085734\n",
      "Iteration 782, loss = 0.00086564\n",
      "Iteration 783, loss = 0.00086560\n",
      "Iteration 784, loss = 0.00085924\n",
      "Iteration 785, loss = 0.00085605\n",
      "Iteration 786, loss = 0.00085560\n",
      "Iteration 787, loss = 0.00084967\n",
      "Iteration 788, loss = 0.00085184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 789, loss = 0.00084675\n",
      "Iteration 790, loss = 0.00084682\n",
      "Iteration 791, loss = 0.00084577\n",
      "Iteration 792, loss = 0.00084854\n",
      "Iteration 793, loss = 0.00084654\n",
      "Iteration 794, loss = 0.00084246\n",
      "Iteration 795, loss = 0.00084351\n",
      "Iteration 796, loss = 0.00084486\n",
      "Iteration 797, loss = 0.00084125\n",
      "Iteration 798, loss = 0.00084074\n",
      "Iteration 799, loss = 0.00084147\n",
      "Iteration 800, loss = 0.00084238\n",
      "Iteration 801, loss = 0.00084189\n",
      "Iteration 802, loss = 0.00083292\n",
      "Iteration 803, loss = 0.00083186\n",
      "Iteration 804, loss = 0.00082855\n",
      "Iteration 805, loss = 0.00082792\n",
      "Iteration 806, loss = 0.00082774\n",
      "Iteration 807, loss = 0.00082561\n",
      "Iteration 808, loss = 0.00082593\n",
      "Iteration 809, loss = 0.00082600\n",
      "Iteration 810, loss = 0.00082368\n",
      "Iteration 811, loss = 0.00082334\n",
      "Iteration 812, loss = 0.00082088\n",
      "Iteration 813, loss = 0.00082083\n",
      "Iteration 814, loss = 0.00081888\n",
      "Iteration 815, loss = 0.00081926\n",
      "Iteration 816, loss = 0.00081697\n",
      "Iteration 817, loss = 0.00081937\n",
      "Iteration 818, loss = 0.00081582\n",
      "Iteration 819, loss = 0.00081117\n",
      "Iteration 820, loss = 0.00081103\n",
      "Iteration 821, loss = 0.00081147\n",
      "Iteration 822, loss = 0.00080992\n",
      "Iteration 823, loss = 0.00081595\n",
      "Iteration 824, loss = 0.00080966\n",
      "Iteration 825, loss = 0.00081151\n",
      "Iteration 826, loss = 0.00080856\n",
      "Iteration 827, loss = 0.00080807\n",
      "Iteration 828, loss = 0.00080694\n",
      "Iteration 829, loss = 0.00080103\n",
      "Iteration 830, loss = 0.00079947\n",
      "Iteration 831, loss = 0.00079975\n",
      "Iteration 832, loss = 0.00080005\n",
      "Iteration 833, loss = 0.00079755\n",
      "Iteration 834, loss = 0.00079810\n",
      "Iteration 835, loss = 0.00079602\n",
      "Iteration 836, loss = 0.00079568\n",
      "Iteration 837, loss = 0.00079306\n",
      "Iteration 838, loss = 0.00079340\n",
      "Iteration 839, loss = 0.00079417\n",
      "Iteration 840, loss = 0.00078921\n",
      "Iteration 841, loss = 0.00078917\n",
      "Iteration 842, loss = 0.00079143\n",
      "Iteration 843, loss = 0.00078939\n",
      "Iteration 844, loss = 0.00079253\n",
      "Iteration 845, loss = 0.00078996\n",
      "Iteration 846, loss = 0.00079163\n",
      "Iteration 847, loss = 0.00078524\n",
      "Iteration 848, loss = 0.00078324\n",
      "Iteration 849, loss = 0.00078160\n",
      "Iteration 850, loss = 0.00078029\n",
      "Iteration 851, loss = 0.00077920\n",
      "Iteration 852, loss = 0.00078158\n",
      "Iteration 853, loss = 0.00077736\n",
      "Iteration 854, loss = 0.00077680\n",
      "Iteration 855, loss = 0.00077535\n",
      "Iteration 856, loss = 0.00077314\n",
      "Iteration 857, loss = 0.00078205\n",
      "Iteration 858, loss = 0.00077579\n",
      "Iteration 859, loss = 0.00077131\n",
      "Iteration 860, loss = 0.00077091\n",
      "Iteration 861, loss = 0.00077084\n",
      "Iteration 862, loss = 0.00076834\n",
      "Iteration 863, loss = 0.00076763\n",
      "Iteration 864, loss = 0.00076851\n",
      "Iteration 865, loss = 0.00076598\n",
      "Iteration 866, loss = 0.00077050\n",
      "Iteration 867, loss = 0.00076778\n",
      "Iteration 868, loss = 0.00077338\n",
      "Iteration 869, loss = 0.00076526\n",
      "Iteration 870, loss = 0.00076115\n",
      "Iteration 871, loss = 0.00076078\n",
      "Iteration 872, loss = 0.00076129\n",
      "Iteration 873, loss = 0.00076490\n",
      "Iteration 874, loss = 0.00076447\n",
      "Iteration 875, loss = 0.00076282\n",
      "Iteration 876, loss = 0.00075623\n",
      "Iteration 877, loss = 0.00075426\n",
      "Iteration 878, loss = 0.00075322\n",
      "Iteration 879, loss = 0.00075520\n",
      "Iteration 880, loss = 0.00075176\n",
      "Iteration 881, loss = 0.00074894\n",
      "Iteration 882, loss = 0.00075016\n",
      "Iteration 883, loss = 0.00074854\n",
      "Iteration 884, loss = 0.00074884\n",
      "Iteration 885, loss = 0.00074644\n",
      "Iteration 886, loss = 0.00074649\n",
      "Iteration 887, loss = 0.00074525\n",
      "Iteration 888, loss = 0.00074515\n",
      "Iteration 889, loss = 0.00074310\n",
      "Iteration 890, loss = 0.00074347\n",
      "Iteration 891, loss = 0.00074205\n",
      "Iteration 892, loss = 0.00074051\n",
      "Iteration 893, loss = 0.00074187\n",
      "Iteration 894, loss = 0.00074254\n",
      "Iteration 895, loss = 0.00074064\n",
      "Iteration 896, loss = 0.00073937\n",
      "Iteration 897, loss = 0.00073936\n",
      "Iteration 898, loss = 0.00074935\n",
      "Iteration 899, loss = 0.00074914\n",
      "Iteration 900, loss = 0.00073720\n",
      "Iteration 901, loss = 0.00073216\n",
      "Iteration 902, loss = 0.00073349\n",
      "Iteration 903, loss = 0.00073410\n",
      "Iteration 904, loss = 0.00073157\n",
      "Iteration 905, loss = 0.00073227\n",
      "Iteration 906, loss = 0.00073211\n",
      "Iteration 907, loss = 0.00073490\n",
      "Iteration 908, loss = 0.00073042\n",
      "Iteration 909, loss = 0.00072584\n",
      "Iteration 910, loss = 0.00072610\n",
      "Iteration 911, loss = 0.00072522\n",
      "Iteration 912, loss = 0.00072822\n",
      "Iteration 913, loss = 0.00072511\n",
      "Iteration 914, loss = 0.00072624\n",
      "Iteration 915, loss = 0.00072306\n",
      "Iteration 916, loss = 0.00073072\n",
      "Iteration 917, loss = 0.00072229\n",
      "Iteration 918, loss = 0.00072303\n",
      "Iteration 919, loss = 0.00072192\n",
      "Iteration 920, loss = 0.00073267\n",
      "Iteration 921, loss = 0.00072842\n",
      "Iteration 922, loss = 0.00071894\n",
      "Iteration 923, loss = 0.00071762\n",
      "Iteration 924, loss = 0.00071608\n",
      "Iteration 925, loss = 0.00071984\n",
      "Iteration 926, loss = 0.00071714\n",
      "Iteration 927, loss = 0.00071802\n",
      "Iteration 928, loss = 0.00071854\n",
      "Iteration 929, loss = 0.00071707\n",
      "Iteration 930, loss = 0.00071592\n",
      "Iteration 931, loss = 0.00071827\n",
      "Iteration 932, loss = 0.00071732\n",
      "Iteration 933, loss = 0.00071436\n",
      "Iteration 934, loss = 0.00071024\n",
      "Iteration 935, loss = 0.00070960\n",
      "Iteration 936, loss = 0.00070724\n",
      "Iteration 937, loss = 0.00070706\n",
      "Iteration 938, loss = 0.00070565\n",
      "Iteration 939, loss = 0.00070615\n",
      "Iteration 940, loss = 0.00070758\n",
      "Iteration 941, loss = 0.00070562\n",
      "Iteration 942, loss = 0.00070637\n",
      "Iteration 943, loss = 0.00070232\n",
      "Iteration 944, loss = 0.00070547\n",
      "Iteration 945, loss = 0.00070695\n",
      "Iteration 946, loss = 0.00070489\n",
      "Iteration 947, loss = 0.00070791\n",
      "Iteration 948, loss = 0.00070166\n",
      "Iteration 949, loss = 0.00070172\n",
      "Iteration 950, loss = 0.00069786\n",
      "Iteration 951, loss = 0.00069770\n",
      "Iteration 952, loss = 0.00070791\n",
      "Iteration 953, loss = 0.00069484\n",
      "Iteration 954, loss = 0.00070571\n",
      "Iteration 955, loss = 0.00069775\n",
      "Iteration 956, loss = 0.00069390\n",
      "Iteration 957, loss = 0.00069229\n",
      "Iteration 958, loss = 0.00069263\n",
      "Iteration 959, loss = 0.00069497\n",
      "Iteration 960, loss = 0.00069747\n",
      "Iteration 961, loss = 0.00322851\n",
      "Iteration 962, loss = 0.01742060\n",
      "Iteration 963, loss = 0.01391804\n",
      "Iteration 964, loss = 0.00318307\n",
      "Iteration 965, loss = 0.00125293\n",
      "Iteration 966, loss = 0.00096532\n",
      "Iteration 967, loss = 0.00086297\n",
      "Iteration 968, loss = 0.00081868\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.60453387\n",
      "Iteration 2, loss = 0.36481617\n",
      "Iteration 3, loss = 0.23732411\n",
      "Iteration 4, loss = 0.15875112\n",
      "Iteration 5, loss = 0.11027211\n",
      "Iteration 6, loss = 0.07962010\n",
      "Iteration 7, loss = 0.06002190\n",
      "Iteration 8, loss = 0.04699878\n",
      "Iteration 9, loss = 0.03775540\n",
      "Iteration 10, loss = 0.03116997\n",
      "Iteration 11, loss = 0.02613771\n",
      "Iteration 12, loss = 0.02225587\n",
      "Iteration 13, loss = 0.01923067\n",
      "Iteration 14, loss = 0.01677029\n",
      "Iteration 15, loss = 0.01478204\n",
      "Iteration 16, loss = 0.01313590\n",
      "Iteration 17, loss = 0.01175197\n",
      "Iteration 18, loss = 0.01062700\n",
      "Iteration 19, loss = 0.00960651\n",
      "Iteration 20, loss = 0.00878077\n",
      "Iteration 21, loss = 0.00801614\n",
      "Iteration 22, loss = 0.00739638\n",
      "Iteration 23, loss = 0.00681509\n",
      "Iteration 24, loss = 0.00632836\n",
      "Iteration 25, loss = 0.00587069\n",
      "Iteration 26, loss = 0.00548200\n",
      "Iteration 27, loss = 0.00512434\n",
      "Iteration 28, loss = 0.00481356\n",
      "Iteration 29, loss = 0.00453088\n",
      "Iteration 30, loss = 0.00426923\n",
      "Iteration 31, loss = 0.00402237\n",
      "Iteration 32, loss = 0.00380371\n",
      "Iteration 33, loss = 0.00360484\n",
      "Iteration 34, loss = 0.00341860\n",
      "Iteration 35, loss = 0.00325454\n",
      "Iteration 36, loss = 0.00310396\n",
      "Iteration 37, loss = 0.00295833\n",
      "Iteration 38, loss = 0.00284086\n",
      "Iteration 39, loss = 0.00269646\n",
      "Iteration 40, loss = 0.00258130\n",
      "Iteration 41, loss = 0.00247379\n",
      "Iteration 42, loss = 0.00237555\n",
      "Iteration 43, loss = 0.00227719\n",
      "Iteration 44, loss = 0.00218877\n",
      "Iteration 45, loss = 0.00210805\n",
      "Iteration 46, loss = 0.00202952\n",
      "Iteration 47, loss = 0.00195591\n",
      "Iteration 48, loss = 0.00188857\n",
      "Iteration 49, loss = 0.00182348\n",
      "Iteration 50, loss = 0.00176929\n",
      "Iteration 51, loss = 0.00171133\n",
      "Iteration 52, loss = 0.00165043\n",
      "Iteration 53, loss = 0.00159706\n",
      "Iteration 54, loss = 0.00154805\n",
      "Iteration 55, loss = 0.00150242\n",
      "Iteration 56, loss = 0.00145707\n",
      "Iteration 57, loss = 0.00141611\n",
      "Iteration 58, loss = 0.00137585\n",
      "Iteration 59, loss = 0.00133857\n",
      "Iteration 60, loss = 0.00130133\n",
      "Iteration 61, loss = 0.00126667\n",
      "Iteration 62, loss = 0.00123427\n",
      "Iteration 63, loss = 0.00120205\n",
      "Iteration 64, loss = 0.00117217\n",
      "Iteration 65, loss = 0.00114249\n",
      "Iteration 66, loss = 0.00112049\n",
      "Iteration 67, loss = 0.00109163\n",
      "Iteration 68, loss = 0.00106336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 69, loss = 0.00103952\n",
      "Iteration 70, loss = 0.00101445\n",
      "Iteration 71, loss = 0.00099241\n",
      "Iteration 72, loss = 0.00097049\n",
      "Iteration 73, loss = 0.00094972\n",
      "Iteration 74, loss = 0.00092977\n",
      "Iteration 75, loss = 0.00091115\n",
      "Iteration 76, loss = 0.00089119\n",
      "Iteration 77, loss = 0.00087377\n",
      "Iteration 78, loss = 0.00085665\n",
      "Iteration 79, loss = 0.00083971\n",
      "Iteration 80, loss = 0.00082368\n",
      "Iteration 81, loss = 0.00080825\n",
      "Iteration 82, loss = 0.00079356\n",
      "Iteration 83, loss = 0.00077872\n",
      "Iteration 84, loss = 0.00076450\n",
      "Iteration 85, loss = 0.00075070\n",
      "Iteration 86, loss = 0.00073802\n",
      "Iteration 87, loss = 0.00072566\n",
      "Iteration 88, loss = 0.00071261\n",
      "Iteration 89, loss = 0.00070105\n",
      "Iteration 90, loss = 0.00069002\n",
      "Iteration 91, loss = 0.00067932\n",
      "Iteration 92, loss = 0.00066820\n",
      "Iteration 93, loss = 0.00065727\n",
      "Iteration 94, loss = 0.00064701\n",
      "Iteration 95, loss = 0.00063706\n",
      "Iteration 96, loss = 0.00062746\n",
      "Iteration 97, loss = 0.00061810\n",
      "Iteration 98, loss = 0.00060968\n",
      "Iteration 99, loss = 0.00060045\n",
      "Iteration 100, loss = 0.00059220\n",
      "Iteration 101, loss = 0.00058351\n",
      "Iteration 102, loss = 0.00057613\n",
      "Iteration 103, loss = 0.00056774\n",
      "Iteration 104, loss = 0.00056045\n",
      "Iteration 105, loss = 0.00055323\n",
      "Iteration 106, loss = 0.00054583\n",
      "Iteration 107, loss = 0.00053974\n",
      "Iteration 108, loss = 0.00053225\n",
      "Iteration 109, loss = 0.00052512\n",
      "Iteration 110, loss = 0.00051850\n",
      "Iteration 111, loss = 0.00051222\n",
      "Iteration 112, loss = 0.00050617\n",
      "Iteration 113, loss = 0.00050025\n",
      "Iteration 114, loss = 0.00049438\n",
      "Iteration 115, loss = 0.00048850\n",
      "Iteration 116, loss = 0.00048303\n",
      "Iteration 117, loss = 0.00047757\n",
      "Iteration 118, loss = 0.00047254\n",
      "Iteration 119, loss = 0.00046726\n",
      "Iteration 120, loss = 0.00046196\n",
      "Iteration 121, loss = 0.00045691\n",
      "Iteration 122, loss = 0.00045195\n",
      "Iteration 123, loss = 0.00044790\n",
      "Iteration 124, loss = 0.00044318\n",
      "Iteration 125, loss = 0.00043826\n",
      "Iteration 126, loss = 0.00043384\n",
      "Iteration 127, loss = 0.00042954\n",
      "Iteration 128, loss = 0.00042517\n",
      "Iteration 129, loss = 0.00042111\n",
      "Iteration 130, loss = 0.00041775\n",
      "Iteration 131, loss = 0.00041439\n",
      "Iteration 132, loss = 0.00040935\n",
      "Iteration 133, loss = 0.00040561\n",
      "Iteration 134, loss = 0.00040193\n",
      "Iteration 135, loss = 0.00039815\n",
      "Iteration 136, loss = 0.00039460\n",
      "Iteration 137, loss = 0.00039123\n",
      "Iteration 138, loss = 0.00038785\n",
      "Iteration 139, loss = 0.00038449\n",
      "Iteration 140, loss = 0.00038107\n",
      "Iteration 141, loss = 0.00037797\n",
      "Iteration 142, loss = 0.00037494\n",
      "Iteration 143, loss = 0.00037172\n",
      "Iteration 144, loss = 0.00036874\n",
      "Iteration 145, loss = 0.00036587\n",
      "Iteration 146, loss = 0.00036329\n",
      "Iteration 147, loss = 0.00035990\n",
      "Iteration 148, loss = 0.00035731\n",
      "Iteration 149, loss = 0.00035454\n",
      "Iteration 150, loss = 0.00035183\n",
      "Iteration 151, loss = 0.00034935\n",
      "Iteration 152, loss = 0.00034670\n",
      "Iteration 153, loss = 0.00034411\n",
      "Iteration 154, loss = 0.00034157\n",
      "Iteration 155, loss = 0.00033925\n",
      "Iteration 156, loss = 0.00033688\n",
      "Iteration 157, loss = 0.00033443\n",
      "Iteration 158, loss = 0.00033209\n",
      "Iteration 159, loss = 0.00032998\n",
      "Iteration 160, loss = 0.00032763\n",
      "Iteration 161, loss = 0.00032545\n",
      "Iteration 162, loss = 0.00032325\n",
      "Iteration 163, loss = 0.00032113\n",
      "Iteration 164, loss = 0.00031933\n",
      "Iteration 165, loss = 0.00031704\n",
      "Iteration 166, loss = 0.00031497\n",
      "Iteration 167, loss = 0.00031304\n",
      "Iteration 168, loss = 0.00031103\n",
      "Iteration 169, loss = 0.00030939\n",
      "Iteration 170, loss = 0.00030723\n",
      "Iteration 171, loss = 0.00030548\n",
      "Iteration 172, loss = 0.00030363\n",
      "Iteration 173, loss = 0.00030176\n",
      "Iteration 174, loss = 0.00030008\n",
      "Iteration 175, loss = 0.00029830\n",
      "Iteration 176, loss = 0.00029657\n",
      "Iteration 177, loss = 0.00029486\n",
      "Iteration 178, loss = 0.00029323\n",
      "Iteration 179, loss = 0.00029169\n",
      "Iteration 180, loss = 0.00029000\n",
      "Iteration 181, loss = 0.00028839\n",
      "Iteration 182, loss = 0.00028694\n",
      "Iteration 183, loss = 0.00028539\n",
      "Iteration 184, loss = 0.00028386\n",
      "Iteration 185, loss = 0.00028236\n",
      "Iteration 186, loss = 0.00028090\n",
      "Iteration 187, loss = 0.00027947\n",
      "Iteration 188, loss = 0.00027805\n",
      "Iteration 189, loss = 0.00027663\n",
      "Iteration 190, loss = 0.00027525\n",
      "Iteration 191, loss = 0.00027392\n",
      "Iteration 192, loss = 0.00027259\n",
      "Iteration 193, loss = 0.00027120\n",
      "Iteration 194, loss = 0.00026999\n",
      "Iteration 195, loss = 0.00026871\n",
      "Iteration 196, loss = 0.00026742\n",
      "Iteration 197, loss = 0.00026611\n",
      "Iteration 198, loss = 0.00026494\n",
      "Iteration 199, loss = 0.00026384\n",
      "Iteration 200, loss = 0.00026245\n",
      "Iteration 201, loss = 0.00026139\n",
      "Iteration 202, loss = 0.00026021\n",
      "Iteration 203, loss = 0.00025906\n",
      "Iteration 204, loss = 0.00025787\n",
      "Iteration 205, loss = 0.00025684\n",
      "Iteration 206, loss = 0.00025562\n",
      "Iteration 207, loss = 0.00025459\n",
      "Iteration 208, loss = 0.00025357\n",
      "Iteration 209, loss = 0.00025241\n",
      "Iteration 210, loss = 0.00025132\n",
      "Iteration 211, loss = 0.00025027\n",
      "Iteration 212, loss = 0.00024925\n",
      "Iteration 213, loss = 0.00024829\n",
      "Iteration 214, loss = 0.00024725\n",
      "Iteration 215, loss = 0.00024639\n",
      "Iteration 216, loss = 0.00024526\n",
      "Iteration 217, loss = 0.00024433\n",
      "Iteration 218, loss = 0.00024337\n",
      "Iteration 219, loss = 0.00024241\n",
      "Iteration 220, loss = 0.00024149\n",
      "Iteration 221, loss = 0.00024055\n",
      "Iteration 222, loss = 0.00023965\n",
      "Iteration 223, loss = 0.00023875\n",
      "Iteration 224, loss = 0.00023789\n",
      "Iteration 225, loss = 0.00023696\n",
      "Iteration 226, loss = 0.00023613\n",
      "Iteration 227, loss = 0.00023524\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the precision-recall curve.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Create the visualizer, fit, score, and poof it\n",
    "viz = PrecisionRecallCurve(mlp)\n",
    "viz.fit(X_train, y_train)\n",
    "viz.score(X_test, y_test)\n",
    "viz.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.98914479\n",
      "Iteration 2, loss = 0.60501746\n",
      "Iteration 3, loss = 0.40227261\n",
      "Iteration 4, loss = 0.29295680\n",
      "Iteration 5, loss = 0.18429444\n",
      "Iteration 6, loss = 0.13152220\n",
      "Iteration 7, loss = 0.11077956\n",
      "Iteration 8, loss = 0.09653206\n",
      "Iteration 9, loss = 0.08151748\n",
      "Iteration 10, loss = 0.06419769\n",
      "Iteration 11, loss = 0.05062338\n",
      "Iteration 12, loss = 0.03955554\n",
      "Iteration 13, loss = 0.02895702\n",
      "Iteration 14, loss = 0.02467639\n",
      "Iteration 15, loss = 0.02323019\n",
      "Iteration 16, loss = 0.02199423\n",
      "Iteration 17, loss = 0.02094098\n",
      "Iteration 18, loss = 0.02000995\n",
      "Iteration 19, loss = 0.01917867\n",
      "Iteration 20, loss = 0.01841893\n",
      "Iteration 21, loss = 0.01767025\n",
      "Iteration 22, loss = 0.01679624\n",
      "Iteration 23, loss = 0.01566671\n",
      "Iteration 24, loss = 0.01499650\n",
      "Iteration 25, loss = 0.01461147\n",
      "Iteration 26, loss = 0.01425613\n",
      "Iteration 27, loss = 0.01392437\n",
      "Iteration 28, loss = 0.01361338\n",
      "Iteration 29, loss = 0.01331963\n",
      "Iteration 30, loss = 0.01304133\n",
      "Iteration 31, loss = 0.01277631\n",
      "Iteration 32, loss = 0.01252136\n",
      "Iteration 33, loss = 0.01227610\n",
      "Iteration 34, loss = 0.01204334\n",
      "Iteration 35, loss = 0.01181835\n",
      "Iteration 36, loss = 0.01160120\n",
      "Iteration 37, loss = 0.01138674\n",
      "Iteration 38, loss = 0.01114185\n",
      "Iteration 39, loss = 0.01068570\n",
      "Iteration 40, loss = 0.00931931\n",
      "Iteration 41, loss = 0.00837135\n",
      "Iteration 42, loss = 0.00823743\n",
      "Iteration 43, loss = 0.00811009\n",
      "Iteration 44, loss = 0.00798858\n",
      "Iteration 45, loss = 0.00787176\n",
      "Iteration 46, loss = 0.00775893\n",
      "Iteration 47, loss = 0.00764993\n",
      "Iteration 48, loss = 0.00754393\n",
      "Iteration 49, loss = 0.00744097\n",
      "Iteration 50, loss = 0.00734108\n",
      "Iteration 51, loss = 0.00724359\n",
      "Iteration 52, loss = 0.00714821\n",
      "Iteration 53, loss = 0.00705459\n",
      "Iteration 54, loss = 0.00696304\n",
      "Iteration 55, loss = 0.00687375\n",
      "Iteration 56, loss = 0.00678624\n",
      "Iteration 57, loss = 0.00669949\n",
      "Iteration 58, loss = 0.00661367\n",
      "Iteration 59, loss = 0.00653079\n",
      "Iteration 60, loss = 0.00644709\n",
      "Iteration 61, loss = 0.00636622\n",
      "Iteration 62, loss = 0.00625151\n",
      "Iteration 63, loss = 0.00588106\n",
      "Iteration 64, loss = 0.00456463\n",
      "Iteration 65, loss = 0.00444449\n",
      "Iteration 66, loss = 0.00403812\n",
      "Iteration 67, loss = 0.00304747\n",
      "Iteration 68, loss = 0.00301922\n",
      "Iteration 69, loss = 0.00298939\n",
      "Iteration 70, loss = 0.00267257\n",
      "Iteration 71, loss = 0.00234813\n",
      "Iteration 72, loss = 0.00182692\n",
      "Iteration 73, loss = 0.00167525\n",
      "Iteration 74, loss = 0.00123243\n",
      "Iteration 75, loss = 0.00087937\n",
      "Iteration 76, loss = 0.00084550\n",
      "Iteration 77, loss = 0.00084203\n",
      "Iteration 78, loss = 0.00083213\n",
      "Iteration 79, loss = 0.00057244\n",
      "Iteration 80, loss = 0.00057051\n",
      "Iteration 81, loss = 0.00056874\n",
      "Iteration 82, loss = 0.00056720\n",
      "Iteration 83, loss = 0.00056548\n",
      "Iteration 84, loss = 0.00055950\n",
      "Iteration 85, loss = 0.00044710\n",
      "Iteration 86, loss = 0.00039683\n",
      "Iteration 87, loss = 0.00039601\n",
      "Iteration 88, loss = 0.00033720\n",
      "Iteration 89, loss = 0.00031149\n",
      "Iteration 90, loss = 0.00031054\n",
      "Iteration 91, loss = 0.00030972\n",
      "Iteration 92, loss = 0.00030893\n",
      "Iteration 93, loss = 0.00023986\n",
      "Iteration 94, loss = 0.00023158\n",
      "Iteration 95, loss = 0.00019475\n",
      "Iteration 96, loss = 0.00019340\n",
      "Iteration 97, loss = 0.00017252\n",
      "Iteration 98, loss = 0.00017152\n",
      "Iteration 99, loss = 0.00017117\n",
      "Iteration 100, loss = 0.00016929\n",
      "Iteration 101, loss = 0.00015110\n",
      "Iteration 102, loss = 0.00015078\n",
      "Iteration 103, loss = 0.00013991\n",
      "Iteration 104, loss = 0.00013930\n",
      "Iteration 105, loss = 0.00013906\n",
      "Iteration 106, loss = 0.00013882\n",
      "Iteration 107, loss = 0.00013859\n",
      "Iteration 108, loss = 0.00013836\n",
      "Iteration 109, loss = 0.00013814\n",
      "Iteration 110, loss = 0.00013792\n",
      "Iteration 111, loss = 0.00013771\n",
      "Iteration 112, loss = 0.00013748\n",
      "Iteration 113, loss = 0.00013725\n",
      "Iteration 114, loss = 0.00013044\n",
      "Iteration 115, loss = 0.00011630\n",
      "Iteration 116, loss = 0.00011603\n",
      "Iteration 117, loss = 0.00010847\n",
      "Iteration 118, loss = 0.00010829\n",
      "Iteration 119, loss = 0.00010812\n",
      "Iteration 120, loss = 0.00010795\n",
      "Iteration 121, loss = 0.00010778\n",
      "Iteration 122, loss = 0.00010762\n",
      "Iteration 123, loss = 0.00010746\n",
      "Iteration 124, loss = 0.00010730\n",
      "Iteration 125, loss = 0.00010715\n",
      "Iteration 126, loss = 0.00010699\n",
      "Iteration 127, loss = 0.00010684\n",
      "Iteration 128, loss = 0.00010670\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.34445514\n",
      "Iteration 2, loss = 0.18494022\n",
      "Iteration 3, loss = 0.12508080\n",
      "Iteration 4, loss = 0.08284978\n",
      "Iteration 5, loss = 0.06052813\n",
      "Iteration 6, loss = 0.05400666\n",
      "Iteration 7, loss = 0.05001465\n",
      "Iteration 8, loss = 0.04715333\n",
      "Iteration 9, loss = 0.04494240\n",
      "Iteration 10, loss = 0.04301826\n",
      "Iteration 11, loss = 0.04106427\n",
      "Iteration 12, loss = 0.04003564\n",
      "Iteration 13, loss = 0.03939484\n",
      "Iteration 14, loss = 0.03871831\n",
      "Iteration 15, loss = 0.03651326\n",
      "Iteration 16, loss = 0.03633602\n",
      "Iteration 17, loss = 0.03649178\n",
      "Iteration 18, loss = 0.03634140\n",
      "Iteration 19, loss = 0.03624101\n",
      "Iteration 20, loss = 0.03615853\n",
      "Iteration 21, loss = 0.03606182\n",
      "Iteration 22, loss = 0.03602777\n",
      "Iteration 23, loss = 0.03595222\n",
      "Iteration 24, loss = 0.03589461\n",
      "Iteration 25, loss = 0.03585327\n",
      "Iteration 26, loss = 0.03580481\n",
      "Iteration 27, loss = 0.03576992\n",
      "Iteration 28, loss = 0.03526325\n",
      "Iteration 29, loss = 0.03525005\n",
      "Iteration 30, loss = 0.03548842\n",
      "Iteration 31, loss = 0.03552272\n",
      "Iteration 32, loss = 0.03555900\n",
      "Iteration 33, loss = 0.03546341\n",
      "Iteration 34, loss = 0.03559386\n",
      "Iteration 35, loss = 0.03587533\n",
      "Iteration 36, loss = 0.03600181\n",
      "Iteration 37, loss = 0.03581091\n",
      "Iteration 38, loss = 0.03558531\n",
      "Iteration 39, loss = 0.03561305\n",
      "Iteration 40, loss = 0.03556650\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.18087404\n",
      "Iteration 2, loss = 0.09578703\n",
      "Iteration 3, loss = 0.07082518\n",
      "Iteration 4, loss = 0.05679987\n",
      "Iteration 5, loss = 0.04775206\n",
      "Iteration 6, loss = 0.04506254\n",
      "Iteration 7, loss = 0.04428415\n",
      "Iteration 8, loss = 0.04367285\n",
      "Iteration 9, loss = 0.04304730\n",
      "Iteration 10, loss = 0.04264604\n",
      "Iteration 11, loss = 0.04248792\n",
      "Iteration 12, loss = 0.04232321\n",
      "Iteration 13, loss = 0.04234017\n",
      "Iteration 14, loss = 0.04208628\n",
      "Iteration 15, loss = 0.04197379\n",
      "Iteration 16, loss = 0.04218998\n",
      "Iteration 17, loss = 0.04233336\n",
      "Iteration 18, loss = 0.04234244\n",
      "Iteration 19, loss = 0.04269656\n",
      "Iteration 20, loss = 0.04266675\n",
      "Iteration 21, loss = 0.04278687\n",
      "Iteration 22, loss = 0.04271714\n",
      "Iteration 23, loss = 0.04218748\n",
      "Iteration 24, loss = 0.04210952\n",
      "Iteration 25, loss = 0.04169509\n",
      "Iteration 26, loss = 0.04157010\n",
      "Iteration 27, loss = 0.04141274\n",
      "Iteration 28, loss = 0.04150662\n",
      "Iteration 29, loss = 0.04120134\n",
      "Iteration 30, loss = 0.04104427\n",
      "Iteration 31, loss = 0.04187453\n",
      "Iteration 32, loss = 0.04204348\n",
      "Iteration 33, loss = 0.04148696\n",
      "Iteration 34, loss = 0.04122547\n",
      "Iteration 35, loss = 0.04074823\n",
      "Iteration 36, loss = 0.04052937\n",
      "Iteration 37, loss = 0.04042727\n",
      "Iteration 38, loss = 0.04017837\n",
      "Iteration 39, loss = 0.03969023\n",
      "Iteration 40, loss = 0.04003890\n",
      "Iteration 41, loss = 0.04043505\n",
      "Iteration 42, loss = 0.04022511\n",
      "Iteration 43, loss = 0.04023402\n",
      "Iteration 44, loss = 0.04019017\n",
      "Iteration 45, loss = 0.03972140\n",
      "Iteration 46, loss = 0.03968156\n",
      "Iteration 47, loss = 0.03967234\n",
      "Iteration 48, loss = 0.03964492\n",
      "Iteration 49, loss = 0.03961945\n",
      "Iteration 50, loss = 0.03938833\n",
      "Iteration 51, loss = 0.03975370\n",
      "Iteration 52, loss = 0.03921034\n",
      "Iteration 53, loss = 0.03950122\n",
      "Iteration 54, loss = 0.03999110\n",
      "Iteration 55, loss = 0.04010396\n",
      "Iteration 56, loss = 0.04128820\n",
      "Iteration 57, loss = 0.04135950\n",
      "Iteration 58, loss = 0.04109505\n",
      "Iteration 59, loss = 0.04063693\n",
      "Iteration 60, loss = 0.04038320\n",
      "Iteration 61, loss = 0.04006305\n",
      "Iteration 62, loss = 0.03949718\n",
      "Iteration 63, loss = 0.04050453\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.84441941\n",
      "Iteration 2, loss = 0.33511588\n",
      "Iteration 3, loss = 0.15521158\n",
      "Iteration 4, loss = 0.08398753\n",
      "Iteration 5, loss = 0.06487016\n",
      "Iteration 6, loss = 0.05944622\n",
      "Iteration 7, loss = 0.05707668\n",
      "Iteration 8, loss = 0.05608001\n",
      "Iteration 9, loss = 0.05514247\n",
      "Iteration 10, loss = 0.05442285\n",
      "Iteration 11, loss = 0.05405681\n",
      "Iteration 12, loss = 0.05296334\n",
      "Iteration 13, loss = 0.05187329\n",
      "Iteration 14, loss = 0.05217699\n",
      "Iteration 15, loss = 0.05099982\n",
      "Iteration 16, loss = 0.05204782\n",
      "Iteration 17, loss = 0.05209569\n",
      "Iteration 18, loss = 0.05176617\n",
      "Iteration 19, loss = 0.05154869\n",
      "Iteration 20, loss = 0.05137105\n",
      "Iteration 21, loss = 0.05107358\n",
      "Iteration 22, loss = 0.05106450\n",
      "Iteration 23, loss = 0.05186662\n",
      "Iteration 24, loss = 0.05174526\n",
      "Iteration 25, loss = 0.05147970\n",
      "Iteration 26, loss = 0.05125769\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.26032997\n",
      "Iteration 2, loss = 0.39150518\n",
      "Iteration 3, loss = 0.22453975\n",
      "Iteration 4, loss = 0.24506020\n",
      "Iteration 5, loss = 0.25159802\n",
      "Iteration 6, loss = 0.24641070\n",
      "Iteration 7, loss = 0.23928523\n",
      "Iteration 8, loss = 0.23803489\n",
      "Iteration 9, loss = 0.23731069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10, loss = 0.23998781\n",
      "Iteration 11, loss = 0.23411021\n",
      "Iteration 12, loss = 0.22857071\n",
      "Iteration 13, loss = 0.22802870\n",
      "Iteration 14, loss = 0.23329716\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.60282308\n",
      "Iteration 2, loss = 0.87952455\n",
      "Iteration 3, loss = 0.64538156\n",
      "Iteration 4, loss = 0.56976848\n",
      "Iteration 5, loss = 0.54330499\n",
      "Iteration 6, loss = 0.53469996\n",
      "Iteration 7, loss = 0.54573685\n",
      "Iteration 8, loss = 0.54122495\n",
      "Iteration 9, loss = 0.54499220\n",
      "Iteration 10, loss = 0.54020513\n",
      "Iteration 11, loss = 0.54547513\n",
      "Iteration 12, loss = 0.54152425\n",
      "Iteration 13, loss = 0.53832408\n",
      "Iteration 14, loss = 0.54255984\n",
      "Iteration 15, loss = 0.54565264\n",
      "Iteration 16, loss = 0.54427826\n",
      "Iteration 17, loss = 0.54270859\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.14167474\n",
      "Iteration 2, loss = 0.76096878\n",
      "Iteration 3, loss = 0.72679666\n",
      "Iteration 4, loss = 0.66966502\n",
      "Iteration 5, loss = 0.66287737\n",
      "Iteration 6, loss = 0.65263545\n",
      "Iteration 7, loss = 0.66280882\n",
      "Iteration 8, loss = 0.68121505\n",
      "Iteration 9, loss = 0.67546496\n",
      "Iteration 10, loss = 0.66873963\n",
      "Iteration 11, loss = 0.68055480\n",
      "Iteration 12, loss = 0.66958052\n",
      "Iteration 13, loss = 0.66155471\n",
      "Iteration 14, loss = 0.66043963\n",
      "Iteration 15, loss = 0.65631195\n",
      "Iteration 16, loss = 0.67762697\n",
      "Iteration 17, loss = 0.67711075\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76387466\n",
      "Iteration 2, loss = 0.88139019\n",
      "Iteration 3, loss = 0.71381607\n",
      "Iteration 4, loss = 0.71804637\n",
      "Iteration 5, loss = 0.70528729\n",
      "Iteration 6, loss = 0.72983183\n",
      "Iteration 7, loss = 0.70956386\n",
      "Iteration 8, loss = 0.69702195\n",
      "Iteration 9, loss = 0.68652448\n",
      "Iteration 10, loss = 0.70431126\n",
      "Iteration 11, loss = 0.71736083\n",
      "Iteration 12, loss = 0.72047187\n",
      "Iteration 13, loss = 0.71452011\n",
      "Iteration 14, loss = 0.70925453\n",
      "Iteration 15, loss = 0.70386682\n",
      "Iteration 16, loss = 0.69885265\n",
      "Iteration 17, loss = 0.70092118\n",
      "Iteration 18, loss = 0.70079854\n",
      "Iteration 19, loss = 0.72687843\n",
      "Iteration 20, loss = 0.70816102\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.83366658\n",
      "Iteration 2, loss = 0.65021694\n",
      "Iteration 3, loss = 0.67005180\n",
      "Iteration 4, loss = 0.72876324\n",
      "Iteration 5, loss = 0.72428937\n",
      "Iteration 6, loss = 0.71716497\n",
      "Iteration 7, loss = 0.70160687\n",
      "Iteration 8, loss = 0.71666172\n",
      "Iteration 9, loss = 0.72087954\n",
      "Iteration 10, loss = 0.72237920\n",
      "Iteration 11, loss = 0.71115785\n",
      "Iteration 12, loss = 0.77477271\n",
      "Iteration 13, loss = 0.72535676\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.15643434\n",
      "Iteration 2, loss = 0.97529221\n",
      "Iteration 3, loss = 0.74896562\n",
      "Iteration 4, loss = 0.72984332\n",
      "Iteration 5, loss = 0.71966469\n",
      "Iteration 6, loss = 0.73061405\n",
      "Iteration 7, loss = 0.69673273\n",
      "Iteration 8, loss = 0.74041494\n",
      "Iteration 9, loss = 0.71202781\n",
      "Iteration 10, loss = 0.71062949\n",
      "Iteration 11, loss = 0.70081306\n",
      "Iteration 12, loss = 0.68899803\n",
      "Iteration 13, loss = 0.71862351\n",
      "Iteration 14, loss = 0.73926962\n",
      "Iteration 15, loss = 0.73160519\n",
      "Iteration 16, loss = 0.72532780\n",
      "Iteration 17, loss = 0.72020021\n",
      "Iteration 18, loss = 0.71589405\n",
      "Iteration 19, loss = 0.71312310\n",
      "Iteration 20, loss = 0.73844435\n",
      "Iteration 21, loss = 0.70195567\n",
      "Iteration 22, loss = 0.75518708\n",
      "Iteration 23, loss = 0.74113109\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.96721013\n",
      "Iteration 2, loss = 0.62858151\n",
      "Iteration 3, loss = 0.44149489\n",
      "Iteration 4, loss = 0.31373718\n",
      "Iteration 5, loss = 0.19700321\n",
      "Iteration 6, loss = 0.12899262\n",
      "Iteration 7, loss = 0.10532114\n",
      "Iteration 8, loss = 0.09303096\n",
      "Iteration 9, loss = 0.08392742\n",
      "Iteration 10, loss = 0.07254816\n",
      "Iteration 11, loss = 0.05700606\n",
      "Iteration 12, loss = 0.05394798\n",
      "Iteration 13, loss = 0.05140805\n",
      "Iteration 14, loss = 0.04943973\n",
      "Iteration 15, loss = 0.04728804\n",
      "Iteration 16, loss = 0.04329540\n",
      "Iteration 17, loss = 0.03852365\n",
      "Iteration 18, loss = 0.03810233\n",
      "Iteration 19, loss = 0.03741779\n",
      "Iteration 20, loss = 0.03683059\n",
      "Iteration 21, loss = 0.03637406\n",
      "Iteration 22, loss = 0.03594859\n",
      "Iteration 23, loss = 0.03555271\n",
      "Iteration 24, loss = 0.03521121\n",
      "Iteration 25, loss = 0.03491342\n",
      "Iteration 26, loss = 0.03462479\n",
      "Iteration 27, loss = 0.03437507\n",
      "Iteration 28, loss = 0.03413022\n",
      "Iteration 29, loss = 0.03388404\n",
      "Iteration 30, loss = 0.03367841\n",
      "Iteration 31, loss = 0.03349070\n",
      "Iteration 32, loss = 0.03328597\n",
      "Iteration 33, loss = 0.03312563\n",
      "Iteration 34, loss = 0.03293410\n",
      "Iteration 35, loss = 0.03237018\n",
      "Iteration 36, loss = 0.02957509\n",
      "Iteration 37, loss = 0.02844477\n",
      "Iteration 38, loss = 0.02887131\n",
      "Iteration 39, loss = 0.02931072\n",
      "Iteration 40, loss = 0.02930433\n",
      "Iteration 41, loss = 0.02926733\n",
      "Iteration 42, loss = 0.02921775\n",
      "Iteration 43, loss = 0.02918487\n",
      "Iteration 44, loss = 0.02914825\n",
      "Iteration 45, loss = 0.02911442\n",
      "Iteration 46, loss = 0.02908355\n",
      "Iteration 47, loss = 0.02904978\n",
      "Iteration 48, loss = 0.02902692\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.50783272\n",
      "Iteration 2, loss = 0.34190883\n",
      "Iteration 3, loss = 0.24331193\n",
      "Iteration 4, loss = 0.17400576\n",
      "Iteration 5, loss = 0.11564212\n",
      "Iteration 6, loss = 0.08279746\n",
      "Iteration 7, loss = 0.07019830\n",
      "Iteration 8, loss = 0.05868593\n",
      "Iteration 9, loss = 0.05427331\n",
      "Iteration 10, loss = 0.05265368\n",
      "Iteration 11, loss = 0.05123948\n",
      "Iteration 12, loss = 0.05034747\n",
      "Iteration 13, loss = 0.04970685\n",
      "Iteration 14, loss = 0.04939265\n",
      "Iteration 15, loss = 0.04933434\n",
      "Iteration 16, loss = 0.04956913\n",
      "Iteration 17, loss = 0.04868350\n",
      "Iteration 18, loss = 0.04873651\n",
      "Iteration 19, loss = 0.04850145\n",
      "Iteration 20, loss = 0.04826692\n",
      "Iteration 21, loss = 0.04799774\n",
      "Iteration 22, loss = 0.04787264\n",
      "Iteration 23, loss = 0.04771547\n",
      "Iteration 24, loss = 0.04609052\n",
      "Iteration 25, loss = 0.04624842\n",
      "Iteration 26, loss = 0.04616607\n",
      "Iteration 27, loss = 0.04528462\n",
      "Iteration 28, loss = 0.04542667\n",
      "Iteration 29, loss = 0.04566081\n",
      "Iteration 30, loss = 0.04463764\n",
      "Iteration 31, loss = 0.04613946\n",
      "Iteration 32, loss = 0.04611751\n",
      "Iteration 33, loss = 0.04609220\n",
      "Iteration 34, loss = 0.04586070\n",
      "Iteration 35, loss = 0.04531143\n",
      "Iteration 36, loss = 0.04553519\n",
      "Iteration 37, loss = 0.04550765\n",
      "Iteration 38, loss = 0.04596517\n",
      "Iteration 39, loss = 0.04565179\n",
      "Iteration 40, loss = 0.04564999\n",
      "Iteration 41, loss = 0.04559684\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30257982\n",
      "Iteration 2, loss = 0.13388238\n",
      "Iteration 3, loss = 0.08341520\n",
      "Iteration 4, loss = 0.06858366\n",
      "Iteration 5, loss = 0.05945865\n",
      "Iteration 6, loss = 0.05184190\n",
      "Iteration 7, loss = 0.04668008\n",
      "Iteration 8, loss = 0.04499292\n",
      "Iteration 9, loss = 0.04360211\n",
      "Iteration 10, loss = 0.04252930\n",
      "Iteration 11, loss = 0.04382986\n",
      "Iteration 12, loss = 0.04298385\n",
      "Iteration 13, loss = 0.04243008\n",
      "Iteration 14, loss = 0.04193018\n",
      "Iteration 15, loss = 0.04152280\n",
      "Iteration 16, loss = 0.04124478\n",
      "Iteration 17, loss = 0.04091926\n",
      "Iteration 18, loss = 0.04065346\n",
      "Iteration 19, loss = 0.04044791\n",
      "Iteration 20, loss = 0.04016199\n",
      "Iteration 21, loss = 0.03998352\n",
      "Iteration 22, loss = 0.03970827\n",
      "Iteration 23, loss = 0.03953878\n",
      "Iteration 24, loss = 0.03942332\n",
      "Iteration 25, loss = 0.03935882\n",
      "Iteration 26, loss = 0.03870683\n",
      "Iteration 27, loss = 0.03848134\n",
      "Iteration 28, loss = 0.03905592\n",
      "Iteration 29, loss = 0.03963240\n",
      "Iteration 30, loss = 0.03961653\n",
      "Iteration 31, loss = 0.03950221\n",
      "Iteration 32, loss = 0.03936173\n",
      "Iteration 33, loss = 0.03932703\n",
      "Iteration 34, loss = 0.03908759\n",
      "Iteration 35, loss = 0.03862945\n",
      "Iteration 36, loss = 0.03849181\n",
      "Iteration 37, loss = 0.03842439\n",
      "Iteration 38, loss = 0.03865855\n",
      "Iteration 39, loss = 0.03822350\n",
      "Iteration 40, loss = 0.03791845\n",
      "Iteration 41, loss = 0.03800361\n",
      "Iteration 42, loss = 0.03789027\n",
      "Iteration 43, loss = 0.03775984\n",
      "Iteration 44, loss = 0.03725640\n",
      "Iteration 45, loss = 0.03719008\n",
      "Iteration 46, loss = 0.03718561\n",
      "Iteration 47, loss = 0.03699535\n",
      "Iteration 48, loss = 0.03698006\n",
      "Iteration 49, loss = 0.03708639\n",
      "Iteration 50, loss = 0.03743970\n",
      "Iteration 51, loss = 0.03734363\n",
      "Iteration 52, loss = 0.03754124\n",
      "Iteration 53, loss = 0.03735603\n",
      "Iteration 54, loss = 0.03732087\n",
      "Iteration 55, loss = 0.03728875\n",
      "Iteration 56, loss = 0.03716646\n",
      "Iteration 57, loss = 0.03731331\n",
      "Iteration 58, loss = 0.03718093\n",
      "Iteration 59, loss = 0.03714316\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.88386549\n",
      "Iteration 2, loss = 0.40018340\n",
      "Iteration 3, loss = 0.19784565\n",
      "Iteration 4, loss = 0.10299779\n",
      "Iteration 5, loss = 0.08567169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6, loss = 0.07638405\n",
      "Iteration 7, loss = 0.07043136\n",
      "Iteration 8, loss = 0.06630272\n",
      "Iteration 9, loss = 0.06340459\n",
      "Iteration 10, loss = 0.06128150\n",
      "Iteration 11, loss = 0.05731906\n",
      "Iteration 12, loss = 0.05253110\n",
      "Iteration 13, loss = 0.05129151\n",
      "Iteration 14, loss = 0.04933782\n",
      "Iteration 15, loss = 0.04961606\n",
      "Iteration 16, loss = 0.04896101\n",
      "Iteration 17, loss = 0.04898397\n",
      "Iteration 18, loss = 0.04914475\n",
      "Iteration 19, loss = 0.04905056\n",
      "Iteration 20, loss = 0.04761962\n",
      "Iteration 21, loss = 0.04863082\n",
      "Iteration 22, loss = 0.04914035\n",
      "Iteration 23, loss = 0.05057448\n",
      "Iteration 24, loss = 0.05133748\n",
      "Iteration 25, loss = 0.05148829\n",
      "Iteration 26, loss = 0.05095073\n",
      "Iteration 27, loss = 0.04921594\n",
      "Iteration 28, loss = 0.04829419\n",
      "Iteration 29, loss = 0.04782871\n",
      "Iteration 30, loss = 0.05009857\n",
      "Iteration 31, loss = 0.04954770\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.37132438\n",
      "Iteration 2, loss = 0.26730766\n",
      "Iteration 3, loss = 0.24272080\n",
      "Iteration 4, loss = 0.22578365\n",
      "Iteration 5, loss = 0.22908389\n",
      "Iteration 6, loss = 0.21666703\n",
      "Iteration 7, loss = 0.21069882\n",
      "Iteration 8, loss = 0.21023132\n",
      "Iteration 9, loss = 0.21439554\n",
      "Iteration 10, loss = 0.21172626\n",
      "Iteration 11, loss = 0.21164073\n",
      "Iteration 12, loss = 0.20960577\n",
      "Iteration 13, loss = 0.20870596\n",
      "Iteration 14, loss = 0.20669924\n",
      "Iteration 15, loss = 0.20675196\n",
      "Iteration 16, loss = 0.19884268\n",
      "Iteration 17, loss = 0.20529965\n",
      "Iteration 18, loss = 0.20272342\n",
      "Iteration 19, loss = 0.20312890\n",
      "Iteration 20, loss = 0.20319330\n",
      "Iteration 21, loss = 0.20231872\n",
      "Iteration 22, loss = 0.20246573\n",
      "Iteration 23, loss = 0.20093772\n",
      "Iteration 24, loss = 0.20080847\n",
      "Iteration 25, loss = 0.19938778\n",
      "Iteration 26, loss = 0.19864242\n",
      "Iteration 27, loss = 0.19843313\n",
      "Iteration 28, loss = 0.19820722\n",
      "Iteration 29, loss = 0.19756038\n",
      "Iteration 30, loss = 0.19554840\n",
      "Iteration 31, loss = 0.19865525\n",
      "Iteration 32, loss = 0.20814560\n",
      "Iteration 33, loss = 0.20343159\n",
      "Iteration 34, loss = 0.20174196\n",
      "Iteration 35, loss = 0.19990377\n",
      "Iteration 36, loss = 0.20000244\n",
      "Iteration 37, loss = 0.19951104\n",
      "Iteration 38, loss = 0.19871887\n",
      "Iteration 39, loss = 0.19811594\n",
      "Iteration 40, loss = 0.19737126\n",
      "Iteration 41, loss = 0.19648738\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67523942\n",
      "Iteration 2, loss = 0.62071245\n",
      "Iteration 3, loss = 0.55159772\n",
      "Iteration 4, loss = 0.52147475\n",
      "Iteration 5, loss = 0.53343410\n",
      "Iteration 6, loss = 0.51924591\n",
      "Iteration 7, loss = 0.56691459\n",
      "Iteration 8, loss = 0.53545525\n",
      "Iteration 9, loss = 0.53934891\n",
      "Iteration 10, loss = 0.52848160\n",
      "Iteration 11, loss = 0.54389072\n",
      "Iteration 12, loss = 0.53027689\n",
      "Iteration 13, loss = 0.50807873\n",
      "Iteration 14, loss = 0.58127963\n",
      "Iteration 15, loss = 0.54971110\n",
      "Iteration 16, loss = 0.55027684\n",
      "Iteration 17, loss = 0.54153260\n",
      "Iteration 18, loss = 0.54139394\n",
      "Iteration 19, loss = 0.54024511\n",
      "Iteration 20, loss = 0.53890047\n",
      "Iteration 21, loss = 0.54585577\n",
      "Iteration 22, loss = 0.54654476\n",
      "Iteration 23, loss = 0.54327285\n",
      "Iteration 24, loss = 0.53876785\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.07298292\n",
      "Iteration 2, loss = 0.72620451\n",
      "Iteration 3, loss = 0.71602112\n",
      "Iteration 4, loss = 0.67130823\n",
      "Iteration 5, loss = 0.66206434\n",
      "Iteration 6, loss = 0.64733593\n",
      "Iteration 7, loss = 0.66760994\n",
      "Iteration 8, loss = 0.65829825\n",
      "Iteration 9, loss = 0.65605408\n",
      "Iteration 10, loss = 0.66889177\n",
      "Iteration 11, loss = 0.66250615\n",
      "Iteration 12, loss = 0.67608033\n",
      "Iteration 13, loss = 0.68497158\n",
      "Iteration 14, loss = 0.66800319\n",
      "Iteration 15, loss = 0.67283703\n",
      "Iteration 16, loss = 0.66527715\n",
      "Iteration 17, loss = 0.66604334\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.10809539\n",
      "Iteration 2, loss = 0.74022147\n",
      "Iteration 3, loss = 0.66831448\n",
      "Iteration 4, loss = 0.68616118\n",
      "Iteration 5, loss = 0.71135179\n",
      "Iteration 6, loss = 0.69983713\n",
      "Iteration 7, loss = 0.74368964\n",
      "Iteration 8, loss = 0.68540692\n",
      "Iteration 9, loss = 0.70992744\n",
      "Iteration 10, loss = 0.71051988\n",
      "Iteration 11, loss = 0.72565699\n",
      "Iteration 12, loss = 0.71599917\n",
      "Iteration 13, loss = 0.70243872\n",
      "Iteration 14, loss = 0.72305210\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.90205248\n",
      "Iteration 2, loss = 0.75247987\n",
      "Iteration 3, loss = 0.70095055\n",
      "Iteration 4, loss = 0.71568382\n",
      "Iteration 5, loss = 0.70680433\n",
      "Iteration 6, loss = 0.71243910\n",
      "Iteration 7, loss = 0.83008884\n",
      "Iteration 8, loss = 0.73911265\n",
      "Iteration 9, loss = 0.71333121\n",
      "Iteration 10, loss = 0.73039591\n",
      "Iteration 11, loss = 0.72650725\n",
      "Iteration 12, loss = 0.75016927\n",
      "Iteration 13, loss = 0.72732367\n",
      "Iteration 14, loss = 0.72376636\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.42806994\n",
      "Iteration 2, loss = 0.79523020\n",
      "Iteration 3, loss = 0.71904743\n",
      "Iteration 4, loss = 0.75477964\n",
      "Iteration 5, loss = 0.70733848\n",
      "Iteration 6, loss = 0.70302775\n",
      "Iteration 7, loss = 0.68820707\n",
      "Iteration 8, loss = 0.70316996\n",
      "Iteration 9, loss = 0.69916359\n",
      "Iteration 10, loss = 0.71219460\n",
      "Iteration 11, loss = 0.67361480\n",
      "Iteration 12, loss = 0.68385870\n",
      "Iteration 13, loss = 0.66790819\n",
      "Iteration 14, loss = 0.65544565\n",
      "Iteration 15, loss = 0.66146313\n",
      "Iteration 16, loss = 0.65418487\n",
      "Iteration 17, loss = 0.63714932\n",
      "Iteration 18, loss = 0.62627938\n",
      "Iteration 19, loss = 0.63026024\n",
      "Iteration 20, loss = 0.61457357\n",
      "Iteration 21, loss = 0.61690583\n",
      "Iteration 22, loss = 0.59455062\n",
      "Iteration 23, loss = 0.58343982\n",
      "Iteration 24, loss = 0.57716554\n",
      "Iteration 25, loss = 0.56724798\n",
      "Iteration 26, loss = 0.55982727\n",
      "Iteration 27, loss = 0.55148737\n",
      "Iteration 28, loss = 0.54243256\n",
      "Iteration 29, loss = 0.54059881\n",
      "Iteration 30, loss = 0.53107286\n",
      "Iteration 31, loss = 0.50995729\n",
      "Iteration 32, loss = 0.49737413\n",
      "Iteration 33, loss = 0.48698504\n",
      "Iteration 34, loss = 0.47448838\n",
      "Iteration 35, loss = 0.45997372\n",
      "Iteration 36, loss = 0.44797640\n",
      "Iteration 37, loss = 0.43226325\n",
      "Iteration 38, loss = 0.40634561\n",
      "Iteration 39, loss = 0.38000715\n",
      "Iteration 40, loss = 0.37235139\n",
      "Iteration 41, loss = 0.36486692\n",
      "Iteration 42, loss = 0.35831861\n",
      "Iteration 43, loss = 0.37716759\n",
      "Iteration 44, loss = 0.49249176\n",
      "Iteration 45, loss = 0.53705221\n",
      "Iteration 46, loss = 0.53760711\n",
      "Iteration 47, loss = 0.53619958\n",
      "Iteration 48, loss = 0.53448012\n",
      "Iteration 49, loss = 0.52926596\n",
      "Iteration 50, loss = 0.52501604\n",
      "Iteration 51, loss = 0.52112273\n",
      "Iteration 52, loss = 0.51090070\n",
      "Iteration 53, loss = 0.50502913\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.60286609\n",
      "Iteration 2, loss = 1.24355341\n",
      "Iteration 3, loss = 1.03601288\n",
      "Iteration 4, loss = 0.83245249\n",
      "Iteration 5, loss = 0.55275149\n",
      "Iteration 6, loss = 0.41217744\n",
      "Iteration 7, loss = 0.35589214\n",
      "Iteration 8, loss = 0.30217157\n",
      "Iteration 9, loss = 0.24700842\n",
      "Iteration 10, loss = 0.17476774\n",
      "Iteration 11, loss = 0.15290324\n",
      "Iteration 12, loss = 0.13333447\n",
      "Iteration 13, loss = 0.11686950\n",
      "Iteration 14, loss = 0.09884536\n",
      "Iteration 15, loss = 0.08086940\n",
      "Iteration 16, loss = 0.07577259\n",
      "Iteration 17, loss = 0.06947832\n",
      "Iteration 18, loss = 0.06356824\n",
      "Iteration 19, loss = 0.05930384\n",
      "Iteration 20, loss = 0.05505936\n",
      "Iteration 21, loss = 0.04812079\n",
      "Iteration 22, loss = 0.04317805\n",
      "Iteration 23, loss = 0.03722259\n",
      "Iteration 24, loss = 0.03535643\n",
      "Iteration 25, loss = 0.03408630\n",
      "Iteration 26, loss = 0.03259638\n",
      "Iteration 27, loss = 0.03232069\n",
      "Iteration 28, loss = 0.03172618\n",
      "Iteration 29, loss = 0.03145539\n",
      "Iteration 30, loss = 0.02979812\n",
      "Iteration 31, loss = 0.02974475\n",
      "Iteration 32, loss = 0.02971399\n",
      "Iteration 33, loss = 0.02966341\n",
      "Iteration 34, loss = 0.02934926\n",
      "Iteration 35, loss = 0.02920612\n",
      "Iteration 36, loss = 0.02873218\n",
      "Iteration 37, loss = 0.02872312\n",
      "Iteration 38, loss = 0.02866920\n",
      "Iteration 39, loss = 0.02877759\n",
      "Iteration 40, loss = 0.02844469\n",
      "Iteration 41, loss = 0.02798162\n",
      "Iteration 42, loss = 0.02820145\n",
      "Iteration 43, loss = 0.02822060\n",
      "Iteration 44, loss = 0.02795101\n",
      "Iteration 45, loss = 0.02822228\n",
      "Iteration 46, loss = 0.02852963\n",
      "Iteration 47, loss = 0.02829080\n",
      "Iteration 48, loss = 0.02827511\n",
      "Iteration 49, loss = 0.02794330\n",
      "Iteration 50, loss = 0.02801023\n",
      "Iteration 51, loss = 0.02882449\n",
      "Iteration 52, loss = 0.02940811\n",
      "Iteration 53, loss = 0.02937727\n",
      "Iteration 54, loss = 0.02951811\n",
      "Iteration 55, loss = 0.02884150\n",
      "Iteration 56, loss = 0.02960106\n",
      "Iteration 57, loss = 0.02872898\n",
      "Iteration 58, loss = 0.02888793\n",
      "Iteration 59, loss = 0.02908148\n",
      "Iteration 60, loss = 0.02917728\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.39306359\n",
      "Iteration 2, loss = 0.16535535\n",
      "Iteration 3, loss = 0.11838070\n",
      "Iteration 4, loss = 0.09660013\n",
      "Iteration 5, loss = 0.07307861\n",
      "Iteration 6, loss = 0.05759187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 0.04577198\n",
      "Iteration 8, loss = 0.03714102\n",
      "Iteration 9, loss = 0.03666428\n",
      "Iteration 10, loss = 0.03630520\n",
      "Iteration 11, loss = 0.03610384\n",
      "Iteration 12, loss = 0.03605764\n",
      "Iteration 13, loss = 0.03583160\n",
      "Iteration 14, loss = 0.03575426\n",
      "Iteration 15, loss = 0.03569349\n",
      "Iteration 16, loss = 0.03564316\n",
      "Iteration 17, loss = 0.03555149\n",
      "Iteration 18, loss = 0.03551131\n",
      "Iteration 19, loss = 0.03542118\n",
      "Iteration 20, loss = 0.03516602\n",
      "Iteration 21, loss = 0.03544117\n",
      "Iteration 22, loss = 0.03539778\n",
      "Iteration 23, loss = 0.03536333\n",
      "Iteration 24, loss = 0.03536056\n",
      "Iteration 25, loss = 0.03507324\n",
      "Iteration 26, loss = 0.03550026\n",
      "Iteration 27, loss = 0.03579106\n",
      "Iteration 28, loss = 0.03577213\n",
      "Iteration 29, loss = 0.03574941\n",
      "Iteration 30, loss = 0.03571097\n",
      "Iteration 31, loss = 0.03567179\n",
      "Iteration 32, loss = 0.03574184\n",
      "Iteration 33, loss = 0.03562375\n",
      "Iteration 34, loss = 0.03559262\n",
      "Iteration 35, loss = 0.03555825\n",
      "Iteration 36, loss = 0.03551194\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.08644433\n",
      "Iteration 2, loss = 0.53708336\n",
      "Iteration 3, loss = 0.37481476\n",
      "Iteration 4, loss = 0.26199772\n",
      "Iteration 5, loss = 0.17139498\n",
      "Iteration 6, loss = 0.11499087\n",
      "Iteration 7, loss = 0.08713784\n",
      "Iteration 8, loss = 0.07840640\n",
      "Iteration 9, loss = 0.07204103\n",
      "Iteration 10, loss = 0.06752721\n",
      "Iteration 11, loss = 0.06414921\n",
      "Iteration 12, loss = 0.05488385\n",
      "Iteration 13, loss = 0.05136157\n",
      "Iteration 14, loss = 0.05086182\n",
      "Iteration 15, loss = 0.05009693\n",
      "Iteration 16, loss = 0.04919836\n",
      "Iteration 17, loss = 0.04835275\n",
      "Iteration 18, loss = 0.04782217\n",
      "Iteration 19, loss = 0.04718487\n",
      "Iteration 20, loss = 0.04677087\n",
      "Iteration 21, loss = 0.04644736\n",
      "Iteration 22, loss = 0.04619351\n",
      "Iteration 23, loss = 0.04590677\n",
      "Iteration 24, loss = 0.04561402\n",
      "Iteration 25, loss = 0.04531829\n",
      "Iteration 26, loss = 0.04494054\n",
      "Iteration 27, loss = 0.04469299\n",
      "Iteration 28, loss = 0.04445122\n",
      "Iteration 29, loss = 0.04440900\n",
      "Iteration 30, loss = 0.04330324\n",
      "Iteration 31, loss = 0.04315098\n",
      "Iteration 32, loss = 0.04288751\n",
      "Iteration 33, loss = 0.04278031\n",
      "Iteration 34, loss = 0.04269750\n",
      "Iteration 35, loss = 0.04253446\n",
      "Iteration 36, loss = 0.04238913\n",
      "Iteration 37, loss = 0.04229220\n",
      "Iteration 38, loss = 0.04219881\n",
      "Iteration 39, loss = 0.04205051\n",
      "Iteration 40, loss = 0.04136938\n",
      "Iteration 41, loss = 0.04138593\n",
      "Iteration 42, loss = 0.04099885\n",
      "Iteration 43, loss = 0.04136379\n",
      "Iteration 44, loss = 0.04112809\n",
      "Iteration 45, loss = 0.04088276\n",
      "Iteration 46, loss = 0.04137905\n",
      "Iteration 47, loss = 0.04176056\n",
      "Iteration 48, loss = 0.04152047\n",
      "Iteration 49, loss = 0.04139159\n",
      "Iteration 50, loss = 0.04108884\n",
      "Iteration 51, loss = 0.04125211\n",
      "Iteration 52, loss = 0.04076084\n",
      "Iteration 53, loss = 0.04077163\n",
      "Iteration 54, loss = 0.04034041\n",
      "Iteration 55, loss = 0.03997155\n",
      "Iteration 56, loss = 0.03977618\n",
      "Iteration 57, loss = 0.03927692\n",
      "Iteration 58, loss = 0.03917099\n",
      "Iteration 59, loss = 0.03916716\n",
      "Iteration 60, loss = 0.03918098\n",
      "Iteration 61, loss = 0.03916908\n",
      "Iteration 62, loss = 0.03915789\n",
      "Iteration 63, loss = 0.03899538\n",
      "Iteration 64, loss = 0.03869813\n",
      "Iteration 65, loss = 0.03859513\n",
      "Iteration 66, loss = 0.03942260\n",
      "Iteration 67, loss = 0.03937116\n",
      "Iteration 68, loss = 0.03931578\n",
      "Iteration 69, loss = 0.03927877\n",
      "Iteration 70, loss = 0.03922987\n",
      "Iteration 71, loss = 0.03920192\n",
      "Iteration 72, loss = 0.03914999\n",
      "Iteration 73, loss = 0.03910347\n",
      "Iteration 74, loss = 0.03907291\n",
      "Iteration 75, loss = 0.03902649\n",
      "Iteration 76, loss = 0.03898301\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.16631345\n",
      "Iteration 2, loss = 0.08579732\n",
      "Iteration 3, loss = 0.05953656\n",
      "Iteration 4, loss = 0.05571222\n",
      "Iteration 5, loss = 0.05409578\n",
      "Iteration 6, loss = 0.05228278\n",
      "Iteration 7, loss = 0.05220805\n",
      "Iteration 8, loss = 0.05211784\n",
      "Iteration 9, loss = 0.05206177\n",
      "Iteration 10, loss = 0.05204232\n",
      "Iteration 11, loss = 0.05197391\n",
      "Iteration 12, loss = 0.05179601\n",
      "Iteration 13, loss = 0.05170265\n",
      "Iteration 14, loss = 0.05103646\n",
      "Iteration 15, loss = 0.05083432\n",
      "Iteration 16, loss = 0.05149763\n",
      "Iteration 17, loss = 0.05138777\n",
      "Iteration 18, loss = 0.05157688\n",
      "Iteration 19, loss = 0.05167684\n",
      "Iteration 20, loss = 0.05070864\n",
      "Iteration 21, loss = 0.05054094\n",
      "Iteration 22, loss = 0.05027279\n",
      "Iteration 23, loss = 0.05007214\n",
      "Iteration 24, loss = 0.05000938\n",
      "Iteration 25, loss = 0.04984280\n",
      "Iteration 26, loss = 0.04986710\n",
      "Iteration 27, loss = 0.04973561\n",
      "Iteration 28, loss = 0.04935876\n",
      "Iteration 29, loss = 0.04937420\n",
      "Iteration 30, loss = 0.04925883\n",
      "Iteration 31, loss = 0.04940564\n",
      "Iteration 32, loss = 0.04931220\n",
      "Iteration 33, loss = 0.04932771\n",
      "Iteration 34, loss = 0.04944155\n",
      "Iteration 35, loss = 0.04936785\n",
      "Iteration 36, loss = 0.04935996\n",
      "Iteration 37, loss = 0.04926423\n",
      "Iteration 38, loss = 0.04893571\n",
      "Iteration 39, loss = 0.04943731\n",
      "Iteration 40, loss = 0.04919137\n",
      "Iteration 41, loss = 0.04758704\n",
      "Iteration 42, loss = 0.04781224\n",
      "Iteration 43, loss = 0.04808408\n",
      "Iteration 44, loss = 0.04847436\n",
      "Iteration 45, loss = 0.04963215\n",
      "Iteration 46, loss = 0.04952095\n",
      "Iteration 47, loss = 0.04942632\n",
      "Iteration 48, loss = 0.04923706\n",
      "Iteration 49, loss = 0.04914351\n",
      "Iteration 50, loss = 0.04904944\n",
      "Iteration 51, loss = 0.04901802\n",
      "Iteration 52, loss = 0.04893553\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.25251726\n",
      "Iteration 2, loss = 0.23735964\n",
      "Iteration 3, loss = 0.22261784\n",
      "Iteration 4, loss = 0.21581503\n",
      "Iteration 5, loss = 0.21712049\n",
      "Iteration 6, loss = 0.22140244\n",
      "Iteration 7, loss = 0.21788817\n",
      "Iteration 8, loss = 0.21708004\n",
      "Iteration 9, loss = 0.21632692\n",
      "Iteration 10, loss = 0.21533463\n",
      "Iteration 11, loss = 0.21280085\n",
      "Iteration 12, loss = 0.22000864\n",
      "Iteration 13, loss = 0.22094664\n",
      "Iteration 14, loss = 0.21732108\n",
      "Iteration 15, loss = 0.21395077\n",
      "Iteration 16, loss = 0.21832162\n",
      "Iteration 17, loss = 0.21556913\n",
      "Iteration 18, loss = 0.21517213\n",
      "Iteration 19, loss = 0.21399313\n",
      "Iteration 20, loss = 0.21364681\n",
      "Iteration 21, loss = 0.21316981\n",
      "Iteration 22, loss = 0.21272734\n",
      "Iteration 23, loss = 0.21220627\n",
      "Iteration 24, loss = 0.21311470\n",
      "Iteration 25, loss = 0.21398789\n",
      "Iteration 26, loss = 0.21282729\n",
      "Iteration 27, loss = 0.21614803\n",
      "Iteration 28, loss = 0.21277129\n",
      "Iteration 29, loss = 0.21222200\n",
      "Iteration 30, loss = 0.21284927\n",
      "Iteration 31, loss = 0.20964294\n",
      "Iteration 32, loss = 0.20982636\n",
      "Iteration 33, loss = 0.20897228\n",
      "Iteration 34, loss = 0.20777091\n",
      "Iteration 35, loss = 0.21309016\n",
      "Iteration 36, loss = 0.21128154\n",
      "Iteration 37, loss = 0.21328948\n",
      "Iteration 38, loss = 0.21486003\n",
      "Iteration 39, loss = 0.21214112\n",
      "Iteration 40, loss = 0.21202991\n",
      "Iteration 41, loss = 0.21105443\n",
      "Iteration 42, loss = 0.21475458\n",
      "Iteration 43, loss = 0.21088569\n",
      "Iteration 44, loss = 0.21016662\n",
      "Iteration 45, loss = 0.21085524\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.97896491\n",
      "Iteration 2, loss = 0.61434732\n",
      "Iteration 3, loss = 0.59854500\n",
      "Iteration 4, loss = 0.56227728\n",
      "Iteration 5, loss = 0.54595135\n",
      "Iteration 6, loss = 0.53755058\n",
      "Iteration 7, loss = 0.52695330\n",
      "Iteration 8, loss = 0.55117693\n",
      "Iteration 9, loss = 0.54053604\n",
      "Iteration 10, loss = 0.52971592\n",
      "Iteration 11, loss = 0.53556826\n",
      "Iteration 12, loss = 0.52637250\n",
      "Iteration 13, loss = 0.54413039\n",
      "Iteration 14, loss = 0.54675235\n",
      "Iteration 15, loss = 0.54589315\n",
      "Iteration 16, loss = 0.54136553\n",
      "Iteration 17, loss = 0.54792186\n",
      "Iteration 18, loss = 0.54823558\n",
      "Iteration 19, loss = 0.54575292\n",
      "Iteration 20, loss = 0.56429166\n",
      "Iteration 21, loss = 0.55039070\n",
      "Iteration 22, loss = 0.54382256\n",
      "Iteration 23, loss = 0.57222875\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.09362249\n",
      "Iteration 2, loss = 0.84369873\n",
      "Iteration 3, loss = 0.67978676\n",
      "Iteration 4, loss = 0.66796410\n",
      "Iteration 5, loss = 0.66662567\n",
      "Iteration 6, loss = 0.66264746\n",
      "Iteration 7, loss = 0.65383607\n",
      "Iteration 8, loss = 0.66157010\n",
      "Iteration 9, loss = 0.66228696\n",
      "Iteration 10, loss = 0.65312359\n",
      "Iteration 11, loss = 0.65257237\n",
      "Iteration 12, loss = 0.67038766\n",
      "Iteration 13, loss = 0.67303400\n",
      "Iteration 14, loss = 0.67767979\n",
      "Iteration 15, loss = 0.66318797\n",
      "Iteration 16, loss = 0.65227018\n",
      "Iteration 17, loss = 0.70524922\n",
      "Iteration 18, loss = 0.68108319\n",
      "Iteration 19, loss = 0.66396347\n",
      "Iteration 20, loss = 0.65917677\n",
      "Iteration 21, loss = 0.66060180\n",
      "Iteration 22, loss = 0.65870221\n",
      "Iteration 23, loss = 0.67892862\n",
      "Iteration 24, loss = 0.66341085\n",
      "Iteration 25, loss = 0.66311745\n",
      "Iteration 26, loss = 0.66801305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 27, loss = 0.66599875\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75974240\n",
      "Iteration 2, loss = 0.71866393\n",
      "Iteration 3, loss = 0.71277696\n",
      "Iteration 4, loss = 0.68159163\n",
      "Iteration 5, loss = 0.71052153\n",
      "Iteration 6, loss = 0.71849876\n",
      "Iteration 7, loss = 0.71062522\n",
      "Iteration 8, loss = 0.70553483\n",
      "Iteration 9, loss = 0.70750164\n",
      "Iteration 10, loss = 0.70686279\n",
      "Iteration 11, loss = 0.71232387\n",
      "Iteration 12, loss = 0.71991590\n",
      "Iteration 13, loss = 0.72705363\n",
      "Iteration 14, loss = 0.71808348\n",
      "Iteration 15, loss = 0.71364728\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.78579978\n",
      "Iteration 2, loss = 0.74008817\n",
      "Iteration 3, loss = 0.65907004\n",
      "Iteration 4, loss = 0.67268184\n",
      "Iteration 5, loss = 0.73845581\n",
      "Iteration 6, loss = 0.72926272\n",
      "Iteration 7, loss = 0.72052131\n",
      "Iteration 8, loss = 0.71727967\n",
      "Iteration 9, loss = 0.72114265\n",
      "Iteration 10, loss = 0.71829773\n",
      "Iteration 11, loss = 0.73700871\n",
      "Iteration 12, loss = 0.74968775\n",
      "Iteration 13, loss = 0.69925105\n",
      "Iteration 14, loss = 0.73337726\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.01681515\n",
      "Iteration 2, loss = 0.90154560\n",
      "Iteration 3, loss = 0.73439673\n",
      "Iteration 4, loss = 0.71211530\n",
      "Iteration 5, loss = 0.70856205\n",
      "Iteration 6, loss = 0.72172732\n",
      "Iteration 7, loss = 0.71524344\n",
      "Iteration 8, loss = 0.70141698\n",
      "Iteration 9, loss = 0.70706682\n",
      "Iteration 10, loss = 0.71684663\n",
      "Iteration 11, loss = 0.70323587\n",
      "Iteration 12, loss = 0.69746832\n",
      "Iteration 13, loss = 0.70394658\n",
      "Iteration 14, loss = 0.69633536\n",
      "Iteration 15, loss = 0.68745597\n",
      "Iteration 16, loss = 0.68295580\n",
      "Iteration 17, loss = 0.67982811\n",
      "Iteration 18, loss = 0.66972341\n",
      "Iteration 19, loss = 0.66303667\n",
      "Iteration 20, loss = 0.64892882\n",
      "Iteration 21, loss = 0.63894951\n",
      "Iteration 22, loss = 0.62251439\n",
      "Iteration 23, loss = 0.61255603\n",
      "Iteration 24, loss = 0.59697019\n",
      "Iteration 25, loss = 0.58313122\n",
      "Iteration 26, loss = 0.65272253\n",
      "Iteration 27, loss = 0.66585960\n",
      "Iteration 28, loss = 0.66459092\n",
      "Iteration 29, loss = 0.66208473\n",
      "Iteration 30, loss = 0.66102728\n",
      "Iteration 31, loss = 0.65925939\n",
      "Iteration 32, loss = 0.65706832\n",
      "Iteration 33, loss = 0.65989869\n",
      "Iteration 34, loss = 0.65560881\n",
      "Iteration 35, loss = 0.65214384\n",
      "Iteration 36, loss = 0.64861846\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.79077271\n",
      "Iteration 2, loss = 0.65663825\n",
      "Iteration 3, loss = 0.54348597\n",
      "Iteration 4, loss = 0.42086437\n",
      "Iteration 5, loss = 0.33134420\n",
      "Iteration 6, loss = 0.25059593\n",
      "Iteration 7, loss = 0.19721262\n",
      "Iteration 8, loss = 0.16156547\n",
      "Iteration 9, loss = 0.12480597\n",
      "Iteration 10, loss = 0.09440074\n",
      "Iteration 11, loss = 0.08138816\n",
      "Iteration 12, loss = 0.07565296\n",
      "Iteration 13, loss = 0.06985628\n",
      "Iteration 14, loss = 0.06354046\n",
      "Iteration 15, loss = 0.05118099\n",
      "Iteration 16, loss = 0.04709265\n",
      "Iteration 17, loss = 0.04540305\n",
      "Iteration 18, loss = 0.04012818\n",
      "Iteration 19, loss = 0.03791241\n",
      "Iteration 20, loss = 0.03762418\n",
      "Iteration 21, loss = 0.03705790\n",
      "Iteration 22, loss = 0.03664091\n",
      "Iteration 23, loss = 0.03620589\n",
      "Iteration 24, loss = 0.03582809\n",
      "Iteration 25, loss = 0.03551164\n",
      "Iteration 26, loss = 0.03523677\n",
      "Iteration 27, loss = 0.03496961\n",
      "Iteration 28, loss = 0.03473992\n",
      "Iteration 29, loss = 0.03450389\n",
      "Iteration 30, loss = 0.03431671\n",
      "Iteration 31, loss = 0.03410883\n",
      "Iteration 32, loss = 0.03395056\n",
      "Iteration 33, loss = 0.03377723\n",
      "Iteration 34, loss = 0.03363315\n",
      "Iteration 35, loss = 0.03347405\n",
      "Iteration 36, loss = 0.03335313\n",
      "Iteration 37, loss = 0.03319776\n",
      "Iteration 38, loss = 0.03299255\n",
      "Iteration 39, loss = 0.03233856\n",
      "Iteration 40, loss = 0.03118010\n",
      "Iteration 41, loss = 0.03138454\n",
      "Iteration 42, loss = 0.03130276\n",
      "Iteration 43, loss = 0.03124223\n",
      "Iteration 44, loss = 0.03117518\n",
      "Iteration 45, loss = 0.03111264\n",
      "Iteration 46, loss = 0.03106458\n",
      "Iteration 47, loss = 0.03100357\n",
      "Iteration 48, loss = 0.03094908\n",
      "Iteration 49, loss = 0.03089355\n",
      "Iteration 50, loss = 0.03083730\n",
      "Iteration 51, loss = 0.03077978\n",
      "Iteration 52, loss = 0.03073503\n",
      "Iteration 53, loss = 0.03041663\n",
      "Iteration 54, loss = 0.02964669\n",
      "Iteration 55, loss = 0.02944769\n",
      "Iteration 56, loss = 0.02970138\n",
      "Iteration 57, loss = 0.02965699\n",
      "Iteration 58, loss = 0.03002369\n",
      "Iteration 59, loss = 0.02960202\n",
      "Iteration 60, loss = 0.02958223\n",
      "Iteration 61, loss = 0.02956468\n",
      "Iteration 62, loss = 0.02953081\n",
      "Iteration 63, loss = 0.02981722\n",
      "Iteration 64, loss = 0.02983612\n",
      "Iteration 65, loss = 0.02901566\n",
      "Iteration 66, loss = 0.02899556\n",
      "Iteration 67, loss = 0.02899586\n",
      "Iteration 68, loss = 0.02899274\n",
      "Iteration 69, loss = 0.02899727\n",
      "Iteration 70, loss = 0.02898692\n",
      "Iteration 71, loss = 0.02898543\n",
      "Iteration 72, loss = 0.02898216\n",
      "Iteration 73, loss = 0.02897080\n",
      "Iteration 74, loss = 0.02941090\n",
      "Iteration 75, loss = 0.02895880\n",
      "Iteration 76, loss = 0.02895587\n",
      "Iteration 77, loss = 0.02895030\n",
      "Iteration 78, loss = 0.02894354\n",
      "Iteration 79, loss = 0.02932314\n",
      "Iteration 80, loss = 0.02893483\n",
      "Iteration 81, loss = 0.02894879\n",
      "Iteration 82, loss = 0.02895308\n",
      "Iteration 83, loss = 0.02895444\n",
      "Iteration 84, loss = 0.02895162\n",
      "Iteration 85, loss = 0.02895088\n",
      "Iteration 86, loss = 0.02894701\n",
      "Iteration 87, loss = 0.02894630\n",
      "Iteration 88, loss = 0.02894347\n",
      "Iteration 89, loss = 0.02890858\n",
      "Iteration 90, loss = 0.02890299\n",
      "Iteration 91, loss = 0.02890197\n",
      "Iteration 92, loss = 0.02890058\n",
      "Iteration 93, loss = 0.02889842\n",
      "Iteration 94, loss = 0.02867508\n",
      "Iteration 95, loss = 0.02891978\n",
      "Iteration 96, loss = 0.02891607\n",
      "Iteration 97, loss = 0.02891387\n",
      "Iteration 98, loss = 0.02891169\n",
      "Iteration 99, loss = 0.02890843\n",
      "Iteration 100, loss = 0.02897309\n",
      "Iteration 101, loss = 0.02890187\n",
      "Iteration 102, loss = 0.02889657\n",
      "Iteration 103, loss = 0.02874304\n",
      "Iteration 104, loss = 0.02937394\n",
      "Iteration 105, loss = 0.02936507\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69607082\n",
      "Iteration 2, loss = 0.46306212\n",
      "Iteration 3, loss = 0.35827543\n",
      "Iteration 4, loss = 0.25858087\n",
      "Iteration 5, loss = 0.15968323\n",
      "Iteration 6, loss = 0.11298032\n",
      "Iteration 7, loss = 0.08091629\n",
      "Iteration 8, loss = 0.06300575\n",
      "Iteration 9, loss = 0.05240675\n",
      "Iteration 10, loss = 0.04872930\n",
      "Iteration 11, loss = 0.04186348\n",
      "Iteration 12, loss = 0.04136478\n",
      "Iteration 13, loss = 0.04064371\n",
      "Iteration 14, loss = 0.03914066\n",
      "Iteration 15, loss = 0.03888421\n",
      "Iteration 16, loss = 0.03881582\n",
      "Iteration 17, loss = 0.03846575\n",
      "Iteration 18, loss = 0.03828468\n",
      "Iteration 19, loss = 0.03812300\n",
      "Iteration 20, loss = 0.03802375\n",
      "Iteration 21, loss = 0.03651631\n",
      "Iteration 22, loss = 0.03588946\n",
      "Iteration 23, loss = 0.03571044\n",
      "Iteration 24, loss = 0.03576092\n",
      "Iteration 25, loss = 0.03514459\n",
      "Iteration 26, loss = 0.03547300\n",
      "Iteration 27, loss = 0.03549577\n",
      "Iteration 28, loss = 0.03546647\n",
      "Iteration 29, loss = 0.03545782\n",
      "Iteration 30, loss = 0.03545753\n",
      "Iteration 31, loss = 0.03531605\n",
      "Iteration 32, loss = 0.03572877\n",
      "Iteration 33, loss = 0.03555850\n",
      "Iteration 34, loss = 0.03526478\n",
      "Iteration 35, loss = 0.03512239\n",
      "Iteration 36, loss = 0.03536000\n",
      "Iteration 37, loss = 0.03536803\n",
      "Iteration 38, loss = 0.03536185\n",
      "Iteration 39, loss = 0.03535644\n",
      "Iteration 40, loss = 0.03535134\n",
      "Iteration 41, loss = 0.03532303\n",
      "Iteration 42, loss = 0.03540745\n",
      "Iteration 43, loss = 0.03539372\n",
      "Iteration 44, loss = 0.03533382\n",
      "Iteration 45, loss = 0.03526555\n",
      "Iteration 46, loss = 0.03530636\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.82150402\n",
      "Iteration 2, loss = 0.50609687\n",
      "Iteration 3, loss = 0.35358155\n",
      "Iteration 4, loss = 0.18667521\n",
      "Iteration 5, loss = 0.09651870\n",
      "Iteration 6, loss = 0.07741794\n",
      "Iteration 7, loss = 0.06102249\n",
      "Iteration 8, loss = 0.05680801\n",
      "Iteration 9, loss = 0.05080761\n",
      "Iteration 10, loss = 0.04854582\n",
      "Iteration 11, loss = 0.04746602\n",
      "Iteration 12, loss = 0.04691919\n",
      "Iteration 13, loss = 0.04644048\n",
      "Iteration 14, loss = 0.04608069\n",
      "Iteration 15, loss = 0.04575647\n",
      "Iteration 16, loss = 0.04547617\n",
      "Iteration 17, loss = 0.04395004\n",
      "Iteration 18, loss = 0.04344216\n",
      "Iteration 19, loss = 0.04323120\n",
      "Iteration 20, loss = 0.04355291\n",
      "Iteration 21, loss = 0.04351525\n",
      "Iteration 22, loss = 0.04346434\n",
      "Iteration 23, loss = 0.04332257\n",
      "Iteration 24, loss = 0.04316424\n",
      "Iteration 25, loss = 0.04297425\n",
      "Iteration 26, loss = 0.04239484\n",
      "Iteration 27, loss = 0.04238048\n",
      "Iteration 28, loss = 0.04236910\n",
      "Iteration 29, loss = 0.04236052\n",
      "Iteration 30, loss = 0.04234828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 31, loss = 0.04213426\n",
      "Iteration 32, loss = 0.04232617\n",
      "Iteration 33, loss = 0.04267839\n",
      "Iteration 34, loss = 0.04234663\n",
      "Iteration 35, loss = 0.04200560\n",
      "Iteration 36, loss = 0.04223344\n",
      "Iteration 37, loss = 0.04228761\n",
      "Iteration 38, loss = 0.04224948\n",
      "Iteration 39, loss = 0.04202477\n",
      "Iteration 40, loss = 0.04192895\n",
      "Iteration 41, loss = 0.04138489\n",
      "Iteration 42, loss = 0.04138484\n",
      "Iteration 43, loss = 0.04104172\n",
      "Iteration 44, loss = 0.04126342\n",
      "Iteration 45, loss = 0.04091456\n",
      "Iteration 46, loss = 0.04063067\n",
      "Iteration 47, loss = 0.04120672\n",
      "Iteration 48, loss = 0.04119159\n",
      "Iteration 49, loss = 0.04119205\n",
      "Iteration 50, loss = 0.04115637\n",
      "Iteration 51, loss = 0.04115274\n",
      "Iteration 52, loss = 0.04114448\n",
      "Iteration 53, loss = 0.04113372\n",
      "Iteration 54, loss = 0.04111896\n",
      "Iteration 55, loss = 0.04141695\n",
      "Iteration 56, loss = 0.04162581\n",
      "Iteration 57, loss = 0.04160562\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67632874\n",
      "Iteration 2, loss = 0.20635580\n",
      "Iteration 3, loss = 0.11590238\n",
      "Iteration 4, loss = 0.07137043\n",
      "Iteration 5, loss = 0.05057501\n",
      "Iteration 6, loss = 0.04788799\n",
      "Iteration 7, loss = 0.04640311\n",
      "Iteration 8, loss = 0.04642461\n",
      "Iteration 9, loss = 0.04631200\n",
      "Iteration 10, loss = 0.04616778\n",
      "Iteration 11, loss = 0.04607243\n",
      "Iteration 12, loss = 0.04599328\n",
      "Iteration 13, loss = 0.04591343\n",
      "Iteration 14, loss = 0.04578722\n",
      "Iteration 15, loss = 0.04576826\n",
      "Iteration 16, loss = 0.04549163\n",
      "Iteration 17, loss = 0.04601654\n",
      "Iteration 18, loss = 0.04595225\n",
      "Iteration 19, loss = 0.04504573\n",
      "Iteration 20, loss = 0.04530120\n",
      "Iteration 21, loss = 0.04521450\n",
      "Iteration 22, loss = 0.04527827\n",
      "Iteration 23, loss = 0.04530078\n",
      "Iteration 24, loss = 0.04525386\n",
      "Iteration 25, loss = 0.04521634\n",
      "Iteration 26, loss = 0.04514613\n",
      "Iteration 27, loss = 0.04511582\n",
      "Iteration 28, loss = 0.04506869\n",
      "Iteration 29, loss = 0.04504160\n",
      "Iteration 30, loss = 0.04502360\n",
      "Iteration 31, loss = 0.04499068\n",
      "Iteration 32, loss = 0.04496276\n",
      "Iteration 33, loss = 0.04501031\n",
      "Iteration 34, loss = 0.04523352\n",
      "Iteration 35, loss = 0.04568054\n",
      "Iteration 36, loss = 0.04562057\n",
      "Iteration 37, loss = 0.04552248\n",
      "Iteration 38, loss = 0.04580243\n",
      "Iteration 39, loss = 0.04571152\n",
      "Iteration 40, loss = 0.04564532\n",
      "Iteration 41, loss = 0.04557783\n",
      "Iteration 42, loss = 0.04582285\n",
      "Iteration 43, loss = 0.04523614\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.49020189\n",
      "Iteration 2, loss = 0.39515914\n",
      "Iteration 3, loss = 0.26065864\n",
      "Iteration 4, loss = 0.25924681\n",
      "Iteration 5, loss = 0.25313873\n",
      "Iteration 6, loss = 0.23551116\n",
      "Iteration 7, loss = 0.22464484\n",
      "Iteration 8, loss = 0.21971819\n",
      "Iteration 9, loss = 0.21697956\n",
      "Iteration 10, loss = 0.21529195\n",
      "Iteration 11, loss = 0.22346835\n",
      "Iteration 12, loss = 0.22676082\n",
      "Iteration 13, loss = 0.22789051\n",
      "Iteration 14, loss = 0.22522290\n",
      "Iteration 15, loss = 0.21926134\n",
      "Iteration 16, loss = 0.21810540\n",
      "Iteration 17, loss = 0.21627274\n",
      "Iteration 18, loss = 0.21873094\n",
      "Iteration 19, loss = 0.21921677\n",
      "Iteration 20, loss = 0.21528103\n",
      "Iteration 21, loss = 0.22736088\n",
      "Iteration 22, loss = 0.23125725\n",
      "Iteration 23, loss = 0.21942669\n",
      "Iteration 24, loss = 0.21843628\n",
      "Iteration 25, loss = 0.21897599\n",
      "Iteration 26, loss = 0.21653906\n",
      "Iteration 27, loss = 0.21737260\n",
      "Iteration 28, loss = 0.21863111\n",
      "Iteration 29, loss = 0.21646937\n",
      "Iteration 30, loss = 0.21367898\n",
      "Iteration 31, loss = 0.21258636\n",
      "Iteration 32, loss = 0.21467429\n",
      "Iteration 33, loss = 0.21398560\n",
      "Iteration 34, loss = 0.21387704\n",
      "Iteration 35, loss = 0.21379542\n",
      "Iteration 36, loss = 0.21359151\n",
      "Iteration 37, loss = 0.21347575\n",
      "Iteration 38, loss = 0.21345462\n",
      "Iteration 39, loss = 0.21348145\n",
      "Iteration 40, loss = 0.21338846\n",
      "Iteration 41, loss = 0.21332935\n",
      "Iteration 42, loss = 0.21326045\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.53203429\n",
      "Iteration 2, loss = 0.71922070\n",
      "Iteration 3, loss = 0.62279789\n",
      "Iteration 4, loss = 0.59243848\n",
      "Iteration 5, loss = 0.56866973\n",
      "Iteration 6, loss = 0.53206294\n",
      "Iteration 7, loss = 0.55057078\n",
      "Iteration 8, loss = 0.54237108\n",
      "Iteration 9, loss = 0.53572096\n",
      "Iteration 10, loss = 0.54889657\n",
      "Iteration 11, loss = 0.53699826\n",
      "Iteration 12, loss = 0.51297796\n",
      "Iteration 13, loss = 0.55983780\n",
      "Iteration 14, loss = 0.54611970\n",
      "Iteration 15, loss = 0.54061942\n",
      "Iteration 16, loss = 0.54811503\n",
      "Iteration 17, loss = 0.58661294\n",
      "Iteration 18, loss = 0.55911891\n",
      "Iteration 19, loss = 0.54106057\n",
      "Iteration 20, loss = 0.54460114\n",
      "Iteration 21, loss = 0.54829686\n",
      "Iteration 22, loss = 0.57497967\n",
      "Iteration 23, loss = 0.54482071\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.56485829\n",
      "Iteration 2, loss = 0.77504999\n",
      "Iteration 3, loss = 0.66877634\n",
      "Iteration 4, loss = 0.66206927\n",
      "Iteration 5, loss = 0.65676503\n",
      "Iteration 6, loss = 0.64011875\n",
      "Iteration 7, loss = 0.68517174\n",
      "Iteration 8, loss = 0.69720240\n",
      "Iteration 9, loss = 0.67621714\n",
      "Iteration 10, loss = 0.63639459\n",
      "Iteration 11, loss = 0.68719744\n",
      "Iteration 12, loss = 0.65700799\n",
      "Iteration 13, loss = 0.65562035\n",
      "Iteration 14, loss = 0.66752808\n",
      "Iteration 15, loss = 0.65446673\n",
      "Iteration 16, loss = 0.66401718\n",
      "Iteration 17, loss = 0.70623916\n",
      "Iteration 18, loss = 0.68249648\n",
      "Iteration 19, loss = 0.66536297\n",
      "Iteration 20, loss = 0.68479792\n",
      "Iteration 21, loss = 0.66454643\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73807976\n",
      "Iteration 2, loss = 0.71867428\n",
      "Iteration 3, loss = 0.71137960\n",
      "Iteration 4, loss = 0.68969668\n",
      "Iteration 5, loss = 0.70244705\n",
      "Iteration 6, loss = 0.70041624\n",
      "Iteration 7, loss = 0.70694502\n",
      "Iteration 8, loss = 0.70954283\n",
      "Iteration 9, loss = 0.70698546\n",
      "Iteration 10, loss = 0.70669746\n",
      "Iteration 11, loss = 0.70090905\n",
      "Iteration 12, loss = 0.71083192\n",
      "Iteration 13, loss = 0.70217511\n",
      "Iteration 14, loss = 0.72303954\n",
      "Iteration 15, loss = 0.71771749\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75389087\n",
      "Iteration 2, loss = 0.70238204\n",
      "Iteration 3, loss = 0.69349850\n",
      "Iteration 4, loss = 0.69990694\n",
      "Iteration 5, loss = 0.69886337\n",
      "Iteration 6, loss = 0.68017817\n",
      "Iteration 7, loss = 0.68808060\n",
      "Iteration 8, loss = 0.67500190\n",
      "Iteration 9, loss = 0.66812134\n",
      "Iteration 10, loss = 0.68465172\n",
      "Iteration 11, loss = 0.67094445\n",
      "Iteration 12, loss = 0.65334034\n",
      "Iteration 13, loss = 0.64760548\n",
      "Iteration 14, loss = 0.65286274\n",
      "Iteration 15, loss = 0.64934399\n",
      "Iteration 16, loss = 0.62820042\n",
      "Iteration 17, loss = 0.61678496\n",
      "Iteration 18, loss = 0.61840648\n",
      "Iteration 19, loss = 0.59398927\n",
      "Iteration 20, loss = 0.60016721\n",
      "Iteration 21, loss = 0.58595420\n",
      "Iteration 22, loss = 0.57346590\n",
      "Iteration 23, loss = 0.56118429\n",
      "Iteration 24, loss = 0.55265292\n",
      "Iteration 25, loss = 0.54986535\n",
      "Iteration 26, loss = 0.53874886\n",
      "Iteration 27, loss = 0.53497050\n",
      "Iteration 28, loss = 0.51503080\n",
      "Iteration 29, loss = 0.49652569\n",
      "Iteration 30, loss = 0.47555018\n",
      "Iteration 31, loss = 0.46985216\n",
      "Iteration 32, loss = 0.67412369\n",
      "Iteration 33, loss = 0.72741479\n",
      "Iteration 34, loss = 0.71904124\n",
      "Iteration 35, loss = 0.70990347\n",
      "Iteration 36, loss = 0.72915612\n",
      "Iteration 37, loss = 0.72475639\n",
      "Iteration 38, loss = 0.72050133\n",
      "Iteration 39, loss = 0.71893243\n",
      "Iteration 40, loss = 0.71878807\n",
      "Iteration 41, loss = 0.72012177\n",
      "Iteration 42, loss = 0.72051301\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.82840114\n",
      "Iteration 2, loss = 0.70107240\n",
      "Iteration 3, loss = 0.69590875\n",
      "Iteration 4, loss = 0.69134277\n",
      "Iteration 5, loss = 0.72370077\n",
      "Iteration 6, loss = 0.72136970\n",
      "Iteration 7, loss = 0.75437736\n",
      "Iteration 8, loss = 0.71699188\n",
      "Iteration 9, loss = 0.80750773\n",
      "Iteration 10, loss = 0.74978303\n",
      "Iteration 11, loss = 0.72666655\n",
      "Iteration 12, loss = 0.68921553\n",
      "Iteration 13, loss = 0.72295569\n",
      "Iteration 14, loss = 0.72230933\n",
      "Iteration 15, loss = 0.71593035\n",
      "Iteration 16, loss = 0.71825571\n",
      "Iteration 17, loss = 0.72280472\n",
      "Iteration 18, loss = 0.71587417\n",
      "Iteration 19, loss = 0.72765182\n",
      "Iteration 20, loss = 0.72222781\n",
      "Iteration 21, loss = 0.71966416\n",
      "Iteration 22, loss = 0.71731576\n",
      "Iteration 23, loss = 0.71884771\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot learning curve.\n",
    "from yellowbrick.model_selection import LearningCurve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# cv = KFold(12)\n",
    "cv = StratifiedKFold(4)\n",
    "sizes = np.linspace(0.1,1.0,10)\n",
    "\n",
    "viz = LearningCurve(mlp, cv=cv, train_sizes=sizes, n_jobs=1)\n",
    "viz.fit(X, y)\n",
    "viz.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
